<?xml version='1.0' encoding='UTF-8' standalone='no'?>
<doxygen xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:noNamespaceSchemaLocation="compound.xsd" version="1.8.11">
  <compounddef id="classnvidia_1_1inferenceserver_1_1client_1_1InferHttpContext" kind="class" language="C++" prot="public">
    <compoundname>nvidia::inferenceserver::client::InferHttpContext</compoundname>
    <basecompoundref refid="classnvidia_1_1inferenceserver_1_1client_1_1InferContext" prot="public" virt="non-virtual">nvidia::inferenceserver::client::InferContext</basecompoundref>
    <includes refid="request_8h" local="no">request.h</includes>
      <sectiondef kind="private-attrib">
      <memberdef kind="variable" id="classnvidia_1_1inferenceserver_1_1client_1_1InferHttpContext_1a1fb8739cc7fb8021b16207b14122286f" prot="private" static="no" mutable="no">
        <type>CURLM *</type>
        <definition>CURLM* nvidia::inferenceserver::client::InferHttpContext::multi_handle_</definition>
        <argsstring></argsstring>
        <name>multi_handle_</name>
        <briefdescription>
        </briefdescription>
        <detaileddescription>
        </detaileddescription>
        <inbodydescription>
        </inbodydescription>
        <location file="src/clients/c++/request.h" line="952" column="1" bodyfile="src/clients/c++/request.h" bodystart="952" bodyend="-1"/>
      </memberdef>
      <memberdef kind="variable" id="classnvidia_1_1inferenceserver_1_1client_1_1InferHttpContext_1a6d089cda0592c997c5046167e8a20e75" prot="private" static="no" mutable="no">
        <type>std::string</type>
        <definition>std::string nvidia::inferenceserver::client::InferHttpContext::url_</definition>
        <argsstring></argsstring>
        <name>url_</name>
        <briefdescription>
        </briefdescription>
        <detaileddescription>
        </detaileddescription>
        <inbodydescription>
        </inbodydescription>
        <location file="src/clients/c++/request.h" line="955" column="1" bodyfile="src/clients/c++/request.h" bodystart="955" bodyend="-1"/>
      </memberdef>
      <memberdef kind="variable" id="classnvidia_1_1inferenceserver_1_1client_1_1InferHttpContext_1a2e79bb4348ba903099c7cf57272d6f0e" prot="private" static="no" mutable="no">
        <type>std::string</type>
        <definition>std::string nvidia::inferenceserver::client::InferHttpContext::infer_request_str_</definition>
        <argsstring></argsstring>
        <name>infer_request_str_</name>
        <briefdescription>
        </briefdescription>
        <detaileddescription>
        </detaileddescription>
        <inbodydescription>
        </inbodydescription>
        <location file="src/clients/c++/request.h" line="958" column="1" bodyfile="src/clients/c++/request.h" bodystart="958" bodyend="-1"/>
      </memberdef>
      <memberdef kind="variable" id="classnvidia_1_1inferenceserver_1_1client_1_1InferHttpContext_1a7ed61d8774275c4b9e0910ead0f42326" prot="private" static="no" mutable="no">
        <type>CURL *</type>
        <definition>CURL* nvidia::inferenceserver::client::InferHttpContext::curl_</definition>
        <argsstring></argsstring>
        <name>curl_</name>
        <briefdescription>
        </briefdescription>
        <detaileddescription>
        </detaileddescription>
        <inbodydescription>
        </inbodydescription>
        <location file="src/clients/c++/request.h" line="961" column="1" bodyfile="src/clients/c++/request.h" bodystart="961" bodyend="-1"/>
      </memberdef>
      </sectiondef>
      <sectiondef kind="public-func">
      <memberdef kind="function" id="classnvidia_1_1inferenceserver_1_1client_1_1InferHttpContext_1a0f1905f3c0e275655dadc2ea9fe074c0" prot="public" static="no" const="no" explicit="no" inline="no" virt="non-virtual">
        <type></type>
        <definition>nvidia::inferenceserver::client::InferHttpContext::~InferHttpContext</definition>
        <argsstring>() override</argsstring>
        <name>~InferHttpContext</name>
        <briefdescription>
        </briefdescription>
        <detaileddescription>
        </detaileddescription>
        <inbodydescription>
        </inbodydescription>
        <location file="src/clients/c++/request.h" line="892" column="1"/>
      </memberdef>
      <memberdef kind="function" id="classnvidia_1_1inferenceserver_1_1client_1_1InferHttpContext_1a9d2e82db48e7a11ea2ee21e39113e267" prot="public" static="no" const="no" explicit="no" inline="no" virt="virtual">
        <type><ref refid="classnvidia_1_1inferenceserver_1_1client_1_1Error" kindref="compound">Error</ref></type>
        <definition>Error nvidia::inferenceserver::client::InferHttpContext::Run</definition>
        <argsstring>(ResultMap *results) override</argsstring>
        <name>Run</name>
        <reimplements refid="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1a4cac8002817c9259851305b14c258f36">Run</reimplements>
        <param>
          <type>ResultMap *</type>
          <declname>results</declname>
        </param>
        <briefdescription>
<para>Send a synchronous request to the inference server to perform an inference to produce results for the outputs specified in the most recent call to <ref refid="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1a7cf1746d814561727fd3b07b0f8349f2" kindref="member">SetRunOptions()</ref>. </para>        </briefdescription>
        <detaileddescription>
<para><parameterlist kind="param"><parameteritem>
<parameternamelist>
<parametername>results</parametername>
</parameternamelist>
<parameterdescription>
<para>Returns Result objects holding inference results as a map from output name to Result object. </para></parameterdescription>
</parameteritem>
</parameterlist>
<simplesect kind="return"><para><ref refid="classnvidia_1_1inferenceserver_1_1client_1_1Error" kindref="compound">Error</ref> object indicating success or failure. </para></simplesect>
</para>        </detaileddescription>
        <inbodydescription>
        </inbodydescription>
        <location file="src/clients/c++/request.h" line="931" column="1"/>
      </memberdef>
      <memberdef kind="function" id="classnvidia_1_1inferenceserver_1_1client_1_1InferHttpContext_1a625d8c3a79b6d3cf08fd442b0d0b2540" prot="public" static="no" const="no" explicit="no" inline="no" virt="virtual">
        <type><ref refid="classnvidia_1_1inferenceserver_1_1client_1_1Error" kindref="compound">Error</ref></type>
        <definition>Error nvidia::inferenceserver::client::InferHttpContext::AsyncRun</definition>
        <argsstring>(std::shared_ptr&lt; Request &gt; *async_request) override</argsstring>
        <name>AsyncRun</name>
        <reimplements refid="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1a255a59df29a53d6e45ce86663887615f">AsyncRun</reimplements>
        <param>
          <type>std::shared_ptr&lt; <ref refid="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Request" kindref="compound">Request</ref> &gt; *</type>
          <declname>async_request</declname>
        </param>
        <briefdescription>
<para>Send an asynchronous request to the inference server to perform an inference to produce results for the outputs specified in the most recent call to <ref refid="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1a7cf1746d814561727fd3b07b0f8349f2" kindref="member">SetRunOptions()</ref>. </para>        </briefdescription>
        <detaileddescription>
<para><parameterlist kind="param"><parameteritem>
<parameternamelist>
<parametername>async_request</parametername>
</parameternamelist>
<parameterdescription>
<para>Returns a Request object that can be used to retrieve the inference results for the request. </para></parameterdescription>
</parameteritem>
</parameterlist>
<simplesect kind="return"><para><ref refid="classnvidia_1_1inferenceserver_1_1client_1_1Error" kindref="compound">Error</ref> object indicating success or failure. </para></simplesect>
</para>        </detaileddescription>
        <inbodydescription>
        </inbodydescription>
        <location file="src/clients/c++/request.h" line="932" column="1"/>
      </memberdef>
      <memberdef kind="function" id="classnvidia_1_1inferenceserver_1_1client_1_1InferHttpContext_1a646ec6ca28f8c38a23b4e85c3ad8e8ec" prot="public" static="no" const="no" explicit="no" inline="no" virt="virtual">
        <type><ref refid="classnvidia_1_1inferenceserver_1_1client_1_1Error" kindref="compound">Error</ref></type>
        <definition>Error nvidia::inferenceserver::client::InferHttpContext::GetAsyncRunResults</definition>
        <argsstring>(ResultMap *results, const std::shared_ptr&lt; Request &gt; &amp;async_request, bool wait) override</argsstring>
        <name>GetAsyncRunResults</name>
        <reimplements refid="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1a6a9a45e35031ce4b0b50af8bd0c4ee8c">GetAsyncRunResults</reimplements>
        <param>
          <type>ResultMap *</type>
          <declname>results</declname>
        </param>
        <param>
          <type>const std::shared_ptr&lt; <ref refid="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Request" kindref="compound">Request</ref> &gt; &amp;</type>
          <declname>async_request</declname>
        </param>
        <param>
          <type>bool</type>
          <declname>wait</declname>
        </param>
        <briefdescription>
<para>Get the results of the asynchronous request referenced by &apos;async_request&apos;. </para>        </briefdescription>
        <detaileddescription>
<para><parameterlist kind="param"><parameteritem>
<parameternamelist>
<parametername>results</parametername>
</parameternamelist>
<parameterdescription>
<para>Returns Result objects holding inference results as a map from output name to Result object. </para></parameterdescription>
</parameteritem>
<parameteritem>
<parameternamelist>
<parametername>async_request</parametername>
</parameternamelist>
<parameterdescription>
<para>Request handle to retrieve results. </para></parameterdescription>
</parameteritem>
<parameteritem>
<parameternamelist>
<parametername>wait</parametername>
</parameternamelist>
<parameterdescription>
<para>If true, block until the request completes. Otherwise, return immediately. </para></parameterdescription>
</parameteritem>
</parameterlist>
<simplesect kind="return"><para><ref refid="classnvidia_1_1inferenceserver_1_1client_1_1Error" kindref="compound">Error</ref> object indicating success or failure. Success will be returned only if the request has been completed succesfully. UNAVAILABLE will be returned if &apos;wait&apos; is false and the request is not ready. </para></simplesect>
</para>        </detaileddescription>
        <inbodydescription>
        </inbodydescription>
        <location file="src/clients/c++/request.h" line="933" column="1"/>
      </memberdef>
      </sectiondef>
      <sectiondef kind="public-static-func">
      <memberdef kind="function" id="classnvidia_1_1inferenceserver_1_1client_1_1InferHttpContext_1a7c210c15455fde64a31c6925f3a8b906" prot="public" static="yes" const="no" explicit="no" inline="no" virt="non-virtual">
        <type><ref refid="classnvidia_1_1inferenceserver_1_1client_1_1Error" kindref="compound">Error</ref></type>
        <definition>static Error nvidia::inferenceserver::client::InferHttpContext::Create</definition>
        <argsstring>(std::unique_ptr&lt; InferContext &gt; *ctx, const std::string &amp;server_url, const std::string &amp;model_name, int64_t model_version=-1, bool verbose=false)</argsstring>
        <name>Create</name>
        <param>
          <type>std::unique_ptr&lt; <ref refid="classnvidia_1_1inferenceserver_1_1client_1_1InferContext" kindref="compound">InferContext</ref> &gt; *</type>
          <declname>ctx</declname>
        </param>
        <param>
          <type>const std::string &amp;</type>
          <declname>server_url</declname>
        </param>
        <param>
          <type>const std::string &amp;</type>
          <declname>model_name</declname>
        </param>
        <param>
          <type>int64_t</type>
          <declname>model_version</declname>
          <defval>-1</defval>
        </param>
        <param>
          <type>bool</type>
          <declname>verbose</declname>
          <defval>false</defval>
        </param>
        <briefdescription>
<para>Create context that performs inference for a non-sequence model using HTTP protocol. </para>        </briefdescription>
        <detaileddescription>
<para><parameterlist kind="param"><parameteritem>
<parameternamelist>
<parametername>ctx</parametername>
</parameternamelist>
<parameterdescription>
<para>Returns a new <ref refid="classnvidia_1_1inferenceserver_1_1client_1_1InferHttpContext" kindref="compound">InferHttpContext</ref> object. </para></parameterdescription>
</parameteritem>
<parameteritem>
<parameternamelist>
<parametername>server_url</parametername>
</parameternamelist>
<parameterdescription>
<para>The inference server name and port. </para></parameterdescription>
</parameteritem>
<parameteritem>
<parameternamelist>
<parametername>model_name</parametername>
</parameternamelist>
<parameterdescription>
<para>The name of the model to get status for. </para></parameterdescription>
</parameteritem>
<parameteritem>
<parameternamelist>
<parametername>model_version</parametername>
</parameternamelist>
<parameterdescription>
<para>The version of the model to use for inference, or -1 to indicate that the latest (i.e. highest version number) version should be used. </para></parameterdescription>
</parameteritem>
<parameteritem>
<parameternamelist>
<parametername>verbose</parametername>
</parameternamelist>
<parameterdescription>
<para>If true generate verbose output when contacting the inference server. </para></parameterdescription>
</parameteritem>
</parameterlist>
<simplesect kind="return"><para><ref refid="classnvidia_1_1inferenceserver_1_1client_1_1Error" kindref="compound">Error</ref> object indicating success or failure. </para></simplesect>
</para>        </detaileddescription>
        <inbodydescription>
        </inbodydescription>
        <location file="src/clients/c++/request.h" line="906" column="1"/>
      </memberdef>
      <memberdef kind="function" id="classnvidia_1_1inferenceserver_1_1client_1_1InferHttpContext_1ab59a20c6f183e55f0791d07e3d8c223b" prot="public" static="yes" const="no" explicit="no" inline="no" virt="non-virtual">
        <type><ref refid="classnvidia_1_1inferenceserver_1_1client_1_1Error" kindref="compound">Error</ref></type>
        <definition>static Error nvidia::inferenceserver::client::InferHttpContext::Create</definition>
        <argsstring>(std::unique_ptr&lt; InferContext &gt; *ctx, CorrelationID correlation_id, const std::string &amp;server_url, const std::string &amp;model_name, int64_t model_version=-1, bool verbose=false)</argsstring>
        <name>Create</name>
        <param>
          <type>std::unique_ptr&lt; <ref refid="classnvidia_1_1inferenceserver_1_1client_1_1InferContext" kindref="compound">InferContext</ref> &gt; *</type>
          <declname>ctx</declname>
        </param>
        <param>
          <type>CorrelationID</type>
          <declname>correlation_id</declname>
        </param>
        <param>
          <type>const std::string &amp;</type>
          <declname>server_url</declname>
        </param>
        <param>
          <type>const std::string &amp;</type>
          <declname>model_name</declname>
        </param>
        <param>
          <type>int64_t</type>
          <declname>model_version</declname>
          <defval>-1</defval>
        </param>
        <param>
          <type>bool</type>
          <declname>verbose</declname>
          <defval>false</defval>
        </param>
        <briefdescription>
<para>Create context that performs inference for a sequence model using a given correlation ID and the HTTP protocol. </para>        </briefdescription>
        <detaileddescription>
<para><parameterlist kind="param"><parameteritem>
<parameternamelist>
<parametername>ctx</parametername>
</parameternamelist>
<parameterdescription>
<para>Returns a new <ref refid="classnvidia_1_1inferenceserver_1_1client_1_1InferHttpContext" kindref="compound">InferHttpContext</ref> object. </para></parameterdescription>
</parameteritem>
<parameteritem>
<parameternamelist>
<parametername>correlation_id</parametername>
</parameternamelist>
<parameterdescription>
<para>The correlation ID to use for all inferences performed with this context. A value of 0 (zero) indicates that no correlation ID should be used. </para></parameterdescription>
</parameteritem>
<parameteritem>
<parameternamelist>
<parametername>server_url</parametername>
</parameternamelist>
<parameterdescription>
<para>The inference server name and port. </para></parameterdescription>
</parameteritem>
<parameteritem>
<parameternamelist>
<parametername>model_name</parametername>
</parameternamelist>
<parameterdescription>
<para>The name of the model to get status for. </para></parameterdescription>
</parameteritem>
<parameteritem>
<parameternamelist>
<parametername>model_version</parametername>
</parameternamelist>
<parameterdescription>
<para>The version of the model to use for inference, or -1 to indicate that the latest (i.e. highest version number) version should be used. </para></parameterdescription>
</parameteritem>
<parameteritem>
<parameternamelist>
<parametername>verbose</parametername>
</parameternamelist>
<parameterdescription>
<para>If true generate verbose output when contacting the inference server. </para></parameterdescription>
</parameteritem>
</parameterlist>
<simplesect kind="return"><para><ref refid="classnvidia_1_1inferenceserver_1_1client_1_1Error" kindref="compound">Error</ref> object indicating success or failure. </para></simplesect>
</para>        </detaileddescription>
        <inbodydescription>
        </inbodydescription>
        <location file="src/clients/c++/request.h" line="926" column="1"/>
      </memberdef>
      </sectiondef>
      <sectiondef kind="private-static-func">
      <memberdef kind="function" id="classnvidia_1_1inferenceserver_1_1client_1_1InferHttpContext_1a227f476a78ad517007610268d8867719" prot="private" static="yes" const="no" explicit="no" inline="no" virt="non-virtual">
        <type>size_t</type>
        <definition>static size_t nvidia::inferenceserver::client::InferHttpContext::RequestProvider</definition>
        <argsstring>(void *, size_t, size_t, void *)</argsstring>
        <name>RequestProvider</name>
        <param>
          <type>void *</type>
        </param>
        <param>
          <type>size_t</type>
        </param>
        <param>
          <type>size_t</type>
        </param>
        <param>
          <type>void *</type>
        </param>
        <briefdescription>
        </briefdescription>
        <detaileddescription>
        </detaileddescription>
        <inbodydescription>
        </inbodydescription>
        <location file="src/clients/c++/request.h" line="938" column="1"/>
      </memberdef>
      <memberdef kind="function" id="classnvidia_1_1inferenceserver_1_1client_1_1InferHttpContext_1aff39cc0dbd3d152b57c7a24a5a3d9420" prot="private" static="yes" const="no" explicit="no" inline="no" virt="non-virtual">
        <type>size_t</type>
        <definition>static size_t nvidia::inferenceserver::client::InferHttpContext::ResponseHeaderHandler</definition>
        <argsstring>(void *, size_t, size_t, void *)</argsstring>
        <name>ResponseHeaderHandler</name>
        <param>
          <type>void *</type>
        </param>
        <param>
          <type>size_t</type>
        </param>
        <param>
          <type>size_t</type>
        </param>
        <param>
          <type>void *</type>
        </param>
        <briefdescription>
        </briefdescription>
        <detaileddescription>
        </detaileddescription>
        <inbodydescription>
        </inbodydescription>
        <location file="src/clients/c++/request.h" line="939" column="1"/>
      </memberdef>
      <memberdef kind="function" id="classnvidia_1_1inferenceserver_1_1client_1_1InferHttpContext_1ad87b9f3ba4b4516f576a4453fac710ae" prot="private" static="yes" const="no" explicit="no" inline="no" virt="non-virtual">
        <type>size_t</type>
        <definition>static size_t nvidia::inferenceserver::client::InferHttpContext::ResponseHandler</definition>
        <argsstring>(void *, size_t, size_t, void *)</argsstring>
        <name>ResponseHandler</name>
        <param>
          <type>void *</type>
        </param>
        <param>
          <type>size_t</type>
        </param>
        <param>
          <type>size_t</type>
        </param>
        <param>
          <type>void *</type>
        </param>
        <briefdescription>
        </briefdescription>
        <detaileddescription>
        </detaileddescription>
        <inbodydescription>
        </inbodydescription>
        <location file="src/clients/c++/request.h" line="940" column="1"/>
      </memberdef>
      </sectiondef>
      <sectiondef kind="private-func">
      <memberdef kind="function" id="classnvidia_1_1inferenceserver_1_1client_1_1InferHttpContext_1ac3b159dca6c95b4b0367bea6705a4a11" prot="private" static="no" const="no" explicit="no" inline="no" virt="non-virtual">
        <type></type>
        <definition>nvidia::inferenceserver::client::InferHttpContext::InferHttpContext</definition>
        <argsstring>(const std::string &amp;, const std::string &amp;, int64_t, CorrelationID, bool)</argsstring>
        <name>InferHttpContext</name>
        <param>
          <type>const std::string &amp;</type>
        </param>
        <param>
          <type>const std::string &amp;</type>
        </param>
        <param>
          <type>int64_t</type>
        </param>
        <param>
          <type>CorrelationID</type>
        </param>
        <param>
          <type>bool</type>
        </param>
        <briefdescription>
        </briefdescription>
        <detaileddescription>
        </detaileddescription>
        <inbodydescription>
        </inbodydescription>
        <location file="src/clients/c++/request.h" line="942" column="1"/>
      </memberdef>
      <memberdef kind="function" id="classnvidia_1_1inferenceserver_1_1client_1_1InferHttpContext_1abb46a65e4952b7fa15c575dedae8f54c" prot="private" static="no" const="no" explicit="no" inline="no" virt="virtual">
        <type>void</type>
        <definition>void nvidia::inferenceserver::client::InferHttpContext::AsyncTransfer</definition>
        <argsstring>() override</argsstring>
        <name>AsyncTransfer</name>
        <reimplements refid="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1ae32e8e83bf498b4e91452703dd12cca0">AsyncTransfer</reimplements>
        <briefdescription>
        </briefdescription>
        <detaileddescription>
        </detaileddescription>
        <inbodydescription>
        </inbodydescription>
        <location file="src/clients/c++/request.h" line="946" column="1"/>
      </memberdef>
      <memberdef kind="function" id="classnvidia_1_1inferenceserver_1_1client_1_1InferHttpContext_1af1a63b2ce2c49270eaab14ed71fd803a" prot="private" static="no" const="no" explicit="no" inline="no" virt="virtual">
        <type><ref refid="classnvidia_1_1inferenceserver_1_1client_1_1Error" kindref="compound">Error</ref></type>
        <definition>Error nvidia::inferenceserver::client::InferHttpContext::PreRunProcessing</definition>
        <argsstring>(std::shared_ptr&lt; Request &gt; &amp;request) override</argsstring>
        <name>PreRunProcessing</name>
        <reimplements refid="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1ad2c7af868ffbb1a2e24f47d9ae0093a5">PreRunProcessing</reimplements>
        <param>
          <type>std::shared_ptr&lt; <ref refid="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Request" kindref="compound">Request</ref> &gt; &amp;</type>
          <declname>request</declname>
        </param>
        <briefdescription>
        </briefdescription>
        <detaileddescription>
        </detaileddescription>
        <inbodydescription>
        </inbodydescription>
        <location file="src/clients/c++/request.h" line="949" column="1"/>
      </memberdef>
      </sectiondef>
    <briefdescription>
<para><ref refid="classnvidia_1_1inferenceserver_1_1client_1_1InferHttpContext" kindref="compound">InferHttpContext</ref> is the HTTP instantiation of <ref refid="classnvidia_1_1inferenceserver_1_1client_1_1InferContext" kindref="compound">InferContext</ref>. </para>    </briefdescription>
    <detaileddescription>
    </detaileddescription>
    <inheritancegraph>
      <node id="106">
        <label>nvidia::inferenceserver::client::InferContext</label>
        <link refid="classnvidia_1_1inferenceserver_1_1client_1_1InferContext"/>
      </node>
      <node id="105">
        <label>nvidia::inferenceserver::client::InferHttpContext</label>
        <link refid="classnvidia_1_1inferenceserver_1_1client_1_1InferHttpContext"/>
        <childnode refid="106" relation="public-inheritance">
        </childnode>
      </node>
    </inheritancegraph>
    <collaborationgraph>
      <node id="109">
        <label>nvidia::inferenceserver::client::InferContext::Stat</label>
        <link refid="structnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Stat"/>
      </node>
      <node id="108">
        <label>nvidia::inferenceserver::client::InferContext</label>
        <link refid="classnvidia_1_1inferenceserver_1_1client_1_1InferContext"/>
        <childnode refid="109" relation="usage">
          <edgelabel>context_stat_</edgelabel>
        </childnode>
      </node>
      <node id="107">
        <label>nvidia::inferenceserver::client::InferHttpContext</label>
        <link refid="classnvidia_1_1inferenceserver_1_1client_1_1InferHttpContext"/>
        <childnode refid="108" relation="public-inheritance">
        </childnode>
      </node>
    </collaborationgraph>
    <location file="src/clients/c++/request.h" line="890" column="1" bodyfile="src/clients/c++/request.h" bodystart="890" bodyend="962"/>
    <listofallmembers>
      <member refid="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1aeb3741fc43d18b9cd7abab193f7ffca0" prot="protected" virt="non-virtual"><scope>nvidia::inferenceserver::client::InferHttpContext</scope><name>async_request_id_</name></member>
      <member refid="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1a6e1d2a28514245af5f71557a18be70c6" prot="protected" virt="non-virtual"><scope>nvidia::inferenceserver::client::InferHttpContext</scope><name>AsyncReqMap</name></member>
      <member refid="classnvidia_1_1inferenceserver_1_1client_1_1InferHttpContext_1a625d8c3a79b6d3cf08fd442b0d0b2540" prot="public" virt="virtual"><scope>nvidia::inferenceserver::client::InferHttpContext</scope><name>AsyncRun</name></member>
      <member refid="classnvidia_1_1inferenceserver_1_1client_1_1InferHttpContext_1abb46a65e4952b7fa15c575dedae8f54c" prot="private" virt="virtual"><scope>nvidia::inferenceserver::client::InferHttpContext</scope><name>AsyncTransfer</name></member>
      <member refid="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1a12cc5d7a5db6437fe9fdc4ccc60874a9" prot="protected" virt="non-virtual"><scope>nvidia::inferenceserver::client::InferHttpContext</scope><name>batch_size_</name></member>
      <member refid="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1a5fde4cd588ca640d2b17e1768406eb1c" prot="protected" virt="non-virtual"><scope>nvidia::inferenceserver::client::InferHttpContext</scope><name>context_stat_</name></member>
      <member refid="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1a1836d4a88fbfa609def01ee59a675f4c" prot="protected" virt="non-virtual"><scope>nvidia::inferenceserver::client::InferHttpContext</scope><name>correlation_id_</name></member>
      <member refid="classnvidia_1_1inferenceserver_1_1client_1_1InferHttpContext_1a7c210c15455fde64a31c6925f3a8b906" prot="public" virt="non-virtual"><scope>nvidia::inferenceserver::client::InferHttpContext</scope><name>Create</name></member>
      <member refid="classnvidia_1_1inferenceserver_1_1client_1_1InferHttpContext_1ab59a20c6f183e55f0791d07e3d8c223b" prot="public" virt="non-virtual"><scope>nvidia::inferenceserver::client::InferHttpContext</scope><name>Create</name></member>
      <member refid="classnvidia_1_1inferenceserver_1_1client_1_1InferHttpContext_1a7ed61d8774275c4b9e0910ead0f42326" prot="private" virt="non-virtual"><scope>nvidia::inferenceserver::client::InferHttpContext</scope><name>curl_</name></member>
      <member refid="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1a685dfb2b9634c050b2d552f1613d0bc0" prot="protected" virt="non-virtual"><scope>nvidia::inferenceserver::client::InferHttpContext</scope><name>cv_</name></member>
      <member refid="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1a5f6d75086a6155fba595351c4e7abdd6" prot="protected" virt="non-virtual"><scope>nvidia::inferenceserver::client::InferHttpContext</scope><name>exiting_</name></member>
      <member refid="classnvidia_1_1inferenceserver_1_1client_1_1InferHttpContext_1a646ec6ca28f8c38a23b4e85c3ad8e8ec" prot="public" virt="virtual"><scope>nvidia::inferenceserver::client::InferHttpContext</scope><name>GetAsyncRunResults</name></member>
      <member refid="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1ac8ca982d9c589b2c27c9a279699ac884" prot="public" virt="non-virtual"><scope>nvidia::inferenceserver::client::InferHttpContext</scope><name>GetInput</name></member>
      <member refid="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1a0d3e9e6e55f3162c7148574af7aec251" prot="public" virt="non-virtual"><scope>nvidia::inferenceserver::client::InferHttpContext</scope><name>GetOutput</name></member>
      <member refid="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1a27512d1f8f7099d20b2e0eaaac90c3f0" prot="public" virt="non-virtual"><scope>nvidia::inferenceserver::client::InferHttpContext</scope><name>GetReadyAsyncRequest</name></member>
      <member refid="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1aa083c81909e7a6d10725306d231ea0cb" prot="public" virt="non-virtual"><scope>nvidia::inferenceserver::client::InferHttpContext</scope><name>GetStat</name></member>
      <member refid="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1ad65a99fa3c6afcf961de531f4421a73d" prot="protected" virt="non-virtual"><scope>nvidia::inferenceserver::client::InferHttpContext</scope><name>infer_request_</name></member>
      <member refid="classnvidia_1_1inferenceserver_1_1client_1_1InferHttpContext_1a2e79bb4348ba903099c7cf57272d6f0e" prot="private" virt="non-virtual"><scope>nvidia::inferenceserver::client::InferHttpContext</scope><name>infer_request_str_</name></member>
      <member refid="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1a1ad0ced372d23d1ed5a0af511ab7aa21" prot="protected" virt="non-virtual"><scope>nvidia::inferenceserver::client::InferHttpContext</scope><name>InferContext</name></member>
      <member refid="classnvidia_1_1inferenceserver_1_1client_1_1InferHttpContext_1ac3b159dca6c95b4b0367bea6705a4a11" prot="private" virt="non-virtual"><scope>nvidia::inferenceserver::client::InferHttpContext</scope><name>InferHttpContext</name></member>
      <member refid="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1a7b98f4f0fa2fe0f948f2a06d91ec92f3" prot="public" virt="non-virtual"><scope>nvidia::inferenceserver::client::InferHttpContext</scope><name>Inputs</name></member>
      <member refid="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1a2959c35ac38d06693ee9e3482f4b344b" prot="protected" virt="non-virtual"><scope>nvidia::inferenceserver::client::InferHttpContext</scope><name>inputs_</name></member>
      <member refid="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1ab7437e029e942557e2d11f8c22ae6d6b" prot="protected" virt="non-virtual"><scope>nvidia::inferenceserver::client::InferHttpContext</scope><name>IsRequestReady</name></member>
      <member refid="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1abfc00d0d8175ac030d881365b7030efd" prot="protected" virt="non-virtual"><scope>nvidia::inferenceserver::client::InferHttpContext</scope><name>max_batch_size_</name></member>
      <member refid="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1ac9bd9ba3234a0c282383cdd939ca16dc" prot="public" virt="non-virtual"><scope>nvidia::inferenceserver::client::InferHttpContext</scope><name>MaxBatchSize</name></member>
      <member refid="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1aca65f4b17c14e3c25350c8af3571f86c" prot="protected" virt="non-virtual"><scope>nvidia::inferenceserver::client::InferHttpContext</scope><name>model_name_</name></member>
      <member refid="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1aea092ac954441e636316644bd94b5979" prot="protected" virt="non-virtual"><scope>nvidia::inferenceserver::client::InferHttpContext</scope><name>model_version_</name></member>
      <member refid="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1a6b01d3aece1500914c773fb9d88cb44b" prot="public" virt="non-virtual"><scope>nvidia::inferenceserver::client::InferHttpContext</scope><name>ModelName</name></member>
      <member refid="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1a88642030870fb82b452a48a1676755d0" prot="public" virt="non-virtual"><scope>nvidia::inferenceserver::client::InferHttpContext</scope><name>ModelVersion</name></member>
      <member refid="classnvidia_1_1inferenceserver_1_1client_1_1InferHttpContext_1a1fb8739cc7fb8021b16207b14122286f" prot="private" virt="non-virtual"><scope>nvidia::inferenceserver::client::InferHttpContext</scope><name>multi_handle_</name></member>
      <member refid="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1a1baafc5376df1a6e16de58633c93b4cc" prot="protected" virt="non-virtual"><scope>nvidia::inferenceserver::client::InferHttpContext</scope><name>mutex_</name></member>
      <member refid="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1aa2dda0b94a39e54ce2d07a3b5b6988b2" prot="protected" virt="non-virtual"><scope>nvidia::inferenceserver::client::InferHttpContext</scope><name>ongoing_async_requests_</name></member>
      <member refid="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1a4a783616ac9f48e21bd89abb69d0b21a" prot="public" virt="non-virtual"><scope>nvidia::inferenceserver::client::InferHttpContext</scope><name>Outputs</name></member>
      <member refid="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1a21c9f59353ef1d4215e033f5a4b5d86a" prot="protected" virt="non-virtual"><scope>nvidia::inferenceserver::client::InferHttpContext</scope><name>outputs_</name></member>
      <member refid="classnvidia_1_1inferenceserver_1_1client_1_1InferHttpContext_1af1a63b2ce2c49270eaab14ed71fd803a" prot="private" virt="virtual"><scope>nvidia::inferenceserver::client::InferHttpContext</scope><name>PreRunProcessing</name></member>
      <member refid="classnvidia_1_1inferenceserver_1_1client_1_1InferHttpContext_1a227f476a78ad517007610268d8867719" prot="private" virt="non-virtual"><scope>nvidia::inferenceserver::client::InferHttpContext</scope><name>RequestProvider</name></member>
      <member refid="classnvidia_1_1inferenceserver_1_1client_1_1InferHttpContext_1ad87b9f3ba4b4516f576a4453fac710ae" prot="private" virt="non-virtual"><scope>nvidia::inferenceserver::client::InferHttpContext</scope><name>ResponseHandler</name></member>
      <member refid="classnvidia_1_1inferenceserver_1_1client_1_1InferHttpContext_1aff39cc0dbd3d152b57c7a24a5a3d9420" prot="private" virt="non-virtual"><scope>nvidia::inferenceserver::client::InferHttpContext</scope><name>ResponseHeaderHandler</name></member>
      <member refid="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1a8540194150819719c190049fab1e125c" prot="public" virt="non-virtual"><scope>nvidia::inferenceserver::client::InferHttpContext</scope><name>ResultMap</name></member>
      <member refid="classnvidia_1_1inferenceserver_1_1client_1_1InferHttpContext_1a9d2e82db48e7a11ea2ee21e39113e267" prot="public" virt="virtual"><scope>nvidia::inferenceserver::client::InferHttpContext</scope><name>Run</name></member>
      <member refid="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1a7cf1746d814561727fd3b07b0f8349f2" prot="public" virt="non-virtual"><scope>nvidia::inferenceserver::client::InferHttpContext</scope><name>SetRunOptions</name></member>
      <member refid="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1a6b2d7784bcf85ee225b574f73d6059ca" prot="protected" virt="non-virtual"><scope>nvidia::inferenceserver::client::InferHttpContext</scope><name>sync_request_</name></member>
      <member refid="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1a11eff835f49b4d9126e1606a3b2c8615" prot="protected" virt="non-virtual"><scope>nvidia::inferenceserver::client::InferHttpContext</scope><name>UpdateStat</name></member>
      <member refid="classnvidia_1_1inferenceserver_1_1client_1_1InferHttpContext_1a6d089cda0592c997c5046167e8a20e75" prot="private" virt="non-virtual"><scope>nvidia::inferenceserver::client::InferHttpContext</scope><name>url_</name></member>
      <member refid="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1a69b72d3b61b07468f5a13c97f36c5e6a" prot="protected" virt="non-virtual"><scope>nvidia::inferenceserver::client::InferHttpContext</scope><name>verbose_</name></member>
      <member refid="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1a39fdbf2a953b23d0005aef8e2618fcaf" prot="protected" virt="non-virtual"><scope>nvidia::inferenceserver::client::InferHttpContext</scope><name>worker_</name></member>
      <member refid="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1a3a996794a2f69c24ab50391fcc36ceec" prot="public" virt="virtual"><scope>nvidia::inferenceserver::client::InferHttpContext</scope><name>~InferContext</name></member>
      <member refid="classnvidia_1_1inferenceserver_1_1client_1_1InferHttpContext_1a0f1905f3c0e275655dadc2ea9fe074c0" prot="public" virt="non-virtual"><scope>nvidia::inferenceserver::client::InferHttpContext</scope><name>~InferHttpContext</name></member>
    </listofallmembers>
  </compounddef>
</doxygen>
