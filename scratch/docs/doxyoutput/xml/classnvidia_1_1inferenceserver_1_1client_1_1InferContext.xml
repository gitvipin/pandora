<?xml version='1.0' encoding='UTF-8' standalone='no'?>
<doxygen xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:noNamespaceSchemaLocation="compound.xsd" version="1.8.11">
  <compounddef id="classnvidia_1_1inferenceserver_1_1client_1_1InferContext" kind="class" language="C++" prot="public" abstract="yes">
    <compoundname>nvidia::inferenceserver::client::InferContext</compoundname>
    <derivedcompoundref refid="classnvidia_1_1inferenceserver_1_1client_1_1InferGrpcContext" prot="public" virt="non-virtual">nvidia::inferenceserver::client::InferGrpcContext</derivedcompoundref>
    <derivedcompoundref refid="classnvidia_1_1inferenceserver_1_1client_1_1InferHttpContext" prot="public" virt="non-virtual">nvidia::inferenceserver::client::InferHttpContext</derivedcompoundref>
    <includes refid="request_8h" local="no">request.h</includes>
    <innerclass refid="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Input" prot="public">nvidia::inferenceserver::client::InferContext::Input</innerclass>
    <innerclass refid="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Options" prot="public">nvidia::inferenceserver::client::InferContext::Options</innerclass>
    <innerclass refid="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Output" prot="public">nvidia::inferenceserver::client::InferContext::Output</innerclass>
    <innerclass refid="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Request" prot="public">nvidia::inferenceserver::client::InferContext::Request</innerclass>
    <innerclass refid="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1RequestTimers" prot="public">nvidia::inferenceserver::client::InferContext::RequestTimers</innerclass>
    <innerclass refid="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result" prot="public">nvidia::inferenceserver::client::InferContext::Result</innerclass>
    <innerclass refid="structnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Stat" prot="public">nvidia::inferenceserver::client::InferContext::Stat</innerclass>
      <sectiondef kind="public-type">
      <memberdef kind="typedef" id="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1a8540194150819719c190049fab1e125c" prot="public" static="no">
        <type>std::map&lt; std::string, std::unique_ptr&lt; <ref refid="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result" kindref="compound">Result</ref> &gt;&gt;</type>
        <definition>using nvidia::inferenceserver::client::InferContext::ResultMap =  std::map&lt;std::string, std::unique_ptr&lt;Result&gt;&gt;</definition>
        <argsstring></argsstring>
        <name>ResultMap</name>
        <briefdescription>
        </briefdescription>
        <detaileddescription>
        </detaileddescription>
        <inbodydescription>
        </inbodydescription>
        <location file="src/clients/c++/request.h" line="601" column="1" bodyfile="src/clients/c++/request.h" bodystart="601" bodyend="-1"/>
      </memberdef>
      </sectiondef>
      <sectiondef kind="protected-type">
      <memberdef kind="typedef" id="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1a6e1d2a28514245af5f71557a18be70c6" prot="protected" static="no">
        <type>std::map&lt; uintptr_t, std::shared_ptr&lt; <ref refid="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Request" kindref="compound">Request</ref> &gt;&gt;</type>
        <definition>using nvidia::inferenceserver::client::InferContext::AsyncReqMap =  std::map&lt;uintptr_t, std::shared_ptr&lt;Request&gt;&gt;</definition>
        <argsstring></argsstring>
        <name>AsyncReqMap</name>
        <briefdescription>
        </briefdescription>
        <detaileddescription>
        </detaileddescription>
        <inbodydescription>
        </inbodydescription>
        <location file="src/clients/c++/request.h" line="711" column="1" bodyfile="src/clients/c++/request.h" bodystart="711" bodyend="-1"/>
      </memberdef>
      </sectiondef>
      <sectiondef kind="protected-attrib">
      <memberdef kind="variable" id="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1aa2dda0b94a39e54ce2d07a3b5b6988b2" prot="protected" static="no" mutable="no">
        <type>AsyncReqMap</type>
        <definition>AsyncReqMap nvidia::inferenceserver::client::InferContext::ongoing_async_requests_</definition>
        <argsstring></argsstring>
        <name>ongoing_async_requests_</name>
        <briefdescription>
        </briefdescription>
        <detaileddescription>
        </detaileddescription>
        <inbodydescription>
        </inbodydescription>
        <location file="src/clients/c++/request.h" line="715" column="1" bodyfile="src/clients/c++/request.h" bodystart="715" bodyend="-1"/>
      </memberdef>
      <memberdef kind="variable" id="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1aca65f4b17c14e3c25350c8af3571f86c" prot="protected" static="no" mutable="no">
        <type>const std::string</type>
        <definition>const std::string nvidia::inferenceserver::client::InferContext::model_name_</definition>
        <argsstring></argsstring>
        <name>model_name_</name>
        <briefdescription>
        </briefdescription>
        <detaileddescription>
        </detaileddescription>
        <inbodydescription>
        </inbodydescription>
        <location file="src/clients/c++/request.h" line="718" column="1" bodyfile="src/clients/c++/request.h" bodystart="718" bodyend="-1"/>
      </memberdef>
      <memberdef kind="variable" id="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1aea092ac954441e636316644bd94b5979" prot="protected" static="no" mutable="no">
        <type>const int64_t</type>
        <definition>const int64_t nvidia::inferenceserver::client::InferContext::model_version_</definition>
        <argsstring></argsstring>
        <name>model_version_</name>
        <briefdescription>
        </briefdescription>
        <detaileddescription>
        </detaileddescription>
        <inbodydescription>
        </inbodydescription>
        <location file="src/clients/c++/request.h" line="721" column="1" bodyfile="src/clients/c++/request.h" bodystart="721" bodyend="-1"/>
      </memberdef>
      <memberdef kind="variable" id="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1a1836d4a88fbfa609def01ee59a675f4c" prot="protected" static="no" mutable="no">
        <type>const CorrelationID</type>
        <definition>const CorrelationID nvidia::inferenceserver::client::InferContext::correlation_id_</definition>
        <argsstring></argsstring>
        <name>correlation_id_</name>
        <briefdescription>
        </briefdescription>
        <detaileddescription>
        </detaileddescription>
        <inbodydescription>
        </inbodydescription>
        <location file="src/clients/c++/request.h" line="725" column="1" bodyfile="src/clients/c++/request.h" bodystart="725" bodyend="-1"/>
      </memberdef>
      <memberdef kind="variable" id="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1a69b72d3b61b07468f5a13c97f36c5e6a" prot="protected" static="no" mutable="no">
        <type>const bool</type>
        <definition>const bool nvidia::inferenceserver::client::InferContext::verbose_</definition>
        <argsstring></argsstring>
        <name>verbose_</name>
        <briefdescription>
        </briefdescription>
        <detaileddescription>
        </detaileddescription>
        <inbodydescription>
        </inbodydescription>
        <location file="src/clients/c++/request.h" line="728" column="1" bodyfile="src/clients/c++/request.h" bodystart="728" bodyend="-1"/>
      </memberdef>
      <memberdef kind="variable" id="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1abfc00d0d8175ac030d881365b7030efd" prot="protected" static="no" mutable="no">
        <type>uint64_t</type>
        <definition>uint64_t nvidia::inferenceserver::client::InferContext::max_batch_size_</definition>
        <argsstring></argsstring>
        <name>max_batch_size_</name>
        <briefdescription>
        </briefdescription>
        <detaileddescription>
        </detaileddescription>
        <inbodydescription>
        </inbodydescription>
        <location file="src/clients/c++/request.h" line="733" column="1" bodyfile="src/clients/c++/request.h" bodystart="733" bodyend="-1"/>
      </memberdef>
      <memberdef kind="variable" id="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1a12cc5d7a5db6437fe9fdc4ccc60874a9" prot="protected" static="no" mutable="no">
        <type>uint64_t</type>
        <definition>uint64_t nvidia::inferenceserver::client::InferContext::batch_size_</definition>
        <argsstring></argsstring>
        <name>batch_size_</name>
        <briefdescription>
        </briefdescription>
        <detaileddescription>
        </detaileddescription>
        <inbodydescription>
        </inbodydescription>
        <location file="src/clients/c++/request.h" line="736" column="1" bodyfile="src/clients/c++/request.h" bodystart="736" bodyend="-1"/>
      </memberdef>
      <memberdef kind="variable" id="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1aeb3741fc43d18b9cd7abab193f7ffca0" prot="protected" static="no" mutable="no">
        <type>uint64_t</type>
        <definition>uint64_t nvidia::inferenceserver::client::InferContext::async_request_id_</definition>
        <argsstring></argsstring>
        <name>async_request_id_</name>
        <briefdescription>
        </briefdescription>
        <detaileddescription>
        </detaileddescription>
        <inbodydescription>
        </inbodydescription>
        <location file="src/clients/c++/request.h" line="739" column="1" bodyfile="src/clients/c++/request.h" bodystart="739" bodyend="-1"/>
      </memberdef>
      <memberdef kind="variable" id="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1a2959c35ac38d06693ee9e3482f4b344b" prot="protected" static="no" mutable="no">
        <type>std::vector&lt; std::shared_ptr&lt; <ref refid="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Input" kindref="compound">Input</ref> &gt; &gt;</type>
        <definition>std::vector&lt;std::shared_ptr&lt;Input&gt; &gt; nvidia::inferenceserver::client::InferContext::inputs_</definition>
        <argsstring></argsstring>
        <name>inputs_</name>
        <briefdescription>
        </briefdescription>
        <detaileddescription>
        </detaileddescription>
        <inbodydescription>
        </inbodydescription>
        <location file="src/clients/c++/request.h" line="742" column="1" bodyfile="src/clients/c++/request.h" bodystart="742" bodyend="-1"/>
      </memberdef>
      <memberdef kind="variable" id="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1a21c9f59353ef1d4215e033f5a4b5d86a" prot="protected" static="no" mutable="no">
        <type>std::vector&lt; std::shared_ptr&lt; <ref refid="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Output" kindref="compound">Output</ref> &gt; &gt;</type>
        <definition>std::vector&lt;std::shared_ptr&lt;Output&gt; &gt; nvidia::inferenceserver::client::InferContext::outputs_</definition>
        <argsstring></argsstring>
        <name>outputs_</name>
        <briefdescription>
        </briefdescription>
        <detaileddescription>
        </detaileddescription>
        <inbodydescription>
        </inbodydescription>
        <location file="src/clients/c++/request.h" line="743" column="1" bodyfile="src/clients/c++/request.h" bodystart="743" bodyend="-1"/>
      </memberdef>
      <memberdef kind="variable" id="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1ad65a99fa3c6afcf961de531f4421a73d" prot="protected" static="no" mutable="no">
        <type>InferRequestHeader</type>
        <definition>InferRequestHeader nvidia::inferenceserver::client::InferContext::infer_request_</definition>
        <argsstring></argsstring>
        <name>infer_request_</name>
        <briefdescription>
        </briefdescription>
        <detaileddescription>
        </detaileddescription>
        <inbodydescription>
        </inbodydescription>
        <location file="src/clients/c++/request.h" line="747" column="1" bodyfile="src/clients/c++/request.h" bodystart="747" bodyend="-1"/>
      </memberdef>
      <memberdef kind="variable" id="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1a6b2d7784bcf85ee225b574f73d6059ca" prot="protected" static="no" mutable="no">
        <type>std::shared_ptr&lt; <ref refid="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Request" kindref="compound">Request</ref> &gt;</type>
        <definition>std::shared_ptr&lt;Request&gt; nvidia::inferenceserver::client::InferContext::sync_request_</definition>
        <argsstring></argsstring>
        <name>sync_request_</name>
        <briefdescription>
        </briefdescription>
        <detaileddescription>
        </detaileddescription>
        <inbodydescription>
        </inbodydescription>
        <location file="src/clients/c++/request.h" line="750" column="1" bodyfile="src/clients/c++/request.h" bodystart="750" bodyend="-1"/>
      </memberdef>
      <memberdef kind="variable" id="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1a5fde4cd588ca640d2b17e1768406eb1c" prot="protected" static="no" mutable="no">
        <type><ref refid="structnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Stat" kindref="compound">Stat</ref></type>
        <definition>Stat nvidia::inferenceserver::client::InferContext::context_stat_</definition>
        <argsstring></argsstring>
        <name>context_stat_</name>
        <briefdescription>
        </briefdescription>
        <detaileddescription>
        </detaileddescription>
        <inbodydescription>
        </inbodydescription>
        <location file="src/clients/c++/request.h" line="753" column="1" bodyfile="src/clients/c++/request.h" bodystart="753" bodyend="-1"/>
      </memberdef>
      <memberdef kind="variable" id="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1a39fdbf2a953b23d0005aef8e2618fcaf" prot="protected" static="no" mutable="no">
        <type>std::thread</type>
        <definition>std::thread nvidia::inferenceserver::client::InferContext::worker_</definition>
        <argsstring></argsstring>
        <name>worker_</name>
        <briefdescription>
        </briefdescription>
        <detaileddescription>
        </detaileddescription>
        <inbodydescription>
        </inbodydescription>
        <location file="src/clients/c++/request.h" line="756" column="1" bodyfile="src/clients/c++/request.h" bodystart="756" bodyend="-1"/>
      </memberdef>
      <memberdef kind="variable" id="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1a1baafc5376df1a6e16de58633c93b4cc" prot="protected" static="no" mutable="no">
        <type>std::mutex</type>
        <definition>std::mutex nvidia::inferenceserver::client::InferContext::mutex_</definition>
        <argsstring></argsstring>
        <name>mutex_</name>
        <briefdescription>
        </briefdescription>
        <detaileddescription>
        </detaileddescription>
        <inbodydescription>
        </inbodydescription>
        <location file="src/clients/c++/request.h" line="759" column="1" bodyfile="src/clients/c++/request.h" bodystart="759" bodyend="-1"/>
      </memberdef>
      <memberdef kind="variable" id="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1a685dfb2b9634c050b2d552f1613d0bc0" prot="protected" static="no" mutable="no">
        <type>std::condition_variable</type>
        <definition>std::condition_variable nvidia::inferenceserver::client::InferContext::cv_</definition>
        <argsstring></argsstring>
        <name>cv_</name>
        <briefdescription>
        </briefdescription>
        <detaileddescription>
        </detaileddescription>
        <inbodydescription>
        </inbodydescription>
        <location file="src/clients/c++/request.h" line="762" column="1" bodyfile="src/clients/c++/request.h" bodystart="762" bodyend="-1"/>
      </memberdef>
      <memberdef kind="variable" id="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1a5f6d75086a6155fba595351c4e7abdd6" prot="protected" static="no" mutable="no">
        <type>bool</type>
        <definition>bool nvidia::inferenceserver::client::InferContext::exiting_</definition>
        <argsstring></argsstring>
        <name>exiting_</name>
        <briefdescription>
        </briefdescription>
        <detaileddescription>
        </detaileddescription>
        <inbodydescription>
        </inbodydescription>
        <location file="src/clients/c++/request.h" line="765" column="1" bodyfile="src/clients/c++/request.h" bodystart="765" bodyend="-1"/>
      </memberdef>
      </sectiondef>
      <sectiondef kind="public-func">
      <memberdef kind="function" id="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1a3a996794a2f69c24ab50391fcc36ceec" prot="public" static="no" const="no" explicit="no" inline="no" virt="virtual">
        <type></type>
        <definition>virtual nvidia::inferenceserver::client::InferContext::~InferContext</definition>
        <argsstring>()=default</argsstring>
        <name>~InferContext</name>
        <briefdescription>
<para>Destroy the inference context. </para>        </briefdescription>
        <detaileddescription>
        </detaileddescription>
        <inbodydescription>
        </inbodydescription>
        <location file="src/clients/c++/request.h" line="604" column="1"/>
      </memberdef>
      <memberdef kind="function" id="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1a6b01d3aece1500914c773fb9d88cb44b" prot="public" static="no" const="yes" explicit="no" inline="yes" virt="non-virtual">
        <type>const std::string &amp;</type>
        <definition>const std::string&amp; nvidia::inferenceserver::client::InferContext::ModelName</definition>
        <argsstring>() const </argsstring>
        <name>ModelName</name>
        <briefdescription>
        </briefdescription>
        <detaileddescription>
<para><simplesect kind="return"><para>The name of the model being used for this context. </para></simplesect>
</para>        </detaileddescription>
        <inbodydescription>
        </inbodydescription>
        <location file="src/clients/c++/request.h" line="607" column="1" bodyfile="src/clients/c++/request.h" bodystart="607" bodyend="607"/>
      </memberdef>
      <memberdef kind="function" id="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1a88642030870fb82b452a48a1676755d0" prot="public" static="no" const="yes" explicit="no" inline="yes" virt="non-virtual">
        <type>int64_t</type>
        <definition>int64_t nvidia::inferenceserver::client::InferContext::ModelVersion</definition>
        <argsstring>() const </argsstring>
        <name>ModelVersion</name>
        <briefdescription>
        </briefdescription>
        <detaileddescription>
<para><simplesect kind="return"><para>The version of the model being used for this context. -1 indicates that the latest (i.e. highest version number) version of that model is being used. </para></simplesect>
</para>        </detaileddescription>
        <inbodydescription>
        </inbodydescription>
        <location file="src/clients/c++/request.h" line="612" column="1" bodyfile="src/clients/c++/request.h" bodystart="612" bodyend="612"/>
      </memberdef>
      <memberdef kind="function" id="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1ac9bd9ba3234a0c282383cdd939ca16dc" prot="public" static="no" const="yes" explicit="no" inline="yes" virt="non-virtual">
        <type>uint64_t</type>
        <definition>uint64_t nvidia::inferenceserver::client::InferContext::MaxBatchSize</definition>
        <argsstring>() const </argsstring>
        <name>MaxBatchSize</name>
        <briefdescription>
        </briefdescription>
        <detaileddescription>
<para><simplesect kind="return"><para>The maximum batch size supported by the context. A maximum batch size indicates that the context does not support batching and so only a single inference at a time can be performed. </para></simplesect>
</para>        </detaileddescription>
        <inbodydescription>
        </inbodydescription>
        <location file="src/clients/c++/request.h" line="618" column="1" bodyfile="src/clients/c++/request.h" bodystart="618" bodyend="618"/>
      </memberdef>
      <memberdef kind="function" id="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1a7b98f4f0fa2fe0f948f2a06d91ec92f3" prot="public" static="no" const="yes" explicit="no" inline="yes" virt="non-virtual">
        <type>const std::vector&lt; std::shared_ptr&lt; <ref refid="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Input" kindref="compound">Input</ref> &gt; &gt; &amp;</type>
        <definition>const std::vector&lt;std::shared_ptr&lt;Input&gt; &gt;&amp; nvidia::inferenceserver::client::InferContext::Inputs</definition>
        <argsstring>() const </argsstring>
        <name>Inputs</name>
        <briefdescription>
        </briefdescription>
        <detaileddescription>
<para><simplesect kind="return"><para>The inputs of the model. </para></simplesect>
</para>        </detaileddescription>
        <inbodydescription>
        </inbodydescription>
        <location file="src/clients/c++/request.h" line="621" column="1" bodyfile="src/clients/c++/request.h" bodystart="621" bodyend="621"/>
      </memberdef>
      <memberdef kind="function" id="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1a4a783616ac9f48e21bd89abb69d0b21a" prot="public" static="no" const="yes" explicit="no" inline="yes" virt="non-virtual">
        <type>const std::vector&lt; std::shared_ptr&lt; <ref refid="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Output" kindref="compound">Output</ref> &gt; &gt; &amp;</type>
        <definition>const std::vector&lt;std::shared_ptr&lt;Output&gt; &gt;&amp; nvidia::inferenceserver::client::InferContext::Outputs</definition>
        <argsstring>() const </argsstring>
        <name>Outputs</name>
        <briefdescription>
        </briefdescription>
        <detaileddescription>
<para><simplesect kind="return"><para>The outputs of the model. </para></simplesect>
</para>        </detaileddescription>
        <inbodydescription>
        </inbodydescription>
        <location file="src/clients/c++/request.h" line="624" column="1" bodyfile="src/clients/c++/request.h" bodystart="624" bodyend="627"/>
      </memberdef>
      <memberdef kind="function" id="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1ac8ca982d9c589b2c27c9a279699ac884" prot="public" static="no" const="yes" explicit="no" inline="no" virt="non-virtual">
        <type><ref refid="classnvidia_1_1inferenceserver_1_1client_1_1Error" kindref="compound">Error</ref></type>
        <definition>Error nvidia::inferenceserver::client::InferContext::GetInput</definition>
        <argsstring>(const std::string &amp;name, std::shared_ptr&lt; Input &gt; *input) const </argsstring>
        <name>GetInput</name>
        <param>
          <type>const std::string &amp;</type>
          <declname>name</declname>
        </param>
        <param>
          <type>std::shared_ptr&lt; <ref refid="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Input" kindref="compound">Input</ref> &gt; *</type>
          <declname>input</declname>
        </param>
        <briefdescription>
<para>Get a named input. </para>        </briefdescription>
        <detaileddescription>
<para><parameterlist kind="param"><parameteritem>
<parameternamelist>
<parametername>name</parametername>
</parameternamelist>
<parameterdescription>
<para>The name of the input. </para></parameterdescription>
</parameteritem>
<parameteritem>
<parameternamelist>
<parametername>input</parametername>
</parameternamelist>
<parameterdescription>
<para>Returns the <ref refid="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Input" kindref="compound">Input</ref> object for &apos;name&apos;. </para></parameterdescription>
</parameteritem>
</parameterlist>
<simplesect kind="return"><para><ref refid="classnvidia_1_1inferenceserver_1_1client_1_1Error" kindref="compound">Error</ref> object indicating success or failure. </para></simplesect>
</para>        </detaileddescription>
        <inbodydescription>
        </inbodydescription>
        <location file="src/clients/c++/request.h" line="633" column="1"/>
      </memberdef>
      <memberdef kind="function" id="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1a0d3e9e6e55f3162c7148574af7aec251" prot="public" static="no" const="yes" explicit="no" inline="no" virt="non-virtual">
        <type><ref refid="classnvidia_1_1inferenceserver_1_1client_1_1Error" kindref="compound">Error</ref></type>
        <definition>Error nvidia::inferenceserver::client::InferContext::GetOutput</definition>
        <argsstring>(const std::string &amp;name, std::shared_ptr&lt; Output &gt; *output) const </argsstring>
        <name>GetOutput</name>
        <param>
          <type>const std::string &amp;</type>
          <declname>name</declname>
        </param>
        <param>
          <type>std::shared_ptr&lt; <ref refid="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Output" kindref="compound">Output</ref> &gt; *</type>
          <declname>output</declname>
        </param>
        <briefdescription>
<para>Get a named output. </para>        </briefdescription>
        <detaileddescription>
<para><parameterlist kind="param"><parameteritem>
<parameternamelist>
<parametername>name</parametername>
</parameternamelist>
<parameterdescription>
<para>The name of the output. </para></parameterdescription>
</parameteritem>
<parameteritem>
<parameternamelist>
<parametername>output</parametername>
</parameternamelist>
<parameterdescription>
<para>Returns the <ref refid="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Output" kindref="compound">Output</ref> object for &apos;name&apos;. </para></parameterdescription>
</parameteritem>
</parameterlist>
<simplesect kind="return"><para><ref refid="classnvidia_1_1inferenceserver_1_1client_1_1Error" kindref="compound">Error</ref> object indicating success or failure. </para></simplesect>
</para>        </detaileddescription>
        <inbodydescription>
        </inbodydescription>
        <location file="src/clients/c++/request.h" line="639" column="1"/>
      </memberdef>
      <memberdef kind="function" id="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1a7cf1746d814561727fd3b07b0f8349f2" prot="public" static="no" const="no" explicit="no" inline="no" virt="non-virtual">
        <type><ref refid="classnvidia_1_1inferenceserver_1_1client_1_1Error" kindref="compound">Error</ref></type>
        <definition>Error nvidia::inferenceserver::client::InferContext::SetRunOptions</definition>
        <argsstring>(const Options &amp;options)</argsstring>
        <name>SetRunOptions</name>
        <param>
          <type>const <ref refid="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Options" kindref="compound">Options</ref> &amp;</type>
          <declname>options</declname>
        </param>
        <briefdescription>
<para>Set the options to use for all subsequent <ref refid="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1a4cac8002817c9259851305b14c258f36" kindref="member">Run()</ref> invocations. </para>        </briefdescription>
        <detaileddescription>
<para><parameterlist kind="param"><parameteritem>
<parameternamelist>
<parametername>options</parametername>
</parameternamelist>
<parameterdescription>
<para>The options. </para></parameterdescription>
</parameteritem>
</parameterlist>
<simplesect kind="return"><para><ref refid="classnvidia_1_1inferenceserver_1_1client_1_1Error" kindref="compound">Error</ref> object indicating success or failure. </para></simplesect>
</para>        </detaileddescription>
        <inbodydescription>
        </inbodydescription>
        <location file="src/clients/c++/request.h" line="645" column="1"/>
      </memberdef>
      <memberdef kind="function" id="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1aa083c81909e7a6d10725306d231ea0cb" prot="public" static="no" const="no" explicit="no" inline="no" virt="non-virtual">
        <type><ref refid="classnvidia_1_1inferenceserver_1_1client_1_1Error" kindref="compound">Error</ref></type>
        <definition>Error nvidia::inferenceserver::client::InferContext::GetStat</definition>
        <argsstring>(Stat *stat)</argsstring>
        <name>GetStat</name>
        <param>
          <type><ref refid="structnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Stat" kindref="compound">Stat</ref> *</type>
          <declname>stat</declname>
        </param>
        <briefdescription>
<para>Get the current statistics of the <ref refid="classnvidia_1_1inferenceserver_1_1client_1_1InferContext" kindref="compound">InferContext</ref>. </para>        </briefdescription>
        <detaileddescription>
<para><parameterlist kind="param"><parameteritem>
<parameternamelist>
<parametername>stat</parametername>
</parameternamelist>
<parameterdescription>
<para>Returns the <ref refid="structnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Stat" kindref="compound">Stat</ref> object holding the statistics. </para></parameterdescription>
</parameteritem>
</parameterlist>
<simplesect kind="return"><para><ref refid="classnvidia_1_1inferenceserver_1_1client_1_1Error" kindref="compound">Error</ref> object indicating success or failure. </para></simplesect>
</para>        </detaileddescription>
        <inbodydescription>
        </inbodydescription>
        <location file="src/clients/c++/request.h" line="650" column="1"/>
      </memberdef>
      <memberdef kind="function" id="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1a4cac8002817c9259851305b14c258f36" prot="public" static="no" const="no" explicit="no" inline="no" virt="pure-virtual">
        <type><ref refid="classnvidia_1_1inferenceserver_1_1client_1_1Error" kindref="compound">Error</ref></type>
        <definition>virtual Error nvidia::inferenceserver::client::InferContext::Run</definition>
        <argsstring>(ResultMap *results)=0</argsstring>
        <name>Run</name>
        <reimplementedby refid="classnvidia_1_1inferenceserver_1_1client_1_1InferHttpContext_1a9d2e82db48e7a11ea2ee21e39113e267">Run</reimplementedby>
        <reimplementedby refid="classnvidia_1_1inferenceserver_1_1client_1_1InferGrpcContext_1a17c4f2f5007867d9f6b5444f9088f6c5">Run</reimplementedby>
        <reimplementedby refid="classnvidia_1_1inferenceserver_1_1client_1_1InferGrpcStreamContext_1a5e42ef49aa0ed810934a18ca1dbb71e4">Run</reimplementedby>
        <param>
          <type>ResultMap *</type>
          <declname>results</declname>
        </param>
        <briefdescription>
<para>Send a synchronous request to the inference server to perform an inference to produce results for the outputs specified in the most recent call to <ref refid="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1a7cf1746d814561727fd3b07b0f8349f2" kindref="member">SetRunOptions()</ref>. </para>        </briefdescription>
        <detaileddescription>
<para><parameterlist kind="param"><parameteritem>
<parameternamelist>
<parametername>results</parametername>
</parameternamelist>
<parameterdescription>
<para>Returns <ref refid="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result" kindref="compound">Result</ref> objects holding inference results as a map from output name to <ref refid="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result" kindref="compound">Result</ref> object. </para></parameterdescription>
</parameteritem>
</parameterlist>
<simplesect kind="return"><para><ref refid="classnvidia_1_1inferenceserver_1_1client_1_1Error" kindref="compound">Error</ref> object indicating success or failure. </para></simplesect>
</para>        </detaileddescription>
        <inbodydescription>
        </inbodydescription>
        <location file="src/clients/c++/request.h" line="658" column="1"/>
      </memberdef>
      <memberdef kind="function" id="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1a255a59df29a53d6e45ce86663887615f" prot="public" static="no" const="no" explicit="no" inline="no" virt="pure-virtual">
        <type><ref refid="classnvidia_1_1inferenceserver_1_1client_1_1Error" kindref="compound">Error</ref></type>
        <definition>virtual Error nvidia::inferenceserver::client::InferContext::AsyncRun</definition>
        <argsstring>(std::shared_ptr&lt; Request &gt; *async_request)=0</argsstring>
        <name>AsyncRun</name>
        <reimplementedby refid="classnvidia_1_1inferenceserver_1_1client_1_1InferHttpContext_1a625d8c3a79b6d3cf08fd442b0d0b2540">AsyncRun</reimplementedby>
        <reimplementedby refid="classnvidia_1_1inferenceserver_1_1client_1_1InferGrpcContext_1adcd341f23d4b3a609c2117c698522a31">AsyncRun</reimplementedby>
        <reimplementedby refid="classnvidia_1_1inferenceserver_1_1client_1_1InferGrpcStreamContext_1a86e4987e666b37c5493373ae1b068d2f">AsyncRun</reimplementedby>
        <param>
          <type>std::shared_ptr&lt; <ref refid="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Request" kindref="compound">Request</ref> &gt; *</type>
          <declname>async_request</declname>
        </param>
        <briefdescription>
<para>Send an asynchronous request to the inference server to perform an inference to produce results for the outputs specified in the most recent call to <ref refid="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1a7cf1746d814561727fd3b07b0f8349f2" kindref="member">SetRunOptions()</ref>. </para>        </briefdescription>
        <detaileddescription>
<para><parameterlist kind="param"><parameteritem>
<parameternamelist>
<parametername>async_request</parametername>
</parameternamelist>
<parameterdescription>
<para>Returns a <ref refid="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Request" kindref="compound">Request</ref> object that can be used to retrieve the inference results for the request. </para></parameterdescription>
</parameteritem>
</parameterlist>
<simplesect kind="return"><para><ref refid="classnvidia_1_1inferenceserver_1_1client_1_1Error" kindref="compound">Error</ref> object indicating success or failure. </para></simplesect>
</para>        </detaileddescription>
        <inbodydescription>
        </inbodydescription>
        <location file="src/clients/c++/request.h" line="666" column="1"/>
      </memberdef>
      <memberdef kind="function" id="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1a6a9a45e35031ce4b0b50af8bd0c4ee8c" prot="public" static="no" const="no" explicit="no" inline="no" virt="pure-virtual">
        <type><ref refid="classnvidia_1_1inferenceserver_1_1client_1_1Error" kindref="compound">Error</ref></type>
        <definition>virtual Error nvidia::inferenceserver::client::InferContext::GetAsyncRunResults</definition>
        <argsstring>(ResultMap *results, const std::shared_ptr&lt; Request &gt; &amp;async_request, bool wait)=0</argsstring>
        <name>GetAsyncRunResults</name>
        <reimplementedby refid="classnvidia_1_1inferenceserver_1_1client_1_1InferHttpContext_1a646ec6ca28f8c38a23b4e85c3ad8e8ec">GetAsyncRunResults</reimplementedby>
        <reimplementedby refid="classnvidia_1_1inferenceserver_1_1client_1_1InferGrpcContext_1af701c02c211163d37ac940a9816cf9ab">GetAsyncRunResults</reimplementedby>
        <param>
          <type>ResultMap *</type>
          <declname>results</declname>
        </param>
        <param>
          <type>const std::shared_ptr&lt; <ref refid="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Request" kindref="compound">Request</ref> &gt; &amp;</type>
          <declname>async_request</declname>
        </param>
        <param>
          <type>bool</type>
          <declname>wait</declname>
        </param>
        <briefdescription>
<para>Get the results of the asynchronous request referenced by &apos;async_request&apos;. </para>        </briefdescription>
        <detaileddescription>
<para><parameterlist kind="param"><parameteritem>
<parameternamelist>
<parametername>results</parametername>
</parameternamelist>
<parameterdescription>
<para>Returns <ref refid="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result" kindref="compound">Result</ref> objects holding inference results as a map from output name to <ref refid="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result" kindref="compound">Result</ref> object. </para></parameterdescription>
</parameteritem>
<parameteritem>
<parameternamelist>
<parametername>async_request</parametername>
</parameternamelist>
<parameterdescription>
<para><ref refid="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Request" kindref="compound">Request</ref> handle to retrieve results. </para></parameterdescription>
</parameteritem>
<parameteritem>
<parameternamelist>
<parametername>wait</parametername>
</parameternamelist>
<parameterdescription>
<para>If true, block until the request completes. Otherwise, return immediately. </para></parameterdescription>
</parameteritem>
</parameterlist>
<simplesect kind="return"><para><ref refid="classnvidia_1_1inferenceserver_1_1client_1_1Error" kindref="compound">Error</ref> object indicating success or failure. Success will be returned only if the request has been completed succesfully. UNAVAILABLE will be returned if &apos;wait&apos; is false and the request is not ready. </para></simplesect>
</para>        </detaileddescription>
        <inbodydescription>
        </inbodydescription>
        <location file="src/clients/c++/request.h" line="678" column="1"/>
      </memberdef>
      <memberdef kind="function" id="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1a27512d1f8f7099d20b2e0eaaac90c3f0" prot="public" static="no" const="no" explicit="no" inline="no" virt="non-virtual">
        <type><ref refid="classnvidia_1_1inferenceserver_1_1client_1_1Error" kindref="compound">Error</ref></type>
        <definition>Error nvidia::inferenceserver::client::InferContext::GetReadyAsyncRequest</definition>
        <argsstring>(std::shared_ptr&lt; Request &gt; *async_request, bool wait)</argsstring>
        <name>GetReadyAsyncRequest</name>
        <param>
          <type>std::shared_ptr&lt; <ref refid="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Request" kindref="compound">Request</ref> &gt; *</type>
          <declname>async_request</declname>
        </param>
        <param>
          <type>bool</type>
          <declname>wait</declname>
        </param>
        <briefdescription>
<para>Get any one completed asynchronous request. </para>        </briefdescription>
        <detaileddescription>
<para><parameterlist kind="param"><parameteritem>
<parameternamelist>
<parametername>async_request</parametername>
</parameternamelist>
<parameterdescription>
<para>Returns the <ref refid="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Request" kindref="compound">Request</ref> object holding the completed request. </para></parameterdescription>
</parameteritem>
<parameteritem>
<parameternamelist>
<parametername>wait</parametername>
</parameternamelist>
<parameterdescription>
<para>If true, block until the request completes. Otherwise, return immediately. </para></parameterdescription>
</parameteritem>
</parameterlist>
<simplesect kind="return"><para><ref refid="classnvidia_1_1inferenceserver_1_1client_1_1Error" kindref="compound">Error</ref> object indicating success or failure. Success will be returned only if a completed request was returned.. UNAVAILABLE will be returned if &apos;wait&apos; is false and no request is ready. </para></simplesect>
</para>        </detaileddescription>
        <inbodydescription>
        </inbodydescription>
        <location file="src/clients/c++/request.h" line="690" column="1"/>
      </memberdef>
      </sectiondef>
      <sectiondef kind="protected-func">
      <memberdef kind="function" id="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1a1ad0ced372d23d1ed5a0af511ab7aa21" prot="protected" static="no" const="no" explicit="no" inline="no" virt="non-virtual">
        <type></type>
        <definition>nvidia::inferenceserver::client::InferContext::InferContext</definition>
        <argsstring>(const std::string &amp;, int64_t, CorrelationID, bool)</argsstring>
        <name>InferContext</name>
        <param>
          <type>const std::string &amp;</type>
        </param>
        <param>
          <type>int64_t</type>
        </param>
        <param>
          <type>CorrelationID</type>
        </param>
        <param>
          <type>bool</type>
        </param>
        <briefdescription>
        </briefdescription>
        <detaileddescription>
        </detaileddescription>
        <inbodydescription>
        </inbodydescription>
        <location file="src/clients/c++/request.h" line="694" column="1"/>
      </memberdef>
      <memberdef kind="function" id="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1ae32e8e83bf498b4e91452703dd12cca0" prot="protected" static="no" const="no" explicit="no" inline="no" virt="pure-virtual">
        <type>void</type>
        <definition>virtual void nvidia::inferenceserver::client::InferContext::AsyncTransfer</definition>
        <argsstring>()=0</argsstring>
        <name>AsyncTransfer</name>
        <reimplementedby refid="classnvidia_1_1inferenceserver_1_1client_1_1InferHttpContext_1abb46a65e4952b7fa15c575dedae8f54c">AsyncTransfer</reimplementedby>
        <reimplementedby refid="classnvidia_1_1inferenceserver_1_1client_1_1InferGrpcContext_1a3f4f41c57b4e74c9ad6e8803c2514ec0">AsyncTransfer</reimplementedby>
        <reimplementedby refid="classnvidia_1_1inferenceserver_1_1client_1_1InferGrpcStreamContext_1ac0b79bc8230741c954ba5901e0ea330c">AsyncTransfer</reimplementedby>
        <briefdescription>
        </briefdescription>
        <detaileddescription>
        </detaileddescription>
        <inbodydescription>
        </inbodydescription>
        <location file="src/clients/c++/request.h" line="697" column="1"/>
      </memberdef>
      <memberdef kind="function" id="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1ad2c7af868ffbb1a2e24f47d9ae0093a5" prot="protected" static="no" const="no" explicit="no" inline="no" virt="pure-virtual">
        <type><ref refid="classnvidia_1_1inferenceserver_1_1client_1_1Error" kindref="compound">Error</ref></type>
        <definition>virtual Error nvidia::inferenceserver::client::InferContext::PreRunProcessing</definition>
        <argsstring>(std::shared_ptr&lt; Request &gt; &amp;request)=0</argsstring>
        <name>PreRunProcessing</name>
        <reimplementedby refid="classnvidia_1_1inferenceserver_1_1client_1_1InferHttpContext_1af1a63b2ce2c49270eaab14ed71fd803a">PreRunProcessing</reimplementedby>
        <reimplementedby refid="classnvidia_1_1inferenceserver_1_1client_1_1InferGrpcContext_1a5902b0ce9b16f0a055bbc0fc09c69031">PreRunProcessing</reimplementedby>
        <param>
          <type>std::shared_ptr&lt; <ref refid="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Request" kindref="compound">Request</ref> &gt; &amp;</type>
          <declname>request</declname>
        </param>
        <briefdescription>
        </briefdescription>
        <detaileddescription>
        </detaileddescription>
        <inbodydescription>
        </inbodydescription>
        <location file="src/clients/c++/request.h" line="700" column="1"/>
      </memberdef>
      <memberdef kind="function" id="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1ab7437e029e942557e2d11f8c22ae6d6b" prot="protected" static="no" const="no" explicit="no" inline="no" virt="non-virtual">
        <type><ref refid="classnvidia_1_1inferenceserver_1_1client_1_1Error" kindref="compound">Error</ref></type>
        <definition>Error nvidia::inferenceserver::client::InferContext::IsRequestReady</definition>
        <argsstring>(const std::shared_ptr&lt; Request &gt; &amp;async_request, bool wait)</argsstring>
        <name>IsRequestReady</name>
        <param>
          <type>const std::shared_ptr&lt; <ref refid="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Request" kindref="compound">Request</ref> &gt; &amp;</type>
          <declname>async_request</declname>
        </param>
        <param>
          <type>bool</type>
          <declname>wait</declname>
        </param>
        <briefdescription>
        </briefdescription>
        <detaileddescription>
        </detaileddescription>
        <inbodydescription>
        </inbodydescription>
        <location file="src/clients/c++/request.h" line="705" column="1"/>
      </memberdef>
      <memberdef kind="function" id="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1a11eff835f49b4d9126e1606a3b2c8615" prot="protected" static="no" const="no" explicit="no" inline="no" virt="non-virtual">
        <type><ref refid="classnvidia_1_1inferenceserver_1_1client_1_1Error" kindref="compound">Error</ref></type>
        <definition>Error nvidia::inferenceserver::client::InferContext::UpdateStat</definition>
        <argsstring>(const RequestTimers &amp;timer)</argsstring>
        <name>UpdateStat</name>
        <param>
          <type>const <ref refid="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1RequestTimers" kindref="compound">RequestTimers</ref> &amp;</type>
          <declname>timer</declname>
        </param>
        <briefdescription>
        </briefdescription>
        <detaileddescription>
        </detaileddescription>
        <inbodydescription>
        </inbodydescription>
        <location file="src/clients/c++/request.h" line="709" column="1"/>
      </memberdef>
      </sectiondef>
    <briefdescription>
<para>An <ref refid="classnvidia_1_1inferenceserver_1_1client_1_1InferContext" kindref="compound">InferContext</ref> object is used to run inference on an inference server for a specific model. </para>    </briefdescription>
    <detaileddescription>
<para>Once created an <ref refid="classnvidia_1_1inferenceserver_1_1client_1_1InferContext" kindref="compound">InferContext</ref> object can be used repeatedly to perform inference using the model. <ref refid="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Options" kindref="compound">Options</ref> that control how inference is performed can be changed in between inference runs.</para><para>A <ref refid="classnvidia_1_1inferenceserver_1_1client_1_1InferContext" kindref="compound">InferContext</ref> object can use either HTTP protocol or GRPC protocol depending on the Create function (<ref refid="classnvidia_1_1inferenceserver_1_1client_1_1InferHttpContext_1a7c210c15455fde64a31c6925f3a8b906" kindref="member">InferHttpContext::Create</ref> or <ref refid="classnvidia_1_1inferenceserver_1_1client_1_1InferGrpcContext_1a432bf0981eb512786263764f090ee83d" kindref="member">InferGrpcContext::Create</ref>). For example:</para><para><programlisting><codeline><highlight class="normal">std::unique_ptr&lt;InferContext&gt;<sp/>ctx;</highlight></codeline>
<codeline><highlight class="normal">InferHttpContext::Create(&amp;ctx,<sp/>&quot;localhost:8000&quot;,<sp/>&quot;mnist&quot;);</highlight></codeline>
<codeline><highlight class="normal">...</highlight></codeline>
<codeline><highlight class="normal">std::unique_ptr&lt;Options&gt;<sp/>options0;</highlight></codeline>
<codeline><highlight class="normal">Options::Create(&amp;options0);</highlight></codeline>
<codeline><highlight class="normal">options-&gt;SetBatchSize(b);</highlight></codeline>
<codeline><highlight class="normal">options-&gt;AddClassResult(output,<sp/>topk);</highlight></codeline>
<codeline><highlight class="normal">ctx-&gt;SetRunOptions(*options0);</highlight></codeline>
<codeline><highlight class="normal">...</highlight></codeline>
<codeline><highlight class="normal">ctx-&gt;Run(&amp;results0);<sp/><sp/>//<sp/>run<sp/>using<sp/>options0</highlight></codeline>
<codeline><highlight class="normal">ctx-&gt;Run(&amp;results1);<sp/><sp/>//<sp/>run<sp/>using<sp/>options0</highlight></codeline>
<codeline><highlight class="normal">...</highlight></codeline>
<codeline><highlight class="normal">std::unique_ptr&lt;Options&gt;<sp/>options1;</highlight></codeline>
<codeline><highlight class="normal">Options::Create(&amp;options1);</highlight></codeline>
<codeline><highlight class="normal">options-&gt;AddRawResult(output);</highlight></codeline>
<codeline><highlight class="normal">ctx-&gt;SetRunOptions(*options);</highlight></codeline>
<codeline><highlight class="normal">...</highlight></codeline>
<codeline><highlight class="normal">ctx-&gt;Run(&amp;results2);<sp/><sp/>//<sp/>run<sp/>using<sp/>options1</highlight></codeline>
<codeline><highlight class="normal">ctx-&gt;Run(&amp;results3);<sp/><sp/>//<sp/>run<sp/>using<sp/>options1</highlight></codeline>
<codeline><highlight class="normal">...</highlight></codeline>
</programlisting></para><para><simplesect kind="note"><para>InferContext::Create methods are thread-safe. All other <ref refid="classnvidia_1_1inferenceserver_1_1client_1_1InferContext" kindref="compound">InferContext</ref> methods, and nested class methods are not thread-safe. </para></simplesect>
<simplesect kind="par"><title></title><para>The <ref refid="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1a4cac8002817c9259851305b14c258f36" kindref="member">Run()</ref> calls are not thread-safe but a new <ref refid="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1a4cac8002817c9259851305b14c258f36" kindref="member">Run()</ref> can be invoked as soon as the previous completes. The returned result objects are owned by the caller and may be retained and accessed even after the <ref refid="classnvidia_1_1inferenceserver_1_1client_1_1InferContext" kindref="compound">InferContext</ref> object is destroyed. </para></simplesect>
<simplesect kind="par"><title></title><para><ref refid="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1a255a59df29a53d6e45ce86663887615f" kindref="member">AsyncRun()</ref> and GetAsyncRunStatus() calls are not thread-safe. What&apos;s more, calling one method while the other one is running will result in undefined behavior given that they will modify the shared data internally. </para></simplesect>
<simplesect kind="par"><title></title><para>For more parallelism multiple <ref refid="classnvidia_1_1inferenceserver_1_1client_1_1InferContext" kindref="compound">InferContext</ref> objects can access the same inference server with no serialization requirements across those objects.  </para></simplesect>
</para>    </detaileddescription>
    <inheritancegraph>
      <node id="86">
        <label>nvidia::inferenceserver::client::InferContext</label>
        <link refid="classnvidia_1_1inferenceserver_1_1client_1_1InferContext"/>
      </node>
      <node id="87">
        <label>nvidia::inferenceserver::client::InferGrpcContext</label>
        <link refid="classnvidia_1_1inferenceserver_1_1client_1_1InferGrpcContext"/>
        <childnode refid="86" relation="public-inheritance">
        </childnode>
      </node>
      <node id="89">
        <label>nvidia::inferenceserver::client::InferHttpContext</label>
        <link refid="classnvidia_1_1inferenceserver_1_1client_1_1InferHttpContext"/>
        <childnode refid="86" relation="public-inheritance">
        </childnode>
      </node>
      <node id="88">
        <label>nvidia::inferenceserver::client::InferGrpcStreamContext</label>
        <link refid="classnvidia_1_1inferenceserver_1_1client_1_1InferGrpcStreamContext"/>
        <childnode refid="87" relation="public-inheritance">
        </childnode>
      </node>
    </inheritancegraph>
    <collaborationgraph>
      <node id="91">
        <label>nvidia::inferenceserver::client::InferContext::Stat</label>
        <link refid="structnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Stat"/>
      </node>
      <node id="90">
        <label>nvidia::inferenceserver::client::InferContext</label>
        <link refid="classnvidia_1_1inferenceserver_1_1client_1_1InferContext"/>
        <childnode refid="91" relation="usage">
          <edgelabel>context_stat_</edgelabel>
        </childnode>
      </node>
    </collaborationgraph>
    <location file="src/clients/c++/request.h" line="242" column="1" bodyfile="src/clients/c++/request.h" bodystart="242" bodyend="766"/>
    <listofallmembers>
      <member refid="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1aeb3741fc43d18b9cd7abab193f7ffca0" prot="protected" virt="non-virtual"><scope>nvidia::inferenceserver::client::InferContext</scope><name>async_request_id_</name></member>
      <member refid="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1a6e1d2a28514245af5f71557a18be70c6" prot="protected" virt="non-virtual"><scope>nvidia::inferenceserver::client::InferContext</scope><name>AsyncReqMap</name></member>
      <member refid="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1a255a59df29a53d6e45ce86663887615f" prot="public" virt="pure-virtual"><scope>nvidia::inferenceserver::client::InferContext</scope><name>AsyncRun</name></member>
      <member refid="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1ae32e8e83bf498b4e91452703dd12cca0" prot="protected" virt="pure-virtual"><scope>nvidia::inferenceserver::client::InferContext</scope><name>AsyncTransfer</name></member>
      <member refid="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1a12cc5d7a5db6437fe9fdc4ccc60874a9" prot="protected" virt="non-virtual"><scope>nvidia::inferenceserver::client::InferContext</scope><name>batch_size_</name></member>
      <member refid="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1a5fde4cd588ca640d2b17e1768406eb1c" prot="protected" virt="non-virtual"><scope>nvidia::inferenceserver::client::InferContext</scope><name>context_stat_</name></member>
      <member refid="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1a1836d4a88fbfa609def01ee59a675f4c" prot="protected" virt="non-virtual"><scope>nvidia::inferenceserver::client::InferContext</scope><name>correlation_id_</name></member>
      <member refid="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1a685dfb2b9634c050b2d552f1613d0bc0" prot="protected" virt="non-virtual"><scope>nvidia::inferenceserver::client::InferContext</scope><name>cv_</name></member>
      <member refid="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1a5f6d75086a6155fba595351c4e7abdd6" prot="protected" virt="non-virtual"><scope>nvidia::inferenceserver::client::InferContext</scope><name>exiting_</name></member>
      <member refid="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1a6a9a45e35031ce4b0b50af8bd0c4ee8c" prot="public" virt="pure-virtual"><scope>nvidia::inferenceserver::client::InferContext</scope><name>GetAsyncRunResults</name></member>
      <member refid="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1ac8ca982d9c589b2c27c9a279699ac884" prot="public" virt="non-virtual"><scope>nvidia::inferenceserver::client::InferContext</scope><name>GetInput</name></member>
      <member refid="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1a0d3e9e6e55f3162c7148574af7aec251" prot="public" virt="non-virtual"><scope>nvidia::inferenceserver::client::InferContext</scope><name>GetOutput</name></member>
      <member refid="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1a27512d1f8f7099d20b2e0eaaac90c3f0" prot="public" virt="non-virtual"><scope>nvidia::inferenceserver::client::InferContext</scope><name>GetReadyAsyncRequest</name></member>
      <member refid="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1aa083c81909e7a6d10725306d231ea0cb" prot="public" virt="non-virtual"><scope>nvidia::inferenceserver::client::InferContext</scope><name>GetStat</name></member>
      <member refid="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1ad65a99fa3c6afcf961de531f4421a73d" prot="protected" virt="non-virtual"><scope>nvidia::inferenceserver::client::InferContext</scope><name>infer_request_</name></member>
      <member refid="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1a1ad0ced372d23d1ed5a0af511ab7aa21" prot="protected" virt="non-virtual"><scope>nvidia::inferenceserver::client::InferContext</scope><name>InferContext</name></member>
      <member refid="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1a7b98f4f0fa2fe0f948f2a06d91ec92f3" prot="public" virt="non-virtual"><scope>nvidia::inferenceserver::client::InferContext</scope><name>Inputs</name></member>
      <member refid="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1a2959c35ac38d06693ee9e3482f4b344b" prot="protected" virt="non-virtual"><scope>nvidia::inferenceserver::client::InferContext</scope><name>inputs_</name></member>
      <member refid="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1ab7437e029e942557e2d11f8c22ae6d6b" prot="protected" virt="non-virtual"><scope>nvidia::inferenceserver::client::InferContext</scope><name>IsRequestReady</name></member>
      <member refid="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1abfc00d0d8175ac030d881365b7030efd" prot="protected" virt="non-virtual"><scope>nvidia::inferenceserver::client::InferContext</scope><name>max_batch_size_</name></member>
      <member refid="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1ac9bd9ba3234a0c282383cdd939ca16dc" prot="public" virt="non-virtual"><scope>nvidia::inferenceserver::client::InferContext</scope><name>MaxBatchSize</name></member>
      <member refid="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1aca65f4b17c14e3c25350c8af3571f86c" prot="protected" virt="non-virtual"><scope>nvidia::inferenceserver::client::InferContext</scope><name>model_name_</name></member>
      <member refid="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1aea092ac954441e636316644bd94b5979" prot="protected" virt="non-virtual"><scope>nvidia::inferenceserver::client::InferContext</scope><name>model_version_</name></member>
      <member refid="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1a6b01d3aece1500914c773fb9d88cb44b" prot="public" virt="non-virtual"><scope>nvidia::inferenceserver::client::InferContext</scope><name>ModelName</name></member>
      <member refid="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1a88642030870fb82b452a48a1676755d0" prot="public" virt="non-virtual"><scope>nvidia::inferenceserver::client::InferContext</scope><name>ModelVersion</name></member>
      <member refid="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1a1baafc5376df1a6e16de58633c93b4cc" prot="protected" virt="non-virtual"><scope>nvidia::inferenceserver::client::InferContext</scope><name>mutex_</name></member>
      <member refid="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1aa2dda0b94a39e54ce2d07a3b5b6988b2" prot="protected" virt="non-virtual"><scope>nvidia::inferenceserver::client::InferContext</scope><name>ongoing_async_requests_</name></member>
      <member refid="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1a4a783616ac9f48e21bd89abb69d0b21a" prot="public" virt="non-virtual"><scope>nvidia::inferenceserver::client::InferContext</scope><name>Outputs</name></member>
      <member refid="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1a21c9f59353ef1d4215e033f5a4b5d86a" prot="protected" virt="non-virtual"><scope>nvidia::inferenceserver::client::InferContext</scope><name>outputs_</name></member>
      <member refid="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1ad2c7af868ffbb1a2e24f47d9ae0093a5" prot="protected" virt="pure-virtual"><scope>nvidia::inferenceserver::client::InferContext</scope><name>PreRunProcessing</name></member>
      <member refid="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1a8540194150819719c190049fab1e125c" prot="public" virt="non-virtual"><scope>nvidia::inferenceserver::client::InferContext</scope><name>ResultMap</name></member>
      <member refid="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1a4cac8002817c9259851305b14c258f36" prot="public" virt="pure-virtual"><scope>nvidia::inferenceserver::client::InferContext</scope><name>Run</name></member>
      <member refid="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1a7cf1746d814561727fd3b07b0f8349f2" prot="public" virt="non-virtual"><scope>nvidia::inferenceserver::client::InferContext</scope><name>SetRunOptions</name></member>
      <member refid="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1a6b2d7784bcf85ee225b574f73d6059ca" prot="protected" virt="non-virtual"><scope>nvidia::inferenceserver::client::InferContext</scope><name>sync_request_</name></member>
      <member refid="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1a11eff835f49b4d9126e1606a3b2c8615" prot="protected" virt="non-virtual"><scope>nvidia::inferenceserver::client::InferContext</scope><name>UpdateStat</name></member>
      <member refid="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1a69b72d3b61b07468f5a13c97f36c5e6a" prot="protected" virt="non-virtual"><scope>nvidia::inferenceserver::client::InferContext</scope><name>verbose_</name></member>
      <member refid="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1a39fdbf2a953b23d0005aef8e2618fcaf" prot="protected" virt="non-virtual"><scope>nvidia::inferenceserver::client::InferContext</scope><name>worker_</name></member>
      <member refid="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1a3a996794a2f69c24ab50391fcc36ceec" prot="public" virt="virtual"><scope>nvidia::inferenceserver::client::InferContext</scope><name>~InferContext</name></member>
    </listofallmembers>
  </compounddef>
</doxygen>
