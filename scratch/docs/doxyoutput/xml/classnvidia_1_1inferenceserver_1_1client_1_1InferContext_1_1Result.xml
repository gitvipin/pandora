<?xml version='1.0' encoding='UTF-8' standalone='no'?>
<doxygen xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:noNamespaceSchemaLocation="compound.xsd" version="1.8.11">
  <compounddef id="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result" kind="class" language="C++" prot="public" abstract="yes">
    <compoundname>nvidia::inferenceserver::client::InferContext::Result</compoundname>
    <includes refid="request_8h" local="no">request.h</includes>
    <innerclass refid="structnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result_1_1ClassResult" prot="public">nvidia::inferenceserver::client::InferContext::Result::ClassResult</innerclass>
      <sectiondef kind="public-type">
      <memberdef kind="enum" id="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result_1a8bb9d36d1808a030dda919af87746dad" prot="public" static="no">
        <name>ResultFormat</name>
        <enumvalue id="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result_1a8bb9d36d1808a030dda919af87746dadada3b38eab85f839509454eb2f3df8ea6" prot="public">
          <name>RAW</name>
          <initializer>= 0</initializer>
          <briefdescription>
<para>RAW format is the entire result tensor of values. </para>          </briefdescription>
          <detaileddescription>
          </detaileddescription>
        </enumvalue>
        <enumvalue id="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result_1a8bb9d36d1808a030dda919af87746dada9f91171ba1a8902be2821d6ba488dbd4" prot="public">
          <name>CLASS</name>
          <initializer>= 1</initializer>
          <briefdescription>
<para>CLASS format is the top-k highest probability values of the result and the associated class label (if provided by the model). </para>          </briefdescription>
          <detaileddescription>
          </detaileddescription>
        </enumvalue>
        <briefdescription>
<para>Format in which result is returned. </para>        </briefdescription>
        <detaileddescription>
        </detaileddescription>
        <inbodydescription>
        </inbodydescription>
        <location file="src/clients/c++/request.h" line="359" column="1" bodyfile="src/clients/c++/request.h" bodystart="359" bodyend="367"/>
      </memberdef>
      </sectiondef>
      <sectiondef kind="public-func">
      <memberdef kind="function" id="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result_1aa87dddd7be7a18f787946a6548398d3b" prot="public" static="no" const="no" explicit="no" inline="yes" virt="virtual">
        <type></type>
        <definition>virtual nvidia::inferenceserver::client::InferContext::Result::~Result</definition>
        <argsstring>()</argsstring>
        <name>~Result</name>
        <briefdescription>
<para>Destroy the result. </para>        </briefdescription>
        <detaileddescription>
        </detaileddescription>
        <inbodydescription>
        </inbodydescription>
        <location file="src/clients/c++/request.h" line="356" column="1" bodyfile="src/clients/c++/request.h" bodystart="356" bodyend="356"/>
      </memberdef>
      <memberdef kind="function" id="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result_1a78206cf92579339875ad3a44318a28f7" prot="public" static="no" const="yes" explicit="no" inline="no" virt="pure-virtual">
        <type>const std::string &amp;</type>
        <definition>virtual const std::string&amp; nvidia::inferenceserver::client::InferContext::Result::ModelName</definition>
        <argsstring>() const =0</argsstring>
        <name>ModelName</name>
        <briefdescription>
        </briefdescription>
        <detaileddescription>
<para><simplesect kind="return"><para>The name of the model that produced this result. </para></simplesect>
</para>        </detaileddescription>
        <inbodydescription>
        </inbodydescription>
        <location file="src/clients/c++/request.h" line="370" column="1"/>
      </memberdef>
      <memberdef kind="function" id="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result_1a9a81d8a1588920dc784fcb4b2520f566" prot="public" static="no" const="yes" explicit="no" inline="no" virt="pure-virtual">
        <type>int64_t</type>
        <definition>virtual int64_t nvidia::inferenceserver::client::InferContext::Result::ModelVersion</definition>
        <argsstring>() const =0</argsstring>
        <name>ModelVersion</name>
        <briefdescription>
        </briefdescription>
        <detaileddescription>
<para><simplesect kind="return"><para>The version of the model that produced this result. </para></simplesect>
</para>        </detaileddescription>
        <inbodydescription>
        </inbodydescription>
        <location file="src/clients/c++/request.h" line="373" column="1"/>
      </memberdef>
      <memberdef kind="function" id="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result_1a7687d7d07c181ba27f32654341390a92" prot="public" static="no" const="yes" explicit="no" inline="no" virt="pure-virtual">
        <type>const std::shared_ptr&lt; <ref refid="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Output" kindref="compound">Output</ref> &gt;</type>
        <definition>virtual const std::shared_ptr&lt;Output&gt; nvidia::inferenceserver::client::InferContext::Result::GetOutput</definition>
        <argsstring>() const =0</argsstring>
        <name>GetOutput</name>
        <briefdescription>
        </briefdescription>
        <detaileddescription>
<para><simplesect kind="return"><para>The <ref refid="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Output" kindref="compound">Output</ref> object corresponding to this result. </para></simplesect>
</para>        </detaileddescription>
        <inbodydescription>
        </inbodydescription>
        <location file="src/clients/c++/request.h" line="376" column="1"/>
      </memberdef>
      <memberdef kind="function" id="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result_1aeeec1b3448cf681e3ec8ac3c3c589374" prot="public" static="no" const="yes" explicit="no" inline="no" virt="pure-virtual">
        <type><ref refid="classnvidia_1_1inferenceserver_1_1client_1_1Error" kindref="compound">Error</ref></type>
        <definition>virtual Error nvidia::inferenceserver::client::InferContext::Result::GetRawShape</definition>
        <argsstring>(std::vector&lt; int64_t &gt; *shape) const =0</argsstring>
        <name>GetRawShape</name>
        <param>
          <type>std::vector&lt; int64_t &gt; *</type>
          <declname>shape</declname>
        </param>
        <briefdescription>
<para>Get the shape of a raw result. </para>        </briefdescription>
        <detaileddescription>
<para>The shape does not include the batch dimension. <parameterlist kind="param"><parameteritem>
<parameternamelist>
<parametername>shape</parametername>
</parameternamelist>
<parameterdescription>
<para>Returns the shape. </para></parameterdescription>
</parameteritem>
</parameterlist>
<simplesect kind="return"><para><ref refid="classnvidia_1_1inferenceserver_1_1client_1_1Error" kindref="compound">Error</ref> object indicating success or failure. </para></simplesect>
</para>        </detaileddescription>
        <inbodydescription>
        </inbodydescription>
        <location file="src/clients/c++/request.h" line="382" column="1"/>
      </memberdef>
      <memberdef kind="function" id="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result_1af97122884c92553992261c501dbe3095" prot="public" static="no" const="yes" explicit="no" inline="no" virt="pure-virtual">
        <type><ref refid="classnvidia_1_1inferenceserver_1_1client_1_1Error" kindref="compound">Error</ref></type>
        <definition>virtual Error nvidia::inferenceserver::client::InferContext::Result::GetRaw</definition>
        <argsstring>(size_t batch_idx, const std::vector&lt; uint8_t &gt; **buf) const =0</argsstring>
        <name>GetRaw</name>
        <param>
          <type>size_t</type>
          <declname>batch_idx</declname>
        </param>
        <param>
          <type>const std::vector&lt; uint8_t &gt; **</type>
          <declname>buf</declname>
        </param>
        <briefdescription>
<para>Get a reference to entire raw result data for a specific batch entry. </para>        </briefdescription>
        <detaileddescription>
<para>Returns error if this result is not RAW format. <parameterlist kind="param"><parameteritem>
<parameternamelist>
<parametername>batch_idx</parametername>
</parameternamelist>
<parameterdescription>
<para>Returns the results for this entry of the batch. </para></parameterdescription>
</parameteritem>
<parameteritem>
<parameternamelist>
<parametername>buf</parametername>
</parameternamelist>
<parameterdescription>
<para>Returns the vector of result bytes. </para></parameterdescription>
</parameteritem>
</parameterlist>
<simplesect kind="return"><para><ref refid="classnvidia_1_1inferenceserver_1_1client_1_1Error" kindref="compound">Error</ref> object indicating success or failure. </para></simplesect>
</para>        </detaileddescription>
        <inbodydescription>
        </inbodydescription>
        <location file="src/clients/c++/request.h" line="389" column="1"/>
      </memberdef>
      <memberdef kind="function" id="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result_1a2a13887812b025896d569f041db7e2c8" prot="public" static="no" const="no" explicit="no" inline="no" virt="pure-virtual">
        <type><ref refid="classnvidia_1_1inferenceserver_1_1client_1_1Error" kindref="compound">Error</ref></type>
        <definition>virtual Error nvidia::inferenceserver::client::InferContext::Result::GetRawAtCursor</definition>
        <argsstring>(size_t batch_idx, const uint8_t **buf, size_t adv_byte_size)=0</argsstring>
        <name>GetRawAtCursor</name>
        <param>
          <type>size_t</type>
          <declname>batch_idx</declname>
        </param>
        <param>
          <type>const uint8_t **</type>
          <declname>buf</declname>
        </param>
        <param>
          <type>size_t</type>
          <declname>adv_byte_size</declname>
        </param>
        <briefdescription>
<para>Get a reference to raw result data for a specific batch entry at the current &quot;cursor&quot; and advance the cursor by the specified number of bytes. </para>        </briefdescription>
        <detaileddescription>
<para>More typically use <ref refid="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result_1a2a13887812b025896d569f041db7e2c8" kindref="member">GetRawAtCursor&lt;T&gt;()</ref> method to return the data as a specific type T. Use <ref refid="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result_1adb27437d415b569a3a4b7700c56002bf" kindref="member">ResetCursor()</ref> to reset the cursor to the beginning of the result. Returns error if this result is not RAW format. <parameterlist kind="param"><parameteritem>
<parameternamelist>
<parametername>batch_idx</parametername>
</parameternamelist>
<parameterdescription>
<para>Returns results for this entry of the batch. </para></parameterdescription>
</parameteritem>
<parameteritem>
<parameternamelist>
<parametername>buf</parametername>
</parameternamelist>
<parameterdescription>
<para>Returns pointer to &apos;adv_byte_size&apos; bytes of data. </para></parameterdescription>
</parameteritem>
<parameteritem>
<parameternamelist>
<parametername>adv_byte_size</parametername>
</parameternamelist>
<parameterdescription>
<para>The number of bytes of data to get a reference to. </para></parameterdescription>
</parameteritem>
</parameterlist>
<simplesect kind="return"><para><ref refid="classnvidia_1_1inferenceserver_1_1client_1_1Error" kindref="compound">Error</ref> object indicating success or failure. </para></simplesect>
</para>        </detaileddescription>
        <inbodydescription>
        </inbodydescription>
        <location file="src/clients/c++/request.h" line="402" column="1"/>
      </memberdef>
      <memberdef kind="function" id="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result_1a64d951fd64068b9d2e3da103f47390fe" prot="public" static="no" const="no" explicit="no" inline="no" virt="non-virtual">
        <templateparamlist>
          <param>
            <type>typename T</type>
          </param>
        </templateparamlist>
        <type><ref refid="classnvidia_1_1inferenceserver_1_1client_1_1Error" kindref="compound">Error</ref></type>
        <definition>Error nvidia::inferenceserver::client::InferContext::Result::GetRawAtCursor</definition>
        <argsstring>(size_t batch_idx, T *out)</argsstring>
        <name>GetRawAtCursor</name>
        <param>
          <type>size_t</type>
          <declname>batch_idx</declname>
        </param>
        <param>
          <type>T *</type>
          <declname>out</declname>
        </param>
        <briefdescription>
<para>Read a value for a specific batch entry at the current &quot;cursor&quot; from the result tensor as the specified type T and advance the cursor. </para>        </briefdescription>
        <detaileddescription>
<para>Use <ref refid="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result_1adb27437d415b569a3a4b7700c56002bf" kindref="member">ResetCursor()</ref> to reset the cursor to the beginning of the result. Returns error if this result is not RAW format. <parameterlist kind="param"><parameteritem>
<parameternamelist>
<parametername>batch_idx</parametername>
</parameternamelist>
<parameterdescription>
<para>Returns results for this entry of the batch. </para></parameterdescription>
</parameteritem>
<parameteritem>
<parameternamelist>
<parametername>out</parametername>
</parameternamelist>
<parameterdescription>
<para>Returns the value at the cursor. </para></parameterdescription>
</parameteritem>
</parameterlist>
<simplesect kind="return"><para><ref refid="classnvidia_1_1inferenceserver_1_1client_1_1Error" kindref="compound">Error</ref> object indicating success or failure. </para></simplesect>
</para>        </detaileddescription>
        <inbodydescription>
        </inbodydescription>
        <location file="src/clients/c++/request.h" line="413" column="1" bodyfile="src/clients/c++/request.h" bodystart="1237" bodyend="1247"/>
      </memberdef>
      <memberdef kind="function" id="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result_1ac50ae04862c58a004547cac5debc7c98" prot="public" static="no" const="yes" explicit="no" inline="no" virt="pure-virtual">
        <type><ref refid="classnvidia_1_1inferenceserver_1_1client_1_1Error" kindref="compound">Error</ref></type>
        <definition>virtual Error nvidia::inferenceserver::client::InferContext::Result::GetClassCount</definition>
        <argsstring>(size_t batch_idx, size_t *cnt) const =0</argsstring>
        <name>GetClassCount</name>
        <param>
          <type>size_t</type>
          <declname>batch_idx</declname>
        </param>
        <param>
          <type>size_t *</type>
          <declname>cnt</declname>
        </param>
        <briefdescription>
<para>Get the number of class results for a batch. </para>        </briefdescription>
        <detaileddescription>
<para>Returns error if this result is not CLASS format. <parameterlist kind="param"><parameteritem>
<parameternamelist>
<parametername>batch_idx</parametername>
</parameternamelist>
<parameterdescription>
<para>The index in the batch. </para></parameterdescription>
</parameteritem>
<parameteritem>
<parameternamelist>
<parametername>cnt</parametername>
</parameternamelist>
<parameterdescription>
<para>Returns the number of <ref refid="structnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result_1_1ClassResult" kindref="compound">ClassResult</ref> entries for the batch entry. </para></parameterdescription>
</parameteritem>
</parameterlist>
<simplesect kind="return"><para><ref refid="classnvidia_1_1inferenceserver_1_1client_1_1Error" kindref="compound">Error</ref> object indicating success or failure. </para></simplesect>
</para>        </detaileddescription>
        <inbodydescription>
        </inbodydescription>
        <location file="src/clients/c++/request.h" line="431" column="1"/>
      </memberdef>
      <memberdef kind="function" id="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result_1a0b78337cb9b55608b310f226dd6e8287" prot="public" static="no" const="no" explicit="no" inline="no" virt="pure-virtual">
        <type><ref refid="classnvidia_1_1inferenceserver_1_1client_1_1Error" kindref="compound">Error</ref></type>
        <definition>virtual Error nvidia::inferenceserver::client::InferContext::Result::GetClassAtCursor</definition>
        <argsstring>(size_t batch_idx, ClassResult *result)=0</argsstring>
        <name>GetClassAtCursor</name>
        <param>
          <type>size_t</type>
          <declname>batch_idx</declname>
        </param>
        <param>
          <type><ref refid="structnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result_1_1ClassResult" kindref="compound">ClassResult</ref> *</type>
          <declname>result</declname>
        </param>
        <briefdescription>
<para>Get the <ref refid="structnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result_1_1ClassResult" kindref="compound">ClassResult</ref> result for a specific batch entry at the current cursor. </para>        </briefdescription>
        <detaileddescription>
<para>Use <ref refid="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result_1adb27437d415b569a3a4b7700c56002bf" kindref="member">ResetCursor()</ref> to reset the cursor to the beginning of the result. Returns error if this result is not CLASS format. <parameterlist kind="param"><parameteritem>
<parameternamelist>
<parametername>batch_idx</parametername>
</parameternamelist>
<parameterdescription>
<para>The index in the batch. </para></parameterdescription>
</parameteritem>
<parameteritem>
<parameternamelist>
<parametername>result</parametername>
</parameternamelist>
<parameterdescription>
<para>Returns the <ref refid="structnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result_1_1ClassResult" kindref="compound">ClassResult</ref> value for the batch at the cursor. </para></parameterdescription>
</parameteritem>
</parameterlist>
<simplesect kind="return"><para><ref refid="classnvidia_1_1inferenceserver_1_1client_1_1Error" kindref="compound">Error</ref> object indicating success or failure. </para></simplesect>
</para>        </detaileddescription>
        <inbodydescription>
        </inbodydescription>
        <location file="src/clients/c++/request.h" line="440" column="1"/>
      </memberdef>
      <memberdef kind="function" id="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result_1a304de867ffb5121c21874e11a4e9d34a" prot="public" static="no" const="no" explicit="no" inline="no" virt="pure-virtual">
        <type><ref refid="classnvidia_1_1inferenceserver_1_1client_1_1Error" kindref="compound">Error</ref></type>
        <definition>virtual Error nvidia::inferenceserver::client::InferContext::Result::ResetCursors</definition>
        <argsstring>()=0</argsstring>
        <name>ResetCursors</name>
        <briefdescription>
<para>Reset cursor to beginning of result for all batch entries. </para>        </briefdescription>
        <detaileddescription>
<para><simplesect kind="return"><para><ref refid="classnvidia_1_1inferenceserver_1_1client_1_1Error" kindref="compound">Error</ref> object indicating success or failure. </para></simplesect>
</para>        </detaileddescription>
        <inbodydescription>
        </inbodydescription>
        <location file="src/clients/c++/request.h" line="444" column="1"/>
      </memberdef>
      <memberdef kind="function" id="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result_1adb27437d415b569a3a4b7700c56002bf" prot="public" static="no" const="no" explicit="no" inline="no" virt="pure-virtual">
        <type><ref refid="classnvidia_1_1inferenceserver_1_1client_1_1Error" kindref="compound">Error</ref></type>
        <definition>virtual Error nvidia::inferenceserver::client::InferContext::Result::ResetCursor</definition>
        <argsstring>(size_t batch_idx)=0</argsstring>
        <name>ResetCursor</name>
        <param>
          <type>size_t</type>
          <declname>batch_idx</declname>
        </param>
        <briefdescription>
<para>Reset cursor to beginning of result for specified batch entry. </para>        </briefdescription>
        <detaileddescription>
<para><parameterlist kind="param"><parameteritem>
<parameternamelist>
<parametername>batch_idx</parametername>
</parameternamelist>
<parameterdescription>
<para>The index in the batch. </para></parameterdescription>
</parameteritem>
</parameterlist>
<simplesect kind="return"><para><ref refid="classnvidia_1_1inferenceserver_1_1client_1_1Error" kindref="compound">Error</ref> object indicating success or failure. </para></simplesect>
</para>        </detaileddescription>
        <inbodydescription>
        </inbodydescription>
        <location file="src/clients/c++/request.h" line="449" column="1"/>
      </memberdef>
      <memberdef kind="function" id="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result_1a742f381607b9915cb2fc558511f491c1" prot="public" static="no" const="no" explicit="no" inline="no" virt="non-virtual">
        <templateparamlist>
        </templateparamlist>
        <type><ref refid="classnvidia_1_1inferenceserver_1_1client_1_1Error" kindref="compound">Error</ref></type>
        <definition>Error nvidia::inferenceserver::client::InferContext::Result::GetRawAtCursor</definition>
        <argsstring>(size_t batch_idx, std::string *out)</argsstring>
        <name>GetRawAtCursor</name>
        <param>
          <type>size_t</type>
          <declname>batch_idx</declname>
        </param>
        <param>
          <type>std::string *</type>
          <declname>out</declname>
        </param>
        <briefdescription>
        </briefdescription>
        <detaileddescription>
        </detaileddescription>
        <inbodydescription>
        </inbodydescription>
        <location file="src/clients/c++/request.h" line="1233" column="1"/>
      </memberdef>
      </sectiondef>
    <briefdescription>
<para>An inference result corresponding to an output. </para>    </briefdescription>
    <detaileddescription>
    </detaileddescription>
    <location file="src/clients/c++/request.h" line="353" column="1" bodyfile="src/clients/c++/request.h" bodystart="353" bodyend="450"/>
    <listofallmembers>
      <member refid="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result_1a8bb9d36d1808a030dda919af87746dada9f91171ba1a8902be2821d6ba488dbd4" prot="public" virt="non-virtual"><scope>nvidia::inferenceserver::client::InferContext::Result</scope><name>CLASS</name></member>
      <member refid="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result_1a0b78337cb9b55608b310f226dd6e8287" prot="public" virt="pure-virtual"><scope>nvidia::inferenceserver::client::InferContext::Result</scope><name>GetClassAtCursor</name></member>
      <member refid="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result_1ac50ae04862c58a004547cac5debc7c98" prot="public" virt="pure-virtual"><scope>nvidia::inferenceserver::client::InferContext::Result</scope><name>GetClassCount</name></member>
      <member refid="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result_1a7687d7d07c181ba27f32654341390a92" prot="public" virt="pure-virtual"><scope>nvidia::inferenceserver::client::InferContext::Result</scope><name>GetOutput</name></member>
      <member refid="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result_1af97122884c92553992261c501dbe3095" prot="public" virt="pure-virtual"><scope>nvidia::inferenceserver::client::InferContext::Result</scope><name>GetRaw</name></member>
      <member refid="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result_1a2a13887812b025896d569f041db7e2c8" prot="public" virt="pure-virtual"><scope>nvidia::inferenceserver::client::InferContext::Result</scope><name>GetRawAtCursor</name></member>
      <member refid="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result_1a64d951fd64068b9d2e3da103f47390fe" prot="public" virt="non-virtual"><scope>nvidia::inferenceserver::client::InferContext::Result</scope><name>GetRawAtCursor</name></member>
      <member refid="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result_1a742f381607b9915cb2fc558511f491c1" prot="public" virt="non-virtual"><scope>nvidia::inferenceserver::client::InferContext::Result</scope><name>GetRawAtCursor</name></member>
      <member refid="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result_1aeeec1b3448cf681e3ec8ac3c3c589374" prot="public" virt="pure-virtual"><scope>nvidia::inferenceserver::client::InferContext::Result</scope><name>GetRawShape</name></member>
      <member refid="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result_1a78206cf92579339875ad3a44318a28f7" prot="public" virt="pure-virtual"><scope>nvidia::inferenceserver::client::InferContext::Result</scope><name>ModelName</name></member>
      <member refid="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result_1a9a81d8a1588920dc784fcb4b2520f566" prot="public" virt="pure-virtual"><scope>nvidia::inferenceserver::client::InferContext::Result</scope><name>ModelVersion</name></member>
      <member refid="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result_1a8bb9d36d1808a030dda919af87746dadada3b38eab85f839509454eb2f3df8ea6" prot="public" virt="non-virtual"><scope>nvidia::inferenceserver::client::InferContext::Result</scope><name>RAW</name></member>
      <member refid="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result_1adb27437d415b569a3a4b7700c56002bf" prot="public" virt="pure-virtual"><scope>nvidia::inferenceserver::client::InferContext::Result</scope><name>ResetCursor</name></member>
      <member refid="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result_1a304de867ffb5121c21874e11a4e9d34a" prot="public" virt="pure-virtual"><scope>nvidia::inferenceserver::client::InferContext::Result</scope><name>ResetCursors</name></member>
      <member refid="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result_1a8bb9d36d1808a030dda919af87746dad" prot="public" virt="non-virtual"><scope>nvidia::inferenceserver::client::InferContext::Result</scope><name>ResultFormat</name></member>
      <member refid="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result_1aa87dddd7be7a18f787946a6548398d3b" prot="public" virt="virtual"><scope>nvidia::inferenceserver::client::InferContext::Result</scope><name>~Result</name></member>
    </listofallmembers>
  </compounddef>
</doxygen>
