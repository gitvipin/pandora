<?xml version='1.0' encoding='UTF-8' standalone='no'?>
<doxygen xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:noNamespaceSchemaLocation="compound.xsd" version="1.8.11">
  <compounddef id="classnvidia_1_1inferenceserver_1_1client_1_1InferGrpcStreamContext" kind="class" language="C++" prot="public">
    <compoundname>nvidia::inferenceserver::client::InferGrpcStreamContext</compoundname>
    <basecompoundref refid="classnvidia_1_1inferenceserver_1_1client_1_1InferGrpcContext" prot="public" virt="non-virtual">nvidia::inferenceserver::client::InferGrpcContext</basecompoundref>
    <includes refid="request_8h" local="no">request.h</includes>
      <sectiondef kind="private-attrib">
      <memberdef kind="variable" id="classnvidia_1_1inferenceserver_1_1client_1_1InferGrpcStreamContext_1af1929f62ca9d2e2818034ee1705dbbd6" prot="private" static="no" mutable="no">
        <type>grpc::ClientContext</type>
        <definition>grpc::ClientContext nvidia::inferenceserver::client::InferGrpcStreamContext::context_</definition>
        <argsstring></argsstring>
        <name>context_</name>
        <briefdescription>
        </briefdescription>
        <detaileddescription>
        </detaileddescription>
        <inbodydescription>
        </inbodydescription>
        <location file="src/clients/c++/request.h" line="1199" column="1" bodyfile="src/clients/c++/request.h" bodystart="1199" bodyend="-1"/>
      </memberdef>
      <memberdef kind="variable" id="classnvidia_1_1inferenceserver_1_1client_1_1InferGrpcStreamContext_1a0ac6456b2f6817ebaefe3008cef51f62" prot="private" static="no" mutable="no">
        <type>std::shared_ptr&lt; grpc::ClientReaderWriter&lt; InferRequest, InferResponse &gt; &gt;</type>
        <definition>std::shared_ptr&lt;grpc::ClientReaderWriter&lt;InferRequest, InferResponse&gt; &gt; nvidia::inferenceserver::client::InferGrpcStreamContext::stream_</definition>
        <argsstring></argsstring>
        <name>stream_</name>
        <briefdescription>
        </briefdescription>
        <detaileddescription>
        </detaileddescription>
        <inbodydescription>
        </inbodydescription>
        <location file="src/clients/c++/request.h" line="1201" column="1" bodyfile="src/clients/c++/request.h" bodystart="1201" bodyend="-1"/>
      </memberdef>
      </sectiondef>
      <sectiondef kind="public-func">
      <memberdef kind="function" id="classnvidia_1_1inferenceserver_1_1client_1_1InferGrpcStreamContext_1a378f8f70646b1256db2e78266b08b1c1" prot="public" static="no" const="no" explicit="no" inline="no" virt="non-virtual">
        <type></type>
        <definition>nvidia::inferenceserver::client::InferGrpcStreamContext::~InferGrpcStreamContext</definition>
        <argsstring>() override</argsstring>
        <name>~InferGrpcStreamContext</name>
        <briefdescription>
        </briefdescription>
        <detaileddescription>
        </detaileddescription>
        <inbodydescription>
        </inbodydescription>
        <location file="src/clients/c++/request.h" line="1149" column="1"/>
      </memberdef>
      <memberdef kind="function" id="classnvidia_1_1inferenceserver_1_1client_1_1InferGrpcStreamContext_1a5e42ef49aa0ed810934a18ca1dbb71e4" prot="public" static="no" const="no" explicit="no" inline="no" virt="virtual">
        <type><ref refid="classnvidia_1_1inferenceserver_1_1client_1_1Error" kindref="compound">Error</ref></type>
        <definition>Error nvidia::inferenceserver::client::InferGrpcStreamContext::Run</definition>
        <argsstring>(ResultMap *results) override</argsstring>
        <name>Run</name>
        <reimplements refid="classnvidia_1_1inferenceserver_1_1client_1_1InferGrpcContext_1a17c4f2f5007867d9f6b5444f9088f6c5">Run</reimplements>
        <param>
          <type>ResultMap *</type>
          <declname>results</declname>
        </param>
        <briefdescription>
<para>Send a synchronous request to the inference server to perform an inference to produce results for the outputs specified in the most recent call to <ref refid="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1a7cf1746d814561727fd3b07b0f8349f2" kindref="member">SetRunOptions()</ref>. </para>        </briefdescription>
        <detaileddescription>
<para><parameterlist kind="param"><parameteritem>
<parameternamelist>
<parametername>results</parametername>
</parameternamelist>
<parameterdescription>
<para>Returns Result objects holding inference results as a map from output name to Result object. </para></parameterdescription>
</parameteritem>
</parameterlist>
<simplesect kind="return"><para><ref refid="classnvidia_1_1inferenceserver_1_1client_1_1Error" kindref="compound">Error</ref> object indicating success or failure. </para></simplesect>
</para>        </detaileddescription>
        <inbodydescription>
        </inbodydescription>
        <location file="src/clients/c++/request.h" line="1188" column="1"/>
      </memberdef>
      <memberdef kind="function" id="classnvidia_1_1inferenceserver_1_1client_1_1InferGrpcStreamContext_1a86e4987e666b37c5493373ae1b068d2f" prot="public" static="no" const="no" explicit="no" inline="no" virt="virtual">
        <type><ref refid="classnvidia_1_1inferenceserver_1_1client_1_1Error" kindref="compound">Error</ref></type>
        <definition>Error nvidia::inferenceserver::client::InferGrpcStreamContext::AsyncRun</definition>
        <argsstring>(std::shared_ptr&lt; Request &gt; *async_request) override</argsstring>
        <name>AsyncRun</name>
        <reimplements refid="classnvidia_1_1inferenceserver_1_1client_1_1InferGrpcContext_1adcd341f23d4b3a609c2117c698522a31">AsyncRun</reimplements>
        <param>
          <type>std::shared_ptr&lt; <ref refid="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Request" kindref="compound">Request</ref> &gt; *</type>
          <declname>async_request</declname>
        </param>
        <briefdescription>
<para>Send an asynchronous request to the inference server to perform an inference to produce results for the outputs specified in the most recent call to <ref refid="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1a7cf1746d814561727fd3b07b0f8349f2" kindref="member">SetRunOptions()</ref>. </para>        </briefdescription>
        <detaileddescription>
<para><parameterlist kind="param"><parameteritem>
<parameternamelist>
<parametername>async_request</parametername>
</parameternamelist>
<parameterdescription>
<para>Returns a Request object that can be used to retrieve the inference results for the request. </para></parameterdescription>
</parameteritem>
</parameterlist>
<simplesect kind="return"><para><ref refid="classnvidia_1_1inferenceserver_1_1client_1_1Error" kindref="compound">Error</ref> object indicating success or failure. </para></simplesect>
</para>        </detaileddescription>
        <inbodydescription>
        </inbodydescription>
        <location file="src/clients/c++/request.h" line="1189" column="1"/>
      </memberdef>
      </sectiondef>
      <sectiondef kind="public-static-func">
      <memberdef kind="function" id="classnvidia_1_1inferenceserver_1_1client_1_1InferGrpcStreamContext_1a6a356d8b5495f6ba471ed989712c8e64" prot="public" static="yes" const="no" explicit="no" inline="no" virt="non-virtual">
        <type><ref refid="classnvidia_1_1inferenceserver_1_1client_1_1Error" kindref="compound">Error</ref></type>
        <definition>static Error nvidia::inferenceserver::client::InferGrpcStreamContext::Create</definition>
        <argsstring>(std::unique_ptr&lt; InferContext &gt; *ctx, const std::string &amp;server_url, const std::string &amp;model_name, int64_t model_version=-1, bool verbose=false)</argsstring>
        <name>Create</name>
        <param>
          <type>std::unique_ptr&lt; <ref refid="classnvidia_1_1inferenceserver_1_1client_1_1InferContext" kindref="compound">InferContext</ref> &gt; *</type>
          <declname>ctx</declname>
        </param>
        <param>
          <type>const std::string &amp;</type>
          <declname>server_url</declname>
        </param>
        <param>
          <type>const std::string &amp;</type>
          <declname>model_name</declname>
        </param>
        <param>
          <type>int64_t</type>
          <declname>model_version</declname>
          <defval>-1</defval>
        </param>
        <param>
          <type>bool</type>
          <declname>verbose</declname>
          <defval>false</defval>
        </param>
        <briefdescription>
<para>Create streaming context that performs inference for a non-sequence model using the GRPC protocol. </para>        </briefdescription>
        <detaileddescription>
<para><parameterlist kind="param"><parameteritem>
<parameternamelist>
<parametername>ctx</parametername>
</parameternamelist>
<parameterdescription>
<para>Returns a new <ref refid="classnvidia_1_1inferenceserver_1_1client_1_1InferGrpcContext" kindref="compound">InferGrpcContext</ref> object. </para></parameterdescription>
</parameteritem>
<parameteritem>
<parameternamelist>
<parametername>server_url</parametername>
</parameternamelist>
<parameterdescription>
<para>The inference server name and port. </para></parameterdescription>
</parameteritem>
<parameteritem>
<parameternamelist>
<parametername>model_name</parametername>
</parameternamelist>
<parameterdescription>
<para>The name of the model to get status for. </para></parameterdescription>
</parameteritem>
<parameteritem>
<parameternamelist>
<parametername>model_version</parametername>
</parameternamelist>
<parameterdescription>
<para>The version of the model to use for inference, or -1 to indicate that the latest (i.e. highest version number) version should be used. </para></parameterdescription>
</parameteritem>
<parameteritem>
<parameternamelist>
<parametername>verbose</parametername>
</parameternamelist>
<parameterdescription>
<para>If true generate verbose output when contacting the inference server. </para></parameterdescription>
</parameteritem>
</parameterlist>
<simplesect kind="return"><para><ref refid="classnvidia_1_1inferenceserver_1_1client_1_1Error" kindref="compound">Error</ref> object indicating success or failure. </para></simplesect>
</para>        </detaileddescription>
        <inbodydescription>
        </inbodydescription>
        <location file="src/clients/c++/request.h" line="1163" column="1"/>
      </memberdef>
      <memberdef kind="function" id="classnvidia_1_1inferenceserver_1_1client_1_1InferGrpcStreamContext_1aa1f3dc161b5bed3a3e4d73ea997f898c" prot="public" static="yes" const="no" explicit="no" inline="no" virt="non-virtual">
        <type><ref refid="classnvidia_1_1inferenceserver_1_1client_1_1Error" kindref="compound">Error</ref></type>
        <definition>static Error nvidia::inferenceserver::client::InferGrpcStreamContext::Create</definition>
        <argsstring>(std::unique_ptr&lt; InferContext &gt; *ctx, CorrelationID correlation_id, const std::string &amp;server_url, const std::string &amp;model_name, int64_t model_version=-1, bool verbose=false)</argsstring>
        <name>Create</name>
        <param>
          <type>std::unique_ptr&lt; <ref refid="classnvidia_1_1inferenceserver_1_1client_1_1InferContext" kindref="compound">InferContext</ref> &gt; *</type>
          <declname>ctx</declname>
        </param>
        <param>
          <type>CorrelationID</type>
          <declname>correlation_id</declname>
        </param>
        <param>
          <type>const std::string &amp;</type>
          <declname>server_url</declname>
        </param>
        <param>
          <type>const std::string &amp;</type>
          <declname>model_name</declname>
        </param>
        <param>
          <type>int64_t</type>
          <declname>model_version</declname>
          <defval>-1</defval>
        </param>
        <param>
          <type>bool</type>
          <declname>verbose</declname>
          <defval>false</defval>
        </param>
        <briefdescription>
<para>Create streaming context that performs inference for a sequence model using a given correlation ID and the GRPC protocol. </para>        </briefdescription>
        <detaileddescription>
<para><parameterlist kind="param"><parameteritem>
<parameternamelist>
<parametername>ctx</parametername>
</parameternamelist>
<parameterdescription>
<para>Returns a new <ref refid="classnvidia_1_1inferenceserver_1_1client_1_1InferGrpcContext" kindref="compound">InferGrpcContext</ref> object. </para></parameterdescription>
</parameteritem>
<parameteritem>
<parameternamelist>
<parametername>correlation_id</parametername>
</parameternamelist>
<parameterdescription>
<para>The correlation ID to use for all inferences performed with this context. A value of 0 (zero) indicates that no correlation ID should be used. </para></parameterdescription>
</parameteritem>
<parameteritem>
<parameternamelist>
<parametername>server_url</parametername>
</parameternamelist>
<parameterdescription>
<para>The inference server name and port. </para></parameterdescription>
</parameteritem>
<parameteritem>
<parameternamelist>
<parametername>model_name</parametername>
</parameternamelist>
<parameterdescription>
<para>The name of the model to get status for. </para></parameterdescription>
</parameteritem>
<parameteritem>
<parameternamelist>
<parametername>model_version</parametername>
</parameternamelist>
<parameterdescription>
<para>The version of the model to use for inference, or -1 to indicate that the latest (i.e. highest version number) version should be used. </para></parameterdescription>
</parameteritem>
<parameteritem>
<parameternamelist>
<parametername>verbose</parametername>
</parameternamelist>
<parameterdescription>
<para>If true generate verbose output when contacting the inference server. </para></parameterdescription>
</parameteritem>
</parameterlist>
<simplesect kind="return"><para><ref refid="classnvidia_1_1inferenceserver_1_1client_1_1Error" kindref="compound">Error</ref> object indicating success or failure. </para></simplesect>
</para>        </detaileddescription>
        <inbodydescription>
        </inbodydescription>
        <location file="src/clients/c++/request.h" line="1183" column="1"/>
      </memberdef>
      </sectiondef>
      <sectiondef kind="private-func">
      <memberdef kind="function" id="classnvidia_1_1inferenceserver_1_1client_1_1InferGrpcStreamContext_1aa8b4a62d25916a9fd405591739a6b847" prot="private" static="no" const="no" explicit="no" inline="no" virt="non-virtual">
        <type></type>
        <definition>nvidia::inferenceserver::client::InferGrpcStreamContext::InferGrpcStreamContext</definition>
        <argsstring>(const std::string &amp;, const std::string &amp;, int64_t, CorrelationID, bool)</argsstring>
        <name>InferGrpcStreamContext</name>
        <param>
          <type>const std::string &amp;</type>
        </param>
        <param>
          <type>const std::string &amp;</type>
        </param>
        <param>
          <type>int64_t</type>
        </param>
        <param>
          <type>CorrelationID</type>
        </param>
        <param>
          <type>bool</type>
        </param>
        <briefdescription>
        </briefdescription>
        <detaileddescription>
        </detaileddescription>
        <inbodydescription>
        </inbodydescription>
        <location file="src/clients/c++/request.h" line="1192" column="1"/>
      </memberdef>
      <memberdef kind="function" id="classnvidia_1_1inferenceserver_1_1client_1_1InferGrpcStreamContext_1ac0b79bc8230741c954ba5901e0ea330c" prot="private" static="no" const="no" explicit="no" inline="no" virt="virtual">
        <type>void</type>
        <definition>void nvidia::inferenceserver::client::InferGrpcStreamContext::AsyncTransfer</definition>
        <argsstring>() override</argsstring>
        <name>AsyncTransfer</name>
        <reimplements refid="classnvidia_1_1inferenceserver_1_1client_1_1InferGrpcContext_1a3f4f41c57b4e74c9ad6e8803c2514ec0">AsyncTransfer</reimplements>
        <briefdescription>
        </briefdescription>
        <detaileddescription>
        </detaileddescription>
        <inbodydescription>
        </inbodydescription>
        <location file="src/clients/c++/request.h" line="1196" column="1"/>
      </memberdef>
      </sectiondef>
    <briefdescription>
<para><ref refid="classnvidia_1_1inferenceserver_1_1client_1_1InferGrpcStreamContext" kindref="compound">InferGrpcStreamContext</ref> is the streaming instantiation of <ref refid="classnvidia_1_1inferenceserver_1_1client_1_1InferGrpcContext" kindref="compound">InferGrpcContext</ref>. </para>    </briefdescription>
    <detaileddescription>
<para>All synchronous and asynchronous requests sent from this context will be sent in the same stream. </para>    </detaileddescription>
    <inheritancegraph>
      <node id="100">
        <label>nvidia::inferenceserver::client::InferContext</label>
        <link refid="classnvidia_1_1inferenceserver_1_1client_1_1InferContext"/>
      </node>
      <node id="99">
        <label>nvidia::inferenceserver::client::InferGrpcContext</label>
        <link refid="classnvidia_1_1inferenceserver_1_1client_1_1InferGrpcContext"/>
        <childnode refid="100" relation="public-inheritance">
        </childnode>
      </node>
      <node id="98">
        <label>nvidia::inferenceserver::client::InferGrpcStreamContext</label>
        <link refid="classnvidia_1_1inferenceserver_1_1client_1_1InferGrpcStreamContext"/>
        <childnode refid="99" relation="public-inheritance">
        </childnode>
      </node>
    </inheritancegraph>
    <collaborationgraph>
      <node id="104">
        <label>nvidia::inferenceserver::client::InferContext::Stat</label>
        <link refid="structnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Stat"/>
      </node>
      <node id="103">
        <label>nvidia::inferenceserver::client::InferContext</label>
        <link refid="classnvidia_1_1inferenceserver_1_1client_1_1InferContext"/>
        <childnode refid="104" relation="usage">
          <edgelabel>context_stat_</edgelabel>
        </childnode>
      </node>
      <node id="102">
        <label>nvidia::inferenceserver::client::InferGrpcContext</label>
        <link refid="classnvidia_1_1inferenceserver_1_1client_1_1InferGrpcContext"/>
        <childnode refid="103" relation="public-inheritance">
        </childnode>
      </node>
      <node id="101">
        <label>nvidia::inferenceserver::client::InferGrpcStreamContext</label>
        <link refid="classnvidia_1_1inferenceserver_1_1client_1_1InferGrpcStreamContext"/>
        <childnode refid="102" relation="public-inheritance">
        </childnode>
      </node>
    </collaborationgraph>
    <location file="src/clients/c++/request.h" line="1147" column="1" bodyfile="src/clients/c++/request.h" bodystart="1147" bodyend="1202"/>
    <listofallmembers>
      <member refid="classnvidia_1_1inferenceserver_1_1client_1_1InferGrpcContext_1a95e7c7274daeceb82669b9f95aef5054" prot="protected" virt="non-virtual"><scope>nvidia::inferenceserver::client::InferGrpcStreamContext</scope><name>async_request_completion_queue_</name></member>
      <member refid="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1aeb3741fc43d18b9cd7abab193f7ffca0" prot="protected" virt="non-virtual"><scope>nvidia::inferenceserver::client::InferGrpcStreamContext</scope><name>async_request_id_</name></member>
      <member refid="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1a6e1d2a28514245af5f71557a18be70c6" prot="protected" virt="non-virtual"><scope>nvidia::inferenceserver::client::InferGrpcStreamContext</scope><name>AsyncReqMap</name></member>
      <member refid="classnvidia_1_1inferenceserver_1_1client_1_1InferGrpcStreamContext_1a86e4987e666b37c5493373ae1b068d2f" prot="public" virt="virtual"><scope>nvidia::inferenceserver::client::InferGrpcStreamContext</scope><name>AsyncRun</name></member>
      <member refid="classnvidia_1_1inferenceserver_1_1client_1_1InferGrpcStreamContext_1ac0b79bc8230741c954ba5901e0ea330c" prot="private" virt="virtual"><scope>nvidia::inferenceserver::client::InferGrpcStreamContext</scope><name>AsyncTransfer</name></member>
      <member refid="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1a12cc5d7a5db6437fe9fdc4ccc60874a9" prot="protected" virt="non-virtual"><scope>nvidia::inferenceserver::client::InferGrpcStreamContext</scope><name>batch_size_</name></member>
      <member refid="classnvidia_1_1inferenceserver_1_1client_1_1InferGrpcStreamContext_1af1929f62ca9d2e2818034ee1705dbbd6" prot="private" virt="non-virtual"><scope>nvidia::inferenceserver::client::InferGrpcStreamContext</scope><name>context_</name></member>
      <member refid="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1a5fde4cd588ca640d2b17e1768406eb1c" prot="protected" virt="non-virtual"><scope>nvidia::inferenceserver::client::InferGrpcStreamContext</scope><name>context_stat_</name></member>
      <member refid="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1a1836d4a88fbfa609def01ee59a675f4c" prot="protected" virt="non-virtual"><scope>nvidia::inferenceserver::client::InferGrpcStreamContext</scope><name>correlation_id_</name></member>
      <member refid="classnvidia_1_1inferenceserver_1_1client_1_1InferGrpcStreamContext_1a6a356d8b5495f6ba471ed989712c8e64" prot="public" virt="non-virtual"><scope>nvidia::inferenceserver::client::InferGrpcStreamContext</scope><name>Create</name></member>
      <member refid="classnvidia_1_1inferenceserver_1_1client_1_1InferGrpcStreamContext_1aa1f3dc161b5bed3a3e4d73ea997f898c" prot="public" virt="non-virtual"><scope>nvidia::inferenceserver::client::InferGrpcStreamContext</scope><name>Create</name></member>
      <member refid="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1a685dfb2b9634c050b2d552f1613d0bc0" prot="protected" virt="non-virtual"><scope>nvidia::inferenceserver::client::InferGrpcStreamContext</scope><name>cv_</name></member>
      <member refid="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1a5f6d75086a6155fba595351c4e7abdd6" prot="protected" virt="non-virtual"><scope>nvidia::inferenceserver::client::InferGrpcStreamContext</scope><name>exiting_</name></member>
      <member refid="classnvidia_1_1inferenceserver_1_1client_1_1InferGrpcContext_1af701c02c211163d37ac940a9816cf9ab" prot="public" virt="virtual"><scope>nvidia::inferenceserver::client::InferGrpcStreamContext</scope><name>GetAsyncRunResults</name></member>
      <member refid="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1ac8ca982d9c589b2c27c9a279699ac884" prot="public" virt="non-virtual"><scope>nvidia::inferenceserver::client::InferGrpcStreamContext</scope><name>GetInput</name></member>
      <member refid="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1a0d3e9e6e55f3162c7148574af7aec251" prot="public" virt="non-virtual"><scope>nvidia::inferenceserver::client::InferGrpcStreamContext</scope><name>GetOutput</name></member>
      <member refid="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1a27512d1f8f7099d20b2e0eaaac90c3f0" prot="public" virt="non-virtual"><scope>nvidia::inferenceserver::client::InferGrpcStreamContext</scope><name>GetReadyAsyncRequest</name></member>
      <member refid="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1aa083c81909e7a6d10725306d231ea0cb" prot="public" virt="non-virtual"><scope>nvidia::inferenceserver::client::InferGrpcStreamContext</scope><name>GetStat</name></member>
      <member refid="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1ad65a99fa3c6afcf961de531f4421a73d" prot="protected" virt="non-virtual"><scope>nvidia::inferenceserver::client::InferGrpcStreamContext</scope><name>infer_request_</name></member>
      <member refid="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1a1ad0ced372d23d1ed5a0af511ab7aa21" prot="protected" virt="non-virtual"><scope>nvidia::inferenceserver::client::InferGrpcStreamContext</scope><name>InferContext</name></member>
      <member refid="classnvidia_1_1inferenceserver_1_1client_1_1InferGrpcContext_1abcd04c87e15144a95f81dceb54f5dd28" prot="protected" virt="non-virtual"><scope>nvidia::inferenceserver::client::InferGrpcStreamContext</scope><name>InferGrpcContext</name></member>
      <member refid="classnvidia_1_1inferenceserver_1_1client_1_1InferGrpcStreamContext_1aa8b4a62d25916a9fd405591739a6b847" prot="private" virt="non-virtual"><scope>nvidia::inferenceserver::client::InferGrpcStreamContext</scope><name>InferGrpcStreamContext</name></member>
      <member refid="classnvidia_1_1inferenceserver_1_1client_1_1InferGrpcContext_1a9b1f7432db5b39108de896b41f5300cd" prot="protected" virt="non-virtual"><scope>nvidia::inferenceserver::client::InferGrpcStreamContext</scope><name>InitHelper</name></member>
      <member refid="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1a7b98f4f0fa2fe0f948f2a06d91ec92f3" prot="public" virt="non-virtual"><scope>nvidia::inferenceserver::client::InferGrpcStreamContext</scope><name>Inputs</name></member>
      <member refid="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1a2959c35ac38d06693ee9e3482f4b344b" prot="protected" virt="non-virtual"><scope>nvidia::inferenceserver::client::InferGrpcStreamContext</scope><name>inputs_</name></member>
      <member refid="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1ab7437e029e942557e2d11f8c22ae6d6b" prot="protected" virt="non-virtual"><scope>nvidia::inferenceserver::client::InferGrpcStreamContext</scope><name>IsRequestReady</name></member>
      <member refid="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1abfc00d0d8175ac030d881365b7030efd" prot="protected" virt="non-virtual"><scope>nvidia::inferenceserver::client::InferGrpcStreamContext</scope><name>max_batch_size_</name></member>
      <member refid="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1ac9bd9ba3234a0c282383cdd939ca16dc" prot="public" virt="non-virtual"><scope>nvidia::inferenceserver::client::InferGrpcStreamContext</scope><name>MaxBatchSize</name></member>
      <member refid="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1aca65f4b17c14e3c25350c8af3571f86c" prot="protected" virt="non-virtual"><scope>nvidia::inferenceserver::client::InferGrpcStreamContext</scope><name>model_name_</name></member>
      <member refid="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1aea092ac954441e636316644bd94b5979" prot="protected" virt="non-virtual"><scope>nvidia::inferenceserver::client::InferGrpcStreamContext</scope><name>model_version_</name></member>
      <member refid="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1a6b01d3aece1500914c773fb9d88cb44b" prot="public" virt="non-virtual"><scope>nvidia::inferenceserver::client::InferGrpcStreamContext</scope><name>ModelName</name></member>
      <member refid="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1a88642030870fb82b452a48a1676755d0" prot="public" virt="non-virtual"><scope>nvidia::inferenceserver::client::InferGrpcStreamContext</scope><name>ModelVersion</name></member>
      <member refid="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1a1baafc5376df1a6e16de58633c93b4cc" prot="protected" virt="non-virtual"><scope>nvidia::inferenceserver::client::InferGrpcStreamContext</scope><name>mutex_</name></member>
      <member refid="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1aa2dda0b94a39e54ce2d07a3b5b6988b2" prot="protected" virt="non-virtual"><scope>nvidia::inferenceserver::client::InferGrpcStreamContext</scope><name>ongoing_async_requests_</name></member>
      <member refid="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1a4a783616ac9f48e21bd89abb69d0b21a" prot="public" virt="non-virtual"><scope>nvidia::inferenceserver::client::InferGrpcStreamContext</scope><name>Outputs</name></member>
      <member refid="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1a21c9f59353ef1d4215e033f5a4b5d86a" prot="protected" virt="non-virtual"><scope>nvidia::inferenceserver::client::InferGrpcStreamContext</scope><name>outputs_</name></member>
      <member refid="classnvidia_1_1inferenceserver_1_1client_1_1InferGrpcContext_1a5902b0ce9b16f0a055bbc0fc09c69031" prot="protected" virt="virtual"><scope>nvidia::inferenceserver::client::InferGrpcStreamContext</scope><name>PreRunProcessing</name></member>
      <member refid="classnvidia_1_1inferenceserver_1_1client_1_1InferGrpcContext_1a964d67791bbd912f2dc13b1472aaaf33" prot="protected" virt="non-virtual"><scope>nvidia::inferenceserver::client::InferGrpcStreamContext</scope><name>request_</name></member>
      <member refid="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1a8540194150819719c190049fab1e125c" prot="public" virt="non-virtual"><scope>nvidia::inferenceserver::client::InferGrpcStreamContext</scope><name>ResultMap</name></member>
      <member refid="classnvidia_1_1inferenceserver_1_1client_1_1InferGrpcStreamContext_1a5e42ef49aa0ed810934a18ca1dbb71e4" prot="public" virt="virtual"><scope>nvidia::inferenceserver::client::InferGrpcStreamContext</scope><name>Run</name></member>
      <member refid="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1a7cf1746d814561727fd3b07b0f8349f2" prot="public" virt="non-virtual"><scope>nvidia::inferenceserver::client::InferGrpcStreamContext</scope><name>SetRunOptions</name></member>
      <member refid="classnvidia_1_1inferenceserver_1_1client_1_1InferGrpcStreamContext_1a0ac6456b2f6817ebaefe3008cef51f62" prot="private" virt="non-virtual"><scope>nvidia::inferenceserver::client::InferGrpcStreamContext</scope><name>stream_</name></member>
      <member refid="classnvidia_1_1inferenceserver_1_1client_1_1InferGrpcContext_1af175710b44ad280ec46e980228b53398" prot="protected" virt="non-virtual"><scope>nvidia::inferenceserver::client::InferGrpcStreamContext</scope><name>stub_</name></member>
      <member refid="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1a6b2d7784bcf85ee225b574f73d6059ca" prot="protected" virt="non-virtual"><scope>nvidia::inferenceserver::client::InferGrpcStreamContext</scope><name>sync_request_</name></member>
      <member refid="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1a11eff835f49b4d9126e1606a3b2c8615" prot="protected" virt="non-virtual"><scope>nvidia::inferenceserver::client::InferGrpcStreamContext</scope><name>UpdateStat</name></member>
      <member refid="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1a69b72d3b61b07468f5a13c97f36c5e6a" prot="protected" virt="non-virtual"><scope>nvidia::inferenceserver::client::InferGrpcStreamContext</scope><name>verbose_</name></member>
      <member refid="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1a39fdbf2a953b23d0005aef8e2618fcaf" prot="protected" virt="non-virtual"><scope>nvidia::inferenceserver::client::InferGrpcStreamContext</scope><name>worker_</name></member>
      <member refid="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1a3a996794a2f69c24ab50391fcc36ceec" prot="public" virt="virtual"><scope>nvidia::inferenceserver::client::InferGrpcStreamContext</scope><name>~InferContext</name></member>
      <member refid="classnvidia_1_1inferenceserver_1_1client_1_1InferGrpcContext_1afc628d254dba38d178f8dcd606b56328" prot="public" virt="virtual"><scope>nvidia::inferenceserver::client::InferGrpcStreamContext</scope><name>~InferGrpcContext</name></member>
      <member refid="classnvidia_1_1inferenceserver_1_1client_1_1InferGrpcStreamContext_1a378f8f70646b1256db2e78266b08b1c1" prot="public" virt="non-virtual"><scope>nvidia::inferenceserver::client::InferGrpcStreamContext</scope><name>~InferGrpcStreamContext</name></member>
    </listofallmembers>
  </compounddef>
</doxygen>
