<!--
# Copyright (c) 2018, NVIDIA CORPORATION. All rights reserved.
#
# Redistribution and use in source and binary forms, with or without
# modification, are permitted provided that the following conditions
# are met:
#  * Redistributions of source code must retain the above copyright
#    notice, this list of conditions and the following disclaimer.
#  * Redistributions in binary form must reproduce the above copyright
#    notice, this list of conditions and the following disclaimer in the
#    documentation and/or other materials provided with the distribution.
#  * Neither the name of NVIDIA CORPORATION nor the names of its
#    contributors may be used to endorse or promote products derived
#    from this software without specific prior written permission.
#
# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
# EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
# PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
# PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
# OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
# (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
# OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
-->


<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Building &mdash; NVIDIA TensorRT Inference Server 0.12.0dev
 documentation</title>
  

  
  
  
  
    <link rel="canonical" href="https://docs.nvidia.com/deeplearning/sdk/tensorrt-inference-server-guide/docs/index.htmlbuild.html"/>
  

  
  <script type="text/javascript" src="_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
        <script type="text/javascript" src="_static/jquery.js"></script>
        <script type="text/javascript" src="_static/underscore.js"></script>
        <script type="text/javascript" src="_static/doctools.js"></script>
        <script type="text/javascript" src="_static/language_data.js"></script>
        <script type="text/javascript" src="_static/collapsible-lists/js/CollapsibleLists.compressed.js"></script>
        <script type="text/javascript" src="_static/collapsible-lists/js/apply-collapsible-lists.js"></script>
    
    <script type="text/javascript" src="_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="_static/collapsible-lists/css/tree_view.css" type="text/css" />
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Testing" href="test.html" />
    <link rel="prev" title="Contributing" href="contribute.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
           

          
            <a href="index.html" class="icon icon-home"> NVIDIA TensorRT Inference Server
          

          
          </a>

          
            
            
              <div class="version">
                0.12.0dev
-ad20b82<br/>
Version select: <select onChange="window.location.href = this.value" onFocus="this.selectedIndex = -1">
    <option value="https://docs.nvidia.com/deeplearning/sdk/tensorrt-inference-server-guide/docs/index.html">Current release</option>
    <option value="https://docs.nvidia.com/deeplearning/sdk/tensorrt-inference-server-master-branch-guide/docs/index.html">master (unstable)</option>
    <option value="https://docs.nvidia.com/deeplearning/sdk/inference-server-archived/index.html">Older releases</option>
</select>
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          

  <style>
    /* Sidebar header (and topbar for mobile) */
    .wy-side-nav-search, .wy-nav-top {
      background: #76b900;
    }

    .wy-side-nav-search a:link, .wy-nav-top a:link {
      color: #fff;
    }
    .wy-side-nav-search a:visited, .wy-nav-top a:visited {
      color: #fff;
    }
    .wy-side-nav-search a:hover, .wy-nav-top a:hover {
      color: #fff;
    }

    .wy-menu-vertical a:link, .wy-menu-vertical a:visited {
      color: #d9d9d9
    }

    .wy-menu-vertical a:active {
      background-color: #76b900
    }

    .wy-side-nav-search>div.version {
      color: rgba(0, 0, 0, 0.3)
    }
  </style>
  
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul>
<li class="toctree-l1"><a class="reference internal" href="index.html">Documentation home</a></li>
</ul>
<p class="caption"><span class="caption-text">User Guide</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="quickstart.html">Quickstart</a><ul>
<li class="toctree-l2"><a class="reference internal" href="quickstart.html#prerequisites">Prerequisites</a></li>
<li class="toctree-l2"><a class="reference internal" href="quickstart.html#using-a-prebuilt-docker-container">Using A Prebuilt Docker Container</a></li>
<li class="toctree-l2"><a class="reference internal" href="quickstart.html#building-from-source-code">Building From Source Code</a></li>
<li class="toctree-l2"><a class="reference internal" href="quickstart.html#run-tensorrt-inference-server">Run TensorRT Inference Server</a></li>
<li class="toctree-l2"><a class="reference internal" href="quickstart.html#verify-inference-server-is-running-correctly">Verify Inference Server Is Running Correctly</a></li>
<li class="toctree-l2"><a class="reference internal" href="quickstart.html#building-the-client-examples">Building The Client Examples</a></li>
<li class="toctree-l2"><a class="reference internal" href="quickstart.html#running-the-image-classification-example">Running The Image Classification Example</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="install.html">Installing the Server</a><ul>
<li class="toctree-l2"><a class="reference internal" href="install.html#installing-prebuilt-containers">Installing Prebuilt Containers</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="run.html">Running the Server</a><ul>
<li class="toctree-l2"><a class="reference internal" href="run.html#example-model-repository">Example Model Repository</a></li>
<li class="toctree-l2"><a class="reference internal" href="run.html#running-the-inference-server">Running The Inference Server</a></li>
<li class="toctree-l2"><a class="reference internal" href="run.html#running-the-inference-server-on-a-system-without-a-gpu">Running The Inference Server On A System Without A GPU</a></li>
<li class="toctree-l2"><a class="reference internal" href="run.html#checking-inference-server-status">Checking Inference Server Status</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="client.html">Client Libraries and Examples</a><ul>
<li class="toctree-l2"><a class="reference internal" href="client.html#building-the-client-libraries-and-examples">Building the Client Libraries and Examples</a></li>
<li class="toctree-l2"><a class="reference internal" href="client.html#image-classification-example-application">Image Classification Example Application</a></li>
<li class="toctree-l2"><a class="reference internal" href="client.html#performance-example-application">Performance Example Application</a></li>
<li class="toctree-l2"><a class="reference internal" href="client.html#client-api">Client API</a><ul>
<li class="toctree-l3"><a class="reference internal" href="client.html#string-datatype">String Datatype</a></li>
<li class="toctree-l3"><a class="reference internal" href="client.html#stream-inference">Stream Inference</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="model_repository.html">Model Repository</a><ul>
<li class="toctree-l2"><a class="reference internal" href="model_repository.html#modifying-the-model-repository">Modifying the Model Repository</a></li>
<li class="toctree-l2"><a class="reference internal" href="model_repository.html#model-versions">Model Versions</a></li>
<li class="toctree-l2"><a class="reference internal" href="model_repository.html#framework-model-definition">Framework Model Definition</a><ul>
<li class="toctree-l3"><a class="reference internal" href="model_repository.html#tensorrt-models">TensorRT Models</a></li>
<li class="toctree-l3"><a class="reference internal" href="model_repository.html#tensorflow-models">TensorFlow Models</a></li>
<li class="toctree-l3"><a class="reference internal" href="model_repository.html#caffe2-models">Caffe2 Models</a></li>
<li class="toctree-l3"><a class="reference internal" href="model_repository.html#tensorrt-tensorflow-models">TensorRT/TensorFlow Models</a></li>
<li class="toctree-l3"><a class="reference internal" href="model_repository.html#onnx-models">ONNX Models</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="model_repository.html#custom-backends">Custom Backends</a><ul>
<li class="toctree-l3"><a class="reference internal" href="model_repository.html#custom-backend-api">Custom Backend API</a></li>
<li class="toctree-l3"><a class="reference internal" href="model_repository.html#example-custom-backend">Example Custom Backend</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="model_configuration.html">Model Configuration</a><ul>
<li class="toctree-l2"><a class="reference internal" href="model_configuration.html#generated-model-configuration">Generated Model Configuration</a></li>
<li class="toctree-l2"><a class="reference internal" href="model_configuration.html#datatypes">Datatypes</a></li>
<li class="toctree-l2"><a class="reference internal" href="model_configuration.html#version-policy">Version Policy</a></li>
<li class="toctree-l2"><a class="reference internal" href="model_configuration.html#instance-groups">Instance Groups</a></li>
<li class="toctree-l2"><a class="reference internal" href="model_configuration.html#dynamic-batching">Dynamic Batching</a></li>
<li class="toctree-l2"><a class="reference internal" href="model_configuration.html#optimization-policy">Optimization Policy</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="http_grpc_api.html">Inference Server API</a><ul>
<li class="toctree-l2"><a class="reference internal" href="http_grpc_api.html#health">Health</a></li>
<li class="toctree-l2"><a class="reference internal" href="http_grpc_api.html#status">Status</a></li>
<li class="toctree-l2"><a class="reference internal" href="http_grpc_api.html#inference">Inference</a></li>
<li class="toctree-l2"><a class="reference internal" href="http_grpc_api.html#stream-inference">Stream Inference</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="metrics.html">Metrics</a></li>
</ul>
<p class="caption"><span class="caption-text">Developer Guide</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="architecture.html">Architecture</a><ul>
<li class="toctree-l2"><a class="reference internal" href="architecture.html#concurrent-model-execution">Concurrent Model Execution</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="contribute.html">Contributing</a><ul>
<li class="toctree-l2"><a class="reference internal" href="contribute.html#coding-convention">Coding Convention</a></li>
</ul>
</li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Building</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#building-the-server">Building the Server</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#incremental-builds">Incremental Builds</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#building-the-client-libraries-and-examples">Building the Client Libraries and Examples</a></li>
<li class="toctree-l2"><a class="reference internal" href="#building-the-documentation">Building the Documentation</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#html-documentation">HTML Documentation</a></li>
<li class="toctree-l3"><a class="reference internal" href="#pdf-documentation">PDF Documentation</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="test.html">Testing</a><ul>
<li class="toctree-l2"><a class="reference internal" href="test.html#generate-qa-model-repositories">Generate QA Model Repositories</a></li>
<li class="toctree-l2"><a class="reference internal" href="test.html#build-qa-container">Build QA Container</a></li>
<li class="toctree-l2"><a class="reference internal" href="test.html#run-qa-container">Run QA Container</a></li>
</ul>
</li>
</ul>
<p class="caption"><span class="caption-text">API Reference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="protobuf_api/protobuf_api_root.html">Protobuf API</a><ul>
<li class="toctree-l2"><a class="reference internal" href="protobuf_api/protobuf_api_root.html#http-grpc-api">HTTP/GRPC API</a></li>
<li class="toctree-l2"><a class="reference internal" href="protobuf_api/protobuf_api_root.html#model-configuration">Model Configuration</a></li>
<li class="toctree-l2"><a class="reference internal" href="protobuf_api/protobuf_api_root.html#status">Status</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="cpp_api/cpp_api_root.html">C++ API</a><ul>
<li class="toctree-l2"><a class="reference internal" href="cpp_api/cpp_api_root.html#class-hierarchy">Class Hierarchy</a></li>
<li class="toctree-l2"><a class="reference internal" href="cpp_api/cpp_api_root.html#file-hierarchy">File Hierarchy</a></li>
<li class="toctree-l2"><a class="reference internal" href="cpp_api/cpp_api_root.html#full-api">Full API</a><ul>
<li class="toctree-l3"><a class="reference internal" href="cpp_api/cpp_api_root.html#namespaces">Namespaces</a><ul>
<li class="toctree-l4"><a class="reference internal" href="cpp_api/namespace_nvidia.html">Namespace nvidia</a></li>
<li class="toctree-l4"><a class="reference internal" href="cpp_api/namespace_nvidia__inferenceserver.html">Namespace nvidia::inferenceserver</a></li>
<li class="toctree-l4"><a class="reference internal" href="cpp_api/namespace_nvidia__inferenceserver__client.html">Namespace nvidia::inferenceserver::client</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="cpp_api/cpp_api_root.html#classes-and-structs">Classes and Structs</a><ul>
<li class="toctree-l4"><a class="reference internal" href="cpp_api/structcustom__payload__struct.html">Struct custom_payload_struct</a></li>
<li class="toctree-l4"><a class="reference internal" href="cpp_api/structnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result_1_1ClassResult.html">Struct Result::ClassResult</a></li>
<li class="toctree-l4"><a class="reference internal" href="cpp_api/structnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Stat.html">Struct InferContext::Stat</a></li>
<li class="toctree-l4"><a class="reference internal" href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1Error.html">Class Error</a></li>
<li class="toctree-l4"><a class="reference internal" href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext.html">Class InferContext</a></li>
<li class="toctree-l4"><a class="reference internal" href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Input.html">Class InferContext::Input</a></li>
<li class="toctree-l4"><a class="reference internal" href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Options.html">Class InferContext::Options</a></li>
<li class="toctree-l4"><a class="reference internal" href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Output.html">Class InferContext::Output</a></li>
<li class="toctree-l4"><a class="reference internal" href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Request.html">Class InferContext::Request</a></li>
<li class="toctree-l4"><a class="reference internal" href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1RequestTimers.html">Class InferContext::RequestTimers</a></li>
<li class="toctree-l4"><a class="reference internal" href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result.html">Class InferContext::Result</a></li>
<li class="toctree-l4"><a class="reference internal" href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferGrpcContext.html">Class InferGrpcContext</a></li>
<li class="toctree-l4"><a class="reference internal" href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferGrpcStreamContext.html">Class InferGrpcStreamContext</a></li>
<li class="toctree-l4"><a class="reference internal" href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferHttpContext.html">Class InferHttpContext</a></li>
<li class="toctree-l4"><a class="reference internal" href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ProfileContext.html">Class ProfileContext</a></li>
<li class="toctree-l4"><a class="reference internal" href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ProfileGrpcContext.html">Class ProfileGrpcContext</a></li>
<li class="toctree-l4"><a class="reference internal" href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ProfileHttpContext.html">Class ProfileHttpContext</a></li>
<li class="toctree-l4"><a class="reference internal" href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ServerHealthContext.html">Class ServerHealthContext</a></li>
<li class="toctree-l4"><a class="reference internal" href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ServerHealthGrpcContext.html">Class ServerHealthGrpcContext</a></li>
<li class="toctree-l4"><a class="reference internal" href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ServerHealthHttpContext.html">Class ServerHealthHttpContext</a></li>
<li class="toctree-l4"><a class="reference internal" href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ServerStatusContext.html">Class ServerStatusContext</a></li>
<li class="toctree-l4"><a class="reference internal" href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ServerStatusGrpcContext.html">Class ServerStatusGrpcContext</a></li>
<li class="toctree-l4"><a class="reference internal" href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ServerStatusHttpContext.html">Class ServerStatusHttpContext</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="cpp_api/cpp_api_root.html#functions">Functions</a><ul>
<li class="toctree-l4"><a class="reference internal" href="cpp_api/function_custom_8h_1aedfdcb69f8eaab594173b197b10dc804.html">Function CustomErrorString</a></li>
<li class="toctree-l4"><a class="reference internal" href="cpp_api/function_custom_8h_1afc75c0a4e1a169562586c2d4ff09ba40.html">Function CustomExecute</a></li>
<li class="toctree-l4"><a class="reference internal" href="cpp_api/function_custom_8h_1a9cd799dfef77db82babc6ddd4a4be707.html">Function CustomFinalize</a></li>
<li class="toctree-l4"><a class="reference internal" href="cpp_api/function_custom_8h_1ad384f6af9bfb11b82b2b4e411f799b6f.html">Function CustomInitialize</a></li>
<li class="toctree-l4"><a class="reference internal" href="cpp_api/function_request_8h_1a19d352bb13848c14b5f03274b25ecee3.html">Function nvidia::inferenceserver::client::operator&lt;&lt;</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="cpp_api/cpp_api_root.html#defines">Defines</a><ul>
<li class="toctree-l4"><a class="reference internal" href="cpp_api/define_custom_8h_1aaf537da73bb55fda05fb4e39defe1949.html">Define CUSTOM_NO_GPU_DEVICE</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="cpp_api/cpp_api_root.html#typedefs">Typedefs</a><ul>
<li class="toctree-l4"><a class="reference internal" href="cpp_api/typedef_custom_8h_1aa6616d50105d3441ba277c6dbce66286.html">Typedef CustomErrorStringFn_t</a></li>
<li class="toctree-l4"><a class="reference internal" href="cpp_api/typedef_custom_8h_1ac0c3508cd00e9db56912ff40deecf9d2.html">Typedef CustomExecuteFn_t</a></li>
<li class="toctree-l4"><a class="reference internal" href="cpp_api/typedef_custom_8h_1aa32f51734df1f217cc7c2b07f31f8f3f.html">Typedef CustomFinalizeFn_t</a></li>
<li class="toctree-l4"><a class="reference internal" href="cpp_api/typedef_custom_8h_1aee3723f1a17ff4664691655e056a309c.html">Typedef CustomGetNextInputFn_t</a></li>
<li class="toctree-l4"><a class="reference internal" href="cpp_api/typedef_custom_8h_1a71cb419d6e7feee902115438973d02f8.html">Typedef CustomGetOutputFn_t</a></li>
<li class="toctree-l4"><a class="reference internal" href="cpp_api/typedef_custom_8h_1a11fea108abb0b28f86a96e6b7cd92a20.html">Typedef CustomInitializeFn_t</a></li>
<li class="toctree-l4"><a class="reference internal" href="cpp_api/typedef_custom_8h_1a99a7b9dcc756702784a869d9afb59df4.html">Typedef CustomPayload</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="cpp_api/cpp_api_root.html#directories">Directories</a><ul>
<li class="toctree-l4"><a class="reference internal" href="cpp_api/dir_src.html">Directory src</a></li>
<li class="toctree-l4"><a class="reference internal" href="cpp_api/dir_src_clients.html">Directory clients</a></li>
<li class="toctree-l4"><a class="reference internal" href="cpp_api/dir_src_clients_c++.html">Directory c++</a></li>
<li class="toctree-l4"><a class="reference internal" href="cpp_api/dir_src_servables.html">Directory servables</a></li>
<li class="toctree-l4"><a class="reference internal" href="cpp_api/dir_src_servables_custom.html">Directory custom</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="cpp_api/cpp_api_root.html#files">Files</a><ul>
<li class="toctree-l4"><a class="reference internal" href="cpp_api/file_src_servables_custom_custom.h.html">File custom.h</a></li>
<li class="toctree-l4"><a class="reference internal" href="cpp_api/file_src_clients_c++_request.h.html">File request.h</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="python_api.html">Python API</a><ul>
<li class="toctree-l2"><a class="reference internal" href="python_api.html#client">Client</a></li>
</ul>
</li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">NVIDIA TensorRT Inference Server</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html">Docs</a> &raquo;</li>
        
      <li>Building</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="_sources/build.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  
<style>
/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast,
.nboutput.nblast {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast + .nbinput {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<div class="section" id="building">
<h1>Building<a class="headerlink" href="#building" title="Permalink to this headline">¶</a></h1>
<p>The TensorRT Inference Server is built using Docker and the TensorFlow
and PyTorch containers from <a class="reference external" href="https://ngc.nvidia.com">NVIDIA GPU Cloud (NGC)</a>. Before building you must install Docker
and nvidia-docker and login to the NGC registry by following the
instructions in <a class="reference internal" href="install.html#section-installing-prebuilt-containers"><span class="std std-ref">Installing Prebuilt Containers</span></a>.</p>
<div class="section" id="building-the-server">
<span id="section-building-the-server"></span><h2>Building the Server<a class="headerlink" href="#building-the-server" title="Permalink to this headline">¶</a></h2>
<p>To build a release version of the TensorRT Inference Server container,
change directory to the root of the repo and issue the following
command:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>$ docker build --pull -t tensorrtserver .
</pre></div>
</div>
<div class="section" id="incremental-builds">
<h3>Incremental Builds<a class="headerlink" href="#incremental-builds" title="Permalink to this headline">¶</a></h3>
<p>For typical development you will want to run the <em>build</em> container
with your local repo’s source files mounted so that your local changes
can be incrementally built. This is done by first building the
<em>tensorrtserver_build</em> container:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>$ docker build --pull -t tensorrtserver_build --target trtserver_build .
</pre></div>
</div>
<p>By mounting /path/to/tensorrtserver/src into the container at
/workspace/src, changes to your local repo will be reflected in the
container:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>$ nvidia-docker run -it --rm -v/path/to/tensorrtserver/src:/workspace/src tensorrtserver_build
</pre></div>
</div>
<p>Within the container you can perform an incremental server build
with:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span># cd /workspace
# bazel build -c opt --config=cuda src/servers/trtserver
# cp /workspace/bazel-bin/src/servers/trtserver /opt/tensorrtserver/bin/trtserver
</pre></div>
</div>
<p>Similarly, within the container you can perform an incremental build
of the C++ and Python client libraries and example executables with:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span># cd /workspace
# bazel build -c opt --config=cuda src/clients/…
# mkdir -p /opt/tensorrtserver/bin
# cp bazel-bin/src/clients/c++/image_client /opt/tensorrtserver/bin/.
# cp bazel-bin/src/clients/c++/perf_client /opt/tensorrtserver/bin/.
# cp bazel-bin/src/clients/c++/simple_client /opt/tensorrtserver/bin/.
# mkdir -p /opt/tensorrtserver/lib
# cp bazel-bin/src/clients/c++/librequest.so /opt/tensorrtserver/lib/.
# cp bazel-bin/src/clients/c++/librequest.a /opt/tensorrtserver/lib/.
# mkdir -p /opt/tensorrtserver/pip
# bazel-bin/src/clients/python/build_pip /opt/tensorrtserver/pip/.
</pre></div>
</div>
<p>Some source changes seem to cause bazel to get confused and not
correctly rebuild all required sources. You can force bazel to rebuild
all of the inference server source without requiring a complete
rebuild of the TensorFlow and Caffe2 components by doing the following
before issuing the above build command:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span># rm -fr bazel-bin/src
</pre></div>
</div>
</div>
</div>
<div class="section" id="building-the-client-libraries-and-examples">
<span id="section-building-the-client-libraries-and-examples"></span><h2>Building the Client Libraries and Examples<a class="headerlink" href="#building-the-client-libraries-and-examples" title="Permalink to this headline">¶</a></h2>
<p>The provided Dockerfile can be used to build just the client libraries
and examples. Issue the following command to build the C++ client
library, C++ and Python examples, and a Python wheel file for the
Python client library:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>$ docker build -t tensorrtserver_clients --target trtserver_build --build-arg &quot;BUILD_CLIENTS_ONLY=1&quot; .
</pre></div>
</div>
<p>You can optionally add <em>--build-arg “PYVER=&lt;ver&gt;”</em> to set the Python
version that you want the Python client library built for. Supported
values for <em>&lt;ver&gt;</em> are 2.6 and 3.5, with 3.5 being the default.</p>
<p>After the build completes the tensorrtserver_clients docker image will
contain the built client libraries and examples. The easiest way to
try the examples described in the following sections is to run the
client image with --net=host so that the client examples can access
the inference server running in its own container (see
<a class="reference internal" href="run.html#section-running-the-inference-server"><span class="std std-ref">Running The Inference Server</span></a> for more information about
running the inference server):</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>$ docker run -it --rm --net=host tensorrtserver_clients
</pre></div>
</div>
<p>In the client image you can find the example executables in
/opt/tensorrtserver/bin, and the Python wheel in
/opt/tensorrtserver/pip.</p>
<p>If your host sytem is Ubuntu-16.04, an alternative to running the
examples within the tensorrtserver_clients container is to instead
copy the libraries and examples from the docker image to the host
system:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>$ docker run -it --rm -v/tmp:/tmp/host tensorrtserver_clients
# cp /opt/tensorrtserver/bin/image_client /tmp/host/.
# cp /opt/tensorrtserver/bin/perf_client /tmp/host/.
# cp /opt/tensorrtserver/bin/simple_client /tmp/host/.
# cp /opt/tensorrtserver/pip/tensorrtserver-*.whl /tmp/host/.
# cp /opt/tensorrtserver/lib/librequest.* /tmp/host/.
</pre></div>
</div>
<p>You can now access the files from /tmp on the host system. To run the
C++ examples you must install some dependencies on your host system:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>$ apt-get install curl libcurl3-dev libopencv-dev libopencv-core-dev
</pre></div>
</div>
<p>To run the Python examples you will need to additionally install the
client whl file and some other dependencies:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>$ apt-get install python3 python3-pip
$ pip3 install --user --upgrade tensorrtserver-*.whl numpy pillow
</pre></div>
</div>
</div>
<div class="section" id="building-the-documentation">
<h2>Building the Documentation<a class="headerlink" href="#building-the-documentation" title="Permalink to this headline">¶</a></h2>
<p>The inference server documentation is found in the docs/ directory and
is based on <a class="reference external" href="http://www.sphinx-doc.org">Sphinx</a>. <a class="reference external" href="http://www.doxygen.org/">Doxygen</a> integrated with <a class="reference external" href="https://github.com/svenevs/exhale">Exhale</a> is used for C++ API
docuementation.</p>
<div class="section" id="html-documentation">
<h3>HTML Documentation<a class="headerlink" href="#html-documentation" title="Permalink to this headline">¶</a></h3>
<p>To build the docs install the required dependencies:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>$ apt-get update
$ apt-get install -y --no-install-recommends doxygen
$ pip install --upgrade sphinx sphinx-rtd-theme nbsphinx exhale
</pre></div>
</div>
<p>To get the Python client library API docs the TensorRT Inference
Server Python package must be installed:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>$ pip install --upgrade tensorrtserver-*.whl
</pre></div>
</div>
<p>Then use Sphinx to build the documentation into the build/html
directory:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>$ cd docs
$ make clean html
</pre></div>
</div>
</div>
<div class="section" id="pdf-documentation">
<h3>PDF Documentation<a class="headerlink" href="#pdf-documentation" title="Permalink to this headline">¶</a></h3>
<p>To build the PDF version of documentation <a class="reference external" href="https://www.tug.org/texlive/quickinstall.html">LaTEX</a>
needs to be installed first. Additional requirements of python modules
can be met by simply running the following commands in docs directory:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>$ pip install -r requirements.txt
</pre></div>
</div>
<p>Once latex and python modules have been installed and updated single PDF for documentation
can be generated by running the following command. It will generate <em>NVIDIATRTIS.pdf</em> in <em>build/latex</em>
directory:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>$make clean latexpdf
</pre></div>
</div>
<p>Before creating a pull request for merging changes in documentation one can validate the same locally. It can be done
by running simple HTTP server in <em>docs/build</em> diretory with command below and navigating to the documentation pages in
the browser at <a class="reference external" href="http://localhost:8000">http://localhost:8000</a>.</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>$ python -m SimpleHTTPServer
</pre></div>
</div>
</div>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="test.html" class="btn btn-neutral float-right" title="Testing" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="contribute.html" class="btn btn-neutral float-left" title="Contributing" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2018, NVIDIA Corporation

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
    

  <style>
  a:link, a:visited {
    color: #76b900;
  }

  a:hover {
    color: #8c0;
  }

  .rst-content dl:not(.docutils) dt {
    background: rgba(118, 185, 0, 0.1);
    color: rgba(59,93,0,1);
    border-top: solid 3px rgba(59,93,0,1);
  }
  </style>
  

</body>
</html>