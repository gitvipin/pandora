<!--
# Copyright (c) 2018, NVIDIA CORPORATION. All rights reserved.
#
# Redistribution and use in source and binary forms, with or without
# modification, are permitted provided that the following conditions
# are met:
#  * Redistributions of source code must retain the above copyright
#    notice, this list of conditions and the following disclaimer.
#  * Redistributions in binary form must reproduce the above copyright
#    notice, this list of conditions and the following disclaimer in the
#    documentation and/or other materials provided with the distribution.
#  * Neither the name of NVIDIA CORPORATION nor the names of its
#    contributors may be used to endorse or promote products derived
#    from this software without specific prior written permission.
#
# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
# EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
# PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
# PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
# OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
# (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
# OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
-->


<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Class InferContext::Options &mdash; NVIDIA TensorRT Inference Server 0.12.0dev
 documentation</title>
  

  
  
  
  
    <link rel="canonical" href="https://docs.nvidia.com/deeplearning/sdk/tensorrt-inference-server-guide/docs/index.htmlcpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Options.html"/>
  

  
  <script type="text/javascript" src="../_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script type="text/javascript" src="../_static/jquery.js"></script>
        <script type="text/javascript" src="../_static/underscore.js"></script>
        <script type="text/javascript" src="../_static/doctools.js"></script>
        <script type="text/javascript" src="../_static/language_data.js"></script>
        <script type="text/javascript" src="../_static/collapsible-lists/js/CollapsibleLists.compressed.js"></script>
        <script type="text/javascript" src="../_static/collapsible-lists/js/apply-collapsible-lists.js"></script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/collapsible-lists/css/tree_view.css" type="text/css" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Class InferContext::Output" href="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Output.html" />
    <link rel="prev" title="Class InferContext::Input" href="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Input.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
           

          
            <a href="../index.html" class="icon icon-home"> NVIDIA TensorRT Inference Server
          

          
          </a>

          
            
            
              <div class="version">
                0.12.0dev
-ad20b82<br/>
Version select: <select onChange="window.location.href = this.value" onFocus="this.selectedIndex = -1">
    <option value="https://docs.nvidia.com/deeplearning/sdk/tensorrt-inference-server-guide/docs/index.html">Current release</option>
    <option value="https://docs.nvidia.com/deeplearning/sdk/tensorrt-inference-server-master-branch-guide/docs/index.html">master (unstable)</option>
    <option value="https://docs.nvidia.com/deeplearning/sdk/inference-server-archived/index.html">Older releases</option>
</select>
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          

  <style>
    /* Sidebar header (and topbar for mobile) */
    .wy-side-nav-search, .wy-nav-top {
      background: #76b900;
    }

    .wy-side-nav-search a:link, .wy-nav-top a:link {
      color: #fff;
    }
    .wy-side-nav-search a:visited, .wy-nav-top a:visited {
      color: #fff;
    }
    .wy-side-nav-search a:hover, .wy-nav-top a:hover {
      color: #fff;
    }

    .wy-menu-vertical a:link, .wy-menu-vertical a:visited {
      color: #d9d9d9
    }

    .wy-menu-vertical a:active {
      background-color: #76b900
    }

    .wy-side-nav-search>div.version {
      color: rgba(0, 0, 0, 0.3)
    }
  </style>
  
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul>
<li class="toctree-l1"><a class="reference internal" href="../index.html">Documentation home</a></li>
</ul>
<p class="caption"><span class="caption-text">User Guide</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../quickstart.html">Quickstart</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../quickstart.html#prerequisites">Prerequisites</a></li>
<li class="toctree-l2"><a class="reference internal" href="../quickstart.html#using-a-prebuilt-docker-container">Using A Prebuilt Docker Container</a></li>
<li class="toctree-l2"><a class="reference internal" href="../quickstart.html#building-from-source-code">Building From Source Code</a></li>
<li class="toctree-l2"><a class="reference internal" href="../quickstart.html#run-tensorrt-inference-server">Run TensorRT Inference Server</a></li>
<li class="toctree-l2"><a class="reference internal" href="../quickstart.html#verify-inference-server-is-running-correctly">Verify Inference Server Is Running Correctly</a></li>
<li class="toctree-l2"><a class="reference internal" href="../quickstart.html#building-the-client-examples">Building The Client Examples</a></li>
<li class="toctree-l2"><a class="reference internal" href="../quickstart.html#running-the-image-classification-example">Running The Image Classification Example</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../install.html">Installing the Server</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../install.html#installing-prebuilt-containers">Installing Prebuilt Containers</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../run.html">Running the Server</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../run.html#example-model-repository">Example Model Repository</a></li>
<li class="toctree-l2"><a class="reference internal" href="../run.html#running-the-inference-server">Running The Inference Server</a></li>
<li class="toctree-l2"><a class="reference internal" href="../run.html#running-the-inference-server-on-a-system-without-a-gpu">Running The Inference Server On A System Without A GPU</a></li>
<li class="toctree-l2"><a class="reference internal" href="../run.html#checking-inference-server-status">Checking Inference Server Status</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../client.html">Client Libraries and Examples</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../client.html#building-the-client-libraries-and-examples">Building the Client Libraries and Examples</a></li>
<li class="toctree-l2"><a class="reference internal" href="../client.html#image-classification-example-application">Image Classification Example Application</a></li>
<li class="toctree-l2"><a class="reference internal" href="../client.html#performance-example-application">Performance Example Application</a></li>
<li class="toctree-l2"><a class="reference internal" href="../client.html#client-api">Client API</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../client.html#string-datatype">String Datatype</a></li>
<li class="toctree-l3"><a class="reference internal" href="../client.html#stream-inference">Stream Inference</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../model_repository.html">Model Repository</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../model_repository.html#modifying-the-model-repository">Modifying the Model Repository</a></li>
<li class="toctree-l2"><a class="reference internal" href="../model_repository.html#model-versions">Model Versions</a></li>
<li class="toctree-l2"><a class="reference internal" href="../model_repository.html#framework-model-definition">Framework Model Definition</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../model_repository.html#tensorrt-models">TensorRT Models</a></li>
<li class="toctree-l3"><a class="reference internal" href="../model_repository.html#tensorflow-models">TensorFlow Models</a></li>
<li class="toctree-l3"><a class="reference internal" href="../model_repository.html#caffe2-models">Caffe2 Models</a></li>
<li class="toctree-l3"><a class="reference internal" href="../model_repository.html#tensorrt-tensorflow-models">TensorRT/TensorFlow Models</a></li>
<li class="toctree-l3"><a class="reference internal" href="../model_repository.html#onnx-models">ONNX Models</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../model_repository.html#custom-backends">Custom Backends</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../model_repository.html#custom-backend-api">Custom Backend API</a></li>
<li class="toctree-l3"><a class="reference internal" href="../model_repository.html#example-custom-backend">Example Custom Backend</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../model_configuration.html">Model Configuration</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../model_configuration.html#generated-model-configuration">Generated Model Configuration</a></li>
<li class="toctree-l2"><a class="reference internal" href="../model_configuration.html#datatypes">Datatypes</a></li>
<li class="toctree-l2"><a class="reference internal" href="../model_configuration.html#version-policy">Version Policy</a></li>
<li class="toctree-l2"><a class="reference internal" href="../model_configuration.html#instance-groups">Instance Groups</a></li>
<li class="toctree-l2"><a class="reference internal" href="../model_configuration.html#dynamic-batching">Dynamic Batching</a></li>
<li class="toctree-l2"><a class="reference internal" href="../model_configuration.html#optimization-policy">Optimization Policy</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../http_grpc_api.html">Inference Server API</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../http_grpc_api.html#health">Health</a></li>
<li class="toctree-l2"><a class="reference internal" href="../http_grpc_api.html#status">Status</a></li>
<li class="toctree-l2"><a class="reference internal" href="../http_grpc_api.html#inference">Inference</a></li>
<li class="toctree-l2"><a class="reference internal" href="../http_grpc_api.html#stream-inference">Stream Inference</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../metrics.html">Metrics</a></li>
</ul>
<p class="caption"><span class="caption-text">Developer Guide</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../architecture.html">Architecture</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../architecture.html#concurrent-model-execution">Concurrent Model Execution</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../contribute.html">Contributing</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../contribute.html#coding-convention">Coding Convention</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../build.html">Building</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../build.html#building-the-server">Building the Server</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../build.html#incremental-builds">Incremental Builds</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../build.html#building-the-client-libraries-and-examples">Building the Client Libraries and Examples</a></li>
<li class="toctree-l2"><a class="reference internal" href="../build.html#building-the-documentation">Building the Documentation</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../build.html#html-documentation">HTML Documentation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../build.html#pdf-documentation">PDF Documentation</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../build.html#updating-documentation">Updating Documentation</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../test.html">Testing</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../test.html#generate-qa-model-repositories">Generate QA Model Repositories</a></li>
<li class="toctree-l2"><a class="reference internal" href="../test.html#build-qa-container">Build QA Container</a></li>
<li class="toctree-l2"><a class="reference internal" href="../test.html#run-qa-container">Run QA Container</a></li>
</ul>
</li>
</ul>
<p class="caption"><span class="caption-text">API Reference</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../protobuf_api/protobuf_api_root.html">Protobuf API</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../protobuf_api/protobuf_api_root.html#http-grpc-api">HTTP/GRPC API</a></li>
<li class="toctree-l2"><a class="reference internal" href="../protobuf_api/protobuf_api_root.html#model-configuration">Model Configuration</a></li>
<li class="toctree-l2"><a class="reference internal" href="../protobuf_api/protobuf_api_root.html#status">Status</a></li>
</ul>
</li>
<li class="toctree-l1 current"><a class="reference internal" href="cpp_api_root.html">C++ API</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="cpp_api_root.html#class-hierarchy">Class Hierarchy</a></li>
<li class="toctree-l2"><a class="reference internal" href="cpp_api_root.html#file-hierarchy">File Hierarchy</a></li>
<li class="toctree-l2 current"><a class="reference internal" href="cpp_api_root.html#full-api">Full API</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="cpp_api_root.html#namespaces">Namespaces</a><ul>
<li class="toctree-l4"><a class="reference internal" href="namespace_nvidia.html">Namespace nvidia</a></li>
<li class="toctree-l4"><a class="reference internal" href="namespace_nvidia__inferenceserver.html">Namespace nvidia::inferenceserver</a></li>
<li class="toctree-l4"><a class="reference internal" href="namespace_nvidia__inferenceserver__client.html">Namespace nvidia::inferenceserver::client</a></li>
</ul>
</li>
<li class="toctree-l3 current"><a class="reference internal" href="cpp_api_root.html#classes-and-structs">Classes and Structs</a><ul class="current">
<li class="toctree-l4"><a class="reference internal" href="structcustom__payload__struct.html">Struct custom_payload_struct</a></li>
<li class="toctree-l4"><a class="reference internal" href="structnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result_1_1ClassResult.html">Struct Result::ClassResult</a></li>
<li class="toctree-l4"><a class="reference internal" href="structnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Stat.html">Struct InferContext::Stat</a></li>
<li class="toctree-l4"><a class="reference internal" href="classnvidia_1_1inferenceserver_1_1client_1_1Error.html">Class Error</a></li>
<li class="toctree-l4"><a class="reference internal" href="classnvidia_1_1inferenceserver_1_1client_1_1InferContext.html">Class InferContext</a></li>
<li class="toctree-l4"><a class="reference internal" href="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Input.html">Class InferContext::Input</a></li>
<li class="toctree-l4 current"><a class="current reference internal" href="#">Class InferContext::Options</a></li>
<li class="toctree-l4"><a class="reference internal" href="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Output.html">Class InferContext::Output</a></li>
<li class="toctree-l4"><a class="reference internal" href="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Request.html">Class InferContext::Request</a></li>
<li class="toctree-l4"><a class="reference internal" href="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1RequestTimers.html">Class InferContext::RequestTimers</a></li>
<li class="toctree-l4"><a class="reference internal" href="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result.html">Class InferContext::Result</a></li>
<li class="toctree-l4"><a class="reference internal" href="classnvidia_1_1inferenceserver_1_1client_1_1InferGrpcContext.html">Class InferGrpcContext</a></li>
<li class="toctree-l4"><a class="reference internal" href="classnvidia_1_1inferenceserver_1_1client_1_1InferGrpcStreamContext.html">Class InferGrpcStreamContext</a></li>
<li class="toctree-l4"><a class="reference internal" href="classnvidia_1_1inferenceserver_1_1client_1_1InferHttpContext.html">Class InferHttpContext</a></li>
<li class="toctree-l4"><a class="reference internal" href="classnvidia_1_1inferenceserver_1_1client_1_1ProfileContext.html">Class ProfileContext</a></li>
<li class="toctree-l4"><a class="reference internal" href="classnvidia_1_1inferenceserver_1_1client_1_1ProfileGrpcContext.html">Class ProfileGrpcContext</a></li>
<li class="toctree-l4"><a class="reference internal" href="classnvidia_1_1inferenceserver_1_1client_1_1ProfileHttpContext.html">Class ProfileHttpContext</a></li>
<li class="toctree-l4"><a class="reference internal" href="classnvidia_1_1inferenceserver_1_1client_1_1ServerHealthContext.html">Class ServerHealthContext</a></li>
<li class="toctree-l4"><a class="reference internal" href="classnvidia_1_1inferenceserver_1_1client_1_1ServerHealthGrpcContext.html">Class ServerHealthGrpcContext</a></li>
<li class="toctree-l4"><a class="reference internal" href="classnvidia_1_1inferenceserver_1_1client_1_1ServerHealthHttpContext.html">Class ServerHealthHttpContext</a></li>
<li class="toctree-l4"><a class="reference internal" href="classnvidia_1_1inferenceserver_1_1client_1_1ServerStatusContext.html">Class ServerStatusContext</a></li>
<li class="toctree-l4"><a class="reference internal" href="classnvidia_1_1inferenceserver_1_1client_1_1ServerStatusGrpcContext.html">Class ServerStatusGrpcContext</a></li>
<li class="toctree-l4"><a class="reference internal" href="classnvidia_1_1inferenceserver_1_1client_1_1ServerStatusHttpContext.html">Class ServerStatusHttpContext</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="cpp_api_root.html#functions">Functions</a><ul>
<li class="toctree-l4"><a class="reference internal" href="function_custom_8h_1aedfdcb69f8eaab594173b197b10dc804.html">Function CustomErrorString</a></li>
<li class="toctree-l4"><a class="reference internal" href="function_custom_8h_1afc75c0a4e1a169562586c2d4ff09ba40.html">Function CustomExecute</a></li>
<li class="toctree-l4"><a class="reference internal" href="function_custom_8h_1a9cd799dfef77db82babc6ddd4a4be707.html">Function CustomFinalize</a></li>
<li class="toctree-l4"><a class="reference internal" href="function_custom_8h_1ad384f6af9bfb11b82b2b4e411f799b6f.html">Function CustomInitialize</a></li>
<li class="toctree-l4"><a class="reference internal" href="function_request_8h_1a19d352bb13848c14b5f03274b25ecee3.html">Function nvidia::inferenceserver::client::operator&lt;&lt;</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="cpp_api_root.html#defines">Defines</a><ul>
<li class="toctree-l4"><a class="reference internal" href="define_custom_8h_1aaf537da73bb55fda05fb4e39defe1949.html">Define CUSTOM_NO_GPU_DEVICE</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="cpp_api_root.html#typedefs">Typedefs</a><ul>
<li class="toctree-l4"><a class="reference internal" href="typedef_custom_8h_1aa6616d50105d3441ba277c6dbce66286.html">Typedef CustomErrorStringFn_t</a></li>
<li class="toctree-l4"><a class="reference internal" href="typedef_custom_8h_1ac0c3508cd00e9db56912ff40deecf9d2.html">Typedef CustomExecuteFn_t</a></li>
<li class="toctree-l4"><a class="reference internal" href="typedef_custom_8h_1aa32f51734df1f217cc7c2b07f31f8f3f.html">Typedef CustomFinalizeFn_t</a></li>
<li class="toctree-l4"><a class="reference internal" href="typedef_custom_8h_1aee3723f1a17ff4664691655e056a309c.html">Typedef CustomGetNextInputFn_t</a></li>
<li class="toctree-l4"><a class="reference internal" href="typedef_custom_8h_1a71cb419d6e7feee902115438973d02f8.html">Typedef CustomGetOutputFn_t</a></li>
<li class="toctree-l4"><a class="reference internal" href="typedef_custom_8h_1a11fea108abb0b28f86a96e6b7cd92a20.html">Typedef CustomInitializeFn_t</a></li>
<li class="toctree-l4"><a class="reference internal" href="typedef_custom_8h_1a99a7b9dcc756702784a869d9afb59df4.html">Typedef CustomPayload</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="cpp_api_root.html#directories">Directories</a><ul>
<li class="toctree-l4"><a class="reference internal" href="dir_src.html">Directory src</a></li>
<li class="toctree-l4"><a class="reference internal" href="dir_src_clients.html">Directory clients</a></li>
<li class="toctree-l4"><a class="reference internal" href="dir_src_clients_c++.html">Directory c++</a></li>
<li class="toctree-l4"><a class="reference internal" href="dir_src_servables.html">Directory servables</a></li>
<li class="toctree-l4"><a class="reference internal" href="dir_src_servables_custom.html">Directory custom</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="cpp_api_root.html#files">Files</a><ul>
<li class="toctree-l4"><a class="reference internal" href="file_src_servables_custom_custom.h.html">File custom.h</a></li>
<li class="toctree-l4"><a class="reference internal" href="file_src_clients_c++_request.h.html">File request.h</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../python_api.html">Python API</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../python_api.html#client">Client</a></li>
</ul>
</li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">NVIDIA TensorRT Inference Server</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html">Docs</a> &raquo;</li>
        
          <li><a href="cpp_api_root.html">C++ API</a> &raquo;</li>
        
      <li>Class InferContext::Options</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../_sources/cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Options.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  
<style>
/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast,
.nboutput.nblast {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast + .nbinput {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<div class="section" id="class-infercontext-options">
<span id="exhale-class-classnvidia-1-1inferenceserver-1-1client-1-1infercontext-1-1options"></span><h1>Class InferContext::Options<a class="headerlink" href="#class-infercontext-options" title="Permalink to this headline">¶</a></h1>
<ul class="simple">
<li>Defined in <a class="reference internal" href="file_src_clients_c++_request.h.html#file-src-clients-c-request-h"><span class="std std-ref">File request.h</span></a></li>
</ul>
<div class="section" id="nested-relationships">
<h2>Nested Relationships<a class="headerlink" href="#nested-relationships" title="Permalink to this headline">¶</a></h2>
<p>This class is a nested type of <a class="reference internal" href="classnvidia_1_1inferenceserver_1_1client_1_1InferContext.html#exhale-class-classnvidia-1-1inferenceserver-1-1client-1-1infercontext"><span class="std std-ref">Class InferContext</span></a>.</p>
</div>
<div class="section" id="class-documentation">
<h2>Class Documentation<a class="headerlink" href="#class-documentation" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="_CPPv4N6nvidia15inferenceserver6client12InferContext7OptionsE">
<span class="target" id="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Options"></span><em class="property">class </em><code class="descname">Options</code><br /></dt>
<dd><p>Run options to be applied to all subsequent <a class="reference internal" href="classnvidia_1_1inferenceserver_1_1client_1_1InferContext.html#classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1a4cac8002817c9259851305b14c258f36"><span class="std std-ref">Run()</span></a> invocations. </p>
<div class="breathe-sectiondef docutils container">
<p class="breathe-sectiondef-title rubric">Public Functions</p>
<dl class="function">
<dt id="_CPPv4N6nvidia15inferenceserver6client12InferContext7OptionsD0Ev">
<span class="target" id="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Options_1abcaa16202fc4751a7657808cc4d71884"></span><em class="property">virtual</em> <code class="descname">~Options</code><span class="sig-paren">(</span><span class="sig-paren">)</span><br /></dt>
<dd></dd></dl>

<dl class="function">
<dt id="_CPPv4NK6nvidia15inferenceserver6client12InferContext7Options4FlagEN18InferRequestHeader4FlagE">
<span class="target" id="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Options_1a23b1ac6a7393c1bad21fc8e7a3373ead"></span><em class="property">virtual</em> bool <code class="descname">Flag</code><span class="sig-paren">(</span><a class="reference internal" href="../protobuf_api/api.proto.html#_CPPv4N6nvidia15inferenceserver18InferRequestHeaderE" title="nvidia::inferenceserver::InferRequestHeader">InferRequestHeader</a>::<a class="reference internal" href="../protobuf_api/api.proto.html#_CPPv4N6nvidia15inferenceserver18InferRequestHeader4FlagE" title="nvidia::inferenceserver::InferRequestHeader::Flag">Flag</a> <em>flag</em><span class="sig-paren">)</span> <em class="property">const</em> = 0<br /></dt>
<dd><p>Get the value of a request flag being used for all subsequent inferences. </p>
<p>Cannot be used with FLAG_NONE. <dl class="docutils">
<dt><strong>Return</strong></dt>
<dd>The true/false value currently set for the flag. If ‘flag’ is FLAG_NONE then return false. </dd>
<dt><strong>Parameters</strong></dt>
<dd><ul class="breatheparameterlist first last simple">
<li><code class="docutils literal notranslate"><span class="pre">flag</span></code>: The flag to get the value for. </li>
</ul>
</dd>
</dl>
</p>
</dd></dl>

<dl class="function">
<dt id="_CPPv4N6nvidia15inferenceserver6client12InferContext7Options7SetFlagEN18InferRequestHeader4FlagEb">
<span class="target" id="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Options_1ae12fc02abb1e1f42d1de0e0d4f88c03d"></span><em class="property">virtual</em> void <code class="descname">SetFlag</code><span class="sig-paren">(</span><a class="reference internal" href="../protobuf_api/api.proto.html#_CPPv4N6nvidia15inferenceserver18InferRequestHeaderE" title="nvidia::inferenceserver::InferRequestHeader">InferRequestHeader</a>::<a class="reference internal" href="../protobuf_api/api.proto.html#_CPPv4N6nvidia15inferenceserver18InferRequestHeader4FlagE" title="nvidia::inferenceserver::InferRequestHeader::Flag">Flag</a> <em>flag</em>, bool <em>value</em><span class="sig-paren">)</span> = 0<br /></dt>
<dd><p>Set a request flag to be used for all subsequent inferences. </p>
<p><dl class="docutils">
<dt><strong>Parameters</strong></dt>
<dd><ul class="breatheparameterlist first last simple">
<li><code class="docutils literal notranslate"><span class="pre">flag</span></code>: The flag to set. Cannot be used with FLAG_NONE. </li>
<li><code class="docutils literal notranslate"><span class="pre">value</span></code>: The true/false value to set for the flag. If ‘flag’ is FLAG_NONE then do nothing. </li>
</ul>
</dd>
</dl>
</p>
</dd></dl>

<dl class="function">
<dt id="_CPPv4NK6nvidia15inferenceserver6client12InferContext7Options5FlagsEv">
<span class="target" id="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Options_1a51f46889d0f436185379def5a99b67fc"></span><em class="property">virtual</em> uint32_t <code class="descname">Flags</code><span class="sig-paren">(</span><span class="sig-paren">)</span> <em class="property">const</em> = 0<br /></dt>
<dd><p>Get the value of all request flags being used for all subsequent inferences. </p>
<p><dl class="docutils">
<dt><strong>Return</strong></dt>
<dd>The bitwise-or of flag values as a single uint32_t value. </dd>
</dl>
</p>
</dd></dl>

<dl class="function">
<dt id="_CPPv4N6nvidia15inferenceserver6client12InferContext7Options8SetFlagsE8uint32_t">
<span class="target" id="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Options_1a06defa0f160399a6e81d31bde89cbcc0"></span><em class="property">virtual</em> void <code class="descname">SetFlags</code><span class="sig-paren">(</span>uint32_t <em>flags</em><span class="sig-paren">)</span> = 0<br /></dt>
<dd><p>Set all request flags to be used for all subsequent inferences. </p>
<p><dl class="docutils">
<dt><strong>Parameters</strong></dt>
<dd><ul class="breatheparameterlist first last simple">
<li><code class="docutils literal notranslate"><span class="pre">flags</span></code>: The bitwise-or of flag values to set. </li>
</ul>
</dd>
</dl>
</p>
</dd></dl>

<dl class="function">
<dt id="_CPPv4NK6nvidia15inferenceserver6client12InferContext7Options9BatchSizeEv">
<span class="target" id="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Options_1a6546eb56589394cc4222272afcc23d89"></span><em class="property">virtual</em> size_t <code class="descname">BatchSize</code><span class="sig-paren">(</span><span class="sig-paren">)</span> <em class="property">const</em> = 0<br /></dt>
<dd><p><dl class="docutils">
<dt><strong>Return</strong></dt>
<dd>The batch size to use for all subsequent inferences. </dd>
</dl>
</p>
</dd></dl>

<dl class="function">
<dt id="_CPPv4N6nvidia15inferenceserver6client12InferContext7Options12SetBatchSizeE6size_t">
<span class="target" id="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Options_1a6f390b31a0a8a265c8e15a77f6f9e7c1"></span><em class="property">virtual</em> void <code class="descname">SetBatchSize</code><span class="sig-paren">(</span>size_t <em>batch_size</em><span class="sig-paren">)</span> = 0<br /></dt>
<dd><p>Set the batch size to use for all subsequent inferences. </p>
<p><dl class="docutils">
<dt><strong>Parameters</strong></dt>
<dd><ul class="breatheparameterlist first last simple">
<li><code class="docutils literal notranslate"><span class="pre">batch_size</span></code>: The batch size. </li>
</ul>
</dd>
</dl>
</p>
</dd></dl>

<dl class="function">
<dt id="_CPPv4N6nvidia15inferenceserver6client12InferContext7Options12AddRawResultERKNSt10shared_ptrIN12InferContext6OutputEEE">
<span class="target" id="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Options_1abb0dceb3499c833684e0dfbe608b3360"></span><em class="property">virtual</em> <a class="reference internal" href="classnvidia_1_1inferenceserver_1_1client_1_1Error.html#_CPPv4N6nvidia15inferenceserver6client5ErrorE" title="nvidia::inferenceserver::client::Error">Error</a> <code class="descname">AddRawResult</code><span class="sig-paren">(</span><em class="property">const</em> std::shared_ptr&lt;<a class="reference internal" href="classnvidia_1_1inferenceserver_1_1client_1_1InferContext.html#_CPPv4N6nvidia15inferenceserver6client12InferContextE" title="nvidia::inferenceserver::client::InferContext">InferContext</a>::<a class="reference internal" href="classnvidia_1_1inferenceserver_1_1client_1_1InferContext.html#_CPPv4N6nvidia15inferenceserver6client12InferContext6OutputE" title="nvidia::inferenceserver::client::InferContext::Output">Output</a>&gt; &amp;<em>output</em><span class="sig-paren">)</span> = 0<br /></dt>
<dd><p>Add ‘output’ to the list of requested RAW results. </p>
<p><a class="reference internal" href="classnvidia_1_1inferenceserver_1_1client_1_1InferContext.html#classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1a4cac8002817c9259851305b14c258f36"><span class="std std-ref">Run()</span></a> will return the output’s full tensor as a result. <dl class="docutils">
<dt><strong>Return</strong></dt>
<dd><a class="reference internal" href="classnvidia_1_1inferenceserver_1_1client_1_1Error.html#classnvidia_1_1inferenceserver_1_1client_1_1Error"><span class="std std-ref">Error</span></a> object indicating success or failure. </dd>
<dt><strong>Parameters</strong></dt>
<dd><ul class="breatheparameterlist first last simple">
<li><code class="docutils literal notranslate"><span class="pre">output</span></code>: The output. </li>
</ul>
</dd>
</dl>
</p>
</dd></dl>

<dl class="function">
<dt id="_CPPv4N6nvidia15inferenceserver6client12InferContext7Options14AddClassResultERKNSt10shared_ptrIN12InferContext6OutputEEE8uint64_t">
<span class="target" id="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Options_1a5b584e6fa3d2867d1ec4ce7a0bfee809"></span><em class="property">virtual</em> <a class="reference internal" href="classnvidia_1_1inferenceserver_1_1client_1_1Error.html#_CPPv4N6nvidia15inferenceserver6client5ErrorE" title="nvidia::inferenceserver::client::Error">Error</a> <code class="descname">AddClassResult</code><span class="sig-paren">(</span><em class="property">const</em> std::shared_ptr&lt;<a class="reference internal" href="classnvidia_1_1inferenceserver_1_1client_1_1InferContext.html#_CPPv4N6nvidia15inferenceserver6client12InferContextE" title="nvidia::inferenceserver::client::InferContext">InferContext</a>::<a class="reference internal" href="classnvidia_1_1inferenceserver_1_1client_1_1InferContext.html#_CPPv4N6nvidia15inferenceserver6client12InferContext6OutputE" title="nvidia::inferenceserver::client::InferContext::Output">Output</a>&gt; &amp;<em>output</em>, uint64_t <em>k</em><span class="sig-paren">)</span> = 0<br /></dt>
<dd><p>Add ‘output’ to the list of requested CLASS results. </p>
<p><a class="reference internal" href="classnvidia_1_1inferenceserver_1_1client_1_1InferContext.html#classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1a4cac8002817c9259851305b14c258f36"><span class="std std-ref">Run()</span></a> will return the highest ‘k’ values of ‘output’ as a result. <dl class="docutils">
<dt><strong>Return</strong></dt>
<dd><a class="reference internal" href="classnvidia_1_1inferenceserver_1_1client_1_1Error.html#classnvidia_1_1inferenceserver_1_1client_1_1Error"><span class="std std-ref">Error</span></a> object indicating success or failure. </dd>
<dt><strong>Parameters</strong></dt>
<dd><ul class="breatheparameterlist first last simple">
<li><code class="docutils literal notranslate"><span class="pre">output</span></code>: The output. </li>
<li><code class="docutils literal notranslate"><span class="pre">k</span></code>: Set how many class results to return for the output. </li>
</ul>
</dd>
</dl>
</p>
</dd></dl>

</div>
<div class="breathe-sectiondef docutils container">
<p class="breathe-sectiondef-title rubric">Public Static Functions</p>
<dl class="function">
<dt id="_CPPv4N6nvidia15inferenceserver6client12InferContext7Options6CreateEPNSt10unique_ptrI7OptionsEE">
<span class="target" id="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Options_1a63d97ebc1993c0034ea6822c1d7600b1"></span><em class="property">static</em> <a class="reference internal" href="classnvidia_1_1inferenceserver_1_1client_1_1Error.html#_CPPv4N6nvidia15inferenceserver6client5ErrorE" title="nvidia::inferenceserver::client::Error">Error</a> <code class="descname">Create</code><span class="sig-paren">(</span>std::unique_ptr&lt;<a class="reference internal" href="classnvidia_1_1inferenceserver_1_1client_1_1InferContext.html#_CPPv4N6nvidia15inferenceserver6client12InferContext7OptionsE" title="nvidia::inferenceserver::client::InferContext::Options">Options</a>&gt; *<em>options</em><span class="sig-paren">)</span><br /></dt>
<dd><p>Create a new <a class="reference internal" href="#classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Options"><span class="std std-ref">Options</span></a> object with default values. </p>
<p><dl class="docutils">
<dt><strong>Return</strong></dt>
<dd><a class="reference internal" href="classnvidia_1_1inferenceserver_1_1client_1_1Error.html#classnvidia_1_1inferenceserver_1_1client_1_1Error"><span class="std std-ref">Error</span></a> object indicating success or failure. </dd>
</dl>
</p>
</dd></dl>

</div>
</dd></dl>

</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Output.html" class="btn btn-neutral float-right" title="Class InferContext::Output" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Input.html" class="btn btn-neutral float-left" title="Class InferContext::Input" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2018, NVIDIA Corporation

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
    

  <style>
  a:link, a:visited {
    color: #76b900;
  }

  a:hover {
    color: #8c0;
  }

  .rst-content dl:not(.docutils) dt {
    background: rgba(118, 185, 0, 0.1);
    color: rgba(59,93,0,1);
    border-top: solid 3px rgba(59,93,0,1);
  }
  </style>
  

</body>
</html>