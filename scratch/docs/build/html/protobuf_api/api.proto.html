<!--
# Copyright (c) 2018, NVIDIA CORPORATION. All rights reserved.
#
# Redistribution and use in source and binary forms, with or without
# modification, are permitted provided that the following conditions
# are met:
#  * Redistributions of source code must retain the above copyright
#    notice, this list of conditions and the following disclaimer.
#  * Redistributions in binary form must reproduce the above copyright
#    notice, this list of conditions and the following disclaimer in the
#    documentation and/or other materials provided with the distribution.
#  * Neither the name of NVIDIA CORPORATION nor the names of its
#    contributors may be used to endorse or promote products derived
#    from this software without specific prior written permission.
#
# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
# EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
# PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
# PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
# OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
# (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
# OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
-->


<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>api.proto &mdash; NVIDIA TensorRT Inference Server 0.12.0dev
 documentation</title>
  

  
  
  
  
    <link rel="canonical" href="https://docs.nvidia.com/deeplearning/sdk/tensorrt-inference-server-guide/docs/index.htmlprotobuf_api/api.proto.html"/>
  

  
  <script type="text/javascript" src="../_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script type="text/javascript" src="../_static/jquery.js"></script>
        <script type="text/javascript" src="../_static/underscore.js"></script>
        <script type="text/javascript" src="../_static/doctools.js"></script>
        <script type="text/javascript" src="../_static/language_data.js"></script>
        <script type="text/javascript" src="../_static/collapsible-lists/js/CollapsibleLists.compressed.js"></script>
        <script type="text/javascript" src="../_static/collapsible-lists/js/apply-collapsible-lists.js"></script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/collapsible-lists/css/tree_view.css" type="text/css" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
           

          
            <a href="../index.html" class="icon icon-home"> NVIDIA TensorRT Inference Server
          

          
          </a>

          
            
            
              <div class="version">
                0.12.0dev
-ad20b82<br/>
Version select: <select onChange="window.location.href = this.value" onFocus="this.selectedIndex = -1">
    <option value="https://docs.nvidia.com/deeplearning/sdk/tensorrt-inference-server-guide/docs/index.html">Current release</option>
    <option value="https://docs.nvidia.com/deeplearning/sdk/tensorrt-inference-server-master-branch-guide/docs/index.html">master (unstable)</option>
    <option value="https://docs.nvidia.com/deeplearning/sdk/inference-server-archived/index.html">Older releases</option>
</select>
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          

  <style>
    /* Sidebar header (and topbar for mobile) */
    .wy-side-nav-search, .wy-nav-top {
      background: #76b900;
    }

    .wy-side-nav-search a:link, .wy-nav-top a:link {
      color: #fff;
    }
    .wy-side-nav-search a:visited, .wy-nav-top a:visited {
      color: #fff;
    }
    .wy-side-nav-search a:hover, .wy-nav-top a:hover {
      color: #fff;
    }

    .wy-menu-vertical a:link, .wy-menu-vertical a:visited {
      color: #d9d9d9
    }

    .wy-menu-vertical a:active {
      background-color: #76b900
    }

    .wy-side-nav-search>div.version {
      color: rgba(0, 0, 0, 0.3)
    }
  </style>
  
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul>
<li class="toctree-l1"><a class="reference internal" href="../index.html">Documentation home</a></li>
</ul>
<p class="caption"><span class="caption-text">User Guide</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../quickstart.html">Quickstart</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../quickstart.html#prerequisites">Prerequisites</a></li>
<li class="toctree-l2"><a class="reference internal" href="../quickstart.html#using-a-prebuilt-docker-container">Using A Prebuilt Docker Container</a></li>
<li class="toctree-l2"><a class="reference internal" href="../quickstart.html#building-from-source-code">Building From Source Code</a></li>
<li class="toctree-l2"><a class="reference internal" href="../quickstart.html#run-tensorrt-inference-server">Run TensorRT Inference Server</a></li>
<li class="toctree-l2"><a class="reference internal" href="../quickstart.html#verify-inference-server-is-running-correctly">Verify Inference Server Is Running Correctly</a></li>
<li class="toctree-l2"><a class="reference internal" href="../quickstart.html#building-the-client-examples">Building The Client Examples</a></li>
<li class="toctree-l2"><a class="reference internal" href="../quickstart.html#running-the-image-classification-example">Running The Image Classification Example</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../install.html">Installing the Server</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../install.html#installing-prebuilt-containers">Installing Prebuilt Containers</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../run.html">Running the Server</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../run.html#example-model-repository">Example Model Repository</a></li>
<li class="toctree-l2"><a class="reference internal" href="../run.html#running-the-inference-server">Running The Inference Server</a></li>
<li class="toctree-l2"><a class="reference internal" href="../run.html#running-the-inference-server-on-a-system-without-a-gpu">Running The Inference Server On A System Without A GPU</a></li>
<li class="toctree-l2"><a class="reference internal" href="../run.html#checking-inference-server-status">Checking Inference Server Status</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../client.html">Client Libraries and Examples</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../client.html#building-the-client-libraries-and-examples">Building the Client Libraries and Examples</a></li>
<li class="toctree-l2"><a class="reference internal" href="../client.html#image-classification-example-application">Image Classification Example Application</a></li>
<li class="toctree-l2"><a class="reference internal" href="../client.html#performance-example-application">Performance Example Application</a></li>
<li class="toctree-l2"><a class="reference internal" href="../client.html#client-api">Client API</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../client.html#string-datatype">String Datatype</a></li>
<li class="toctree-l3"><a class="reference internal" href="../client.html#stream-inference">Stream Inference</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../model_repository.html">Model Repository</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../model_repository.html#modifying-the-model-repository">Modifying the Model Repository</a></li>
<li class="toctree-l2"><a class="reference internal" href="../model_repository.html#model-versions">Model Versions</a></li>
<li class="toctree-l2"><a class="reference internal" href="../model_repository.html#framework-model-definition">Framework Model Definition</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../model_repository.html#tensorrt-models">TensorRT Models</a></li>
<li class="toctree-l3"><a class="reference internal" href="../model_repository.html#tensorflow-models">TensorFlow Models</a></li>
<li class="toctree-l3"><a class="reference internal" href="../model_repository.html#caffe2-models">Caffe2 Models</a></li>
<li class="toctree-l3"><a class="reference internal" href="../model_repository.html#tensorrt-tensorflow-models">TensorRT/TensorFlow Models</a></li>
<li class="toctree-l3"><a class="reference internal" href="../model_repository.html#onnx-models">ONNX Models</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../model_repository.html#custom-backends">Custom Backends</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../model_repository.html#custom-backend-api">Custom Backend API</a></li>
<li class="toctree-l3"><a class="reference internal" href="../model_repository.html#example-custom-backend">Example Custom Backend</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../model_configuration.html">Model Configuration</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../model_configuration.html#generated-model-configuration">Generated Model Configuration</a></li>
<li class="toctree-l2"><a class="reference internal" href="../model_configuration.html#datatypes">Datatypes</a></li>
<li class="toctree-l2"><a class="reference internal" href="../model_configuration.html#version-policy">Version Policy</a></li>
<li class="toctree-l2"><a class="reference internal" href="../model_configuration.html#instance-groups">Instance Groups</a></li>
<li class="toctree-l2"><a class="reference internal" href="../model_configuration.html#dynamic-batching">Dynamic Batching</a></li>
<li class="toctree-l2"><a class="reference internal" href="../model_configuration.html#optimization-policy">Optimization Policy</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../http_grpc_api.html">Inference Server API</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../http_grpc_api.html#health">Health</a></li>
<li class="toctree-l2"><a class="reference internal" href="../http_grpc_api.html#status">Status</a></li>
<li class="toctree-l2"><a class="reference internal" href="../http_grpc_api.html#inference">Inference</a></li>
<li class="toctree-l2"><a class="reference internal" href="../http_grpc_api.html#stream-inference">Stream Inference</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../metrics.html">Metrics</a></li>
</ul>
<p class="caption"><span class="caption-text">Developer Guide</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../architecture.html">Architecture</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../architecture.html#concurrent-model-execution">Concurrent Model Execution</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../contribute.html">Contributing</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../contribute.html#coding-convention">Coding Convention</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../build.html">Building</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../build.html#building-the-server">Building the Server</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../build.html#incremental-builds">Incremental Builds</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../build.html#building-the-client-libraries-and-examples">Building the Client Libraries and Examples</a></li>
<li class="toctree-l2"><a class="reference internal" href="../build.html#building-the-documentation">Building the Documentation</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../build.html#html-documentation">HTML Documentation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../build.html#pdf-documentation">PDF Documentation</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../test.html">Testing</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../test.html#generate-qa-model-repositories">Generate QA Model Repositories</a></li>
<li class="toctree-l2"><a class="reference internal" href="../test.html#build-qa-container">Build QA Container</a></li>
<li class="toctree-l2"><a class="reference internal" href="../test.html#run-qa-container">Run QA Container</a></li>
</ul>
</li>
</ul>
<p class="caption"><span class="caption-text">API Reference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="protobuf_api_root.html">Protobuf API</a><ul>
<li class="toctree-l2"><a class="reference internal" href="protobuf_api_root.html#http-grpc-api">HTTP/GRPC API</a></li>
<li class="toctree-l2"><a class="reference internal" href="protobuf_api_root.html#model-configuration">Model Configuration</a></li>
<li class="toctree-l2"><a class="reference internal" href="protobuf_api_root.html#status">Status</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../cpp_api/cpp_api_root.html">C++ API</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../cpp_api/cpp_api_root.html#class-hierarchy">Class Hierarchy</a></li>
<li class="toctree-l2"><a class="reference internal" href="../cpp_api/cpp_api_root.html#file-hierarchy">File Hierarchy</a></li>
<li class="toctree-l2"><a class="reference internal" href="../cpp_api/cpp_api_root.html#full-api">Full API</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../cpp_api/cpp_api_root.html#namespaces">Namespaces</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../cpp_api/namespace_nvidia.html">Namespace nvidia</a></li>
<li class="toctree-l4"><a class="reference internal" href="../cpp_api/namespace_nvidia__inferenceserver.html">Namespace nvidia::inferenceserver</a></li>
<li class="toctree-l4"><a class="reference internal" href="../cpp_api/namespace_nvidia__inferenceserver__client.html">Namespace nvidia::inferenceserver::client</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../cpp_api/cpp_api_root.html#classes-and-structs">Classes and Structs</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../cpp_api/structcustom__payload__struct.html">Struct custom_payload_struct</a></li>
<li class="toctree-l4"><a class="reference internal" href="../cpp_api/structnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result_1_1ClassResult.html">Struct Result::ClassResult</a></li>
<li class="toctree-l4"><a class="reference internal" href="../cpp_api/structnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Stat.html">Struct InferContext::Stat</a></li>
<li class="toctree-l4"><a class="reference internal" href="../cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1Error.html">Class Error</a></li>
<li class="toctree-l4"><a class="reference internal" href="../cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext.html">Class InferContext</a></li>
<li class="toctree-l4"><a class="reference internal" href="../cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Input.html">Class InferContext::Input</a></li>
<li class="toctree-l4"><a class="reference internal" href="../cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Options.html">Class InferContext::Options</a></li>
<li class="toctree-l4"><a class="reference internal" href="../cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Output.html">Class InferContext::Output</a></li>
<li class="toctree-l4"><a class="reference internal" href="../cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Request.html">Class InferContext::Request</a></li>
<li class="toctree-l4"><a class="reference internal" href="../cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1RequestTimers.html">Class InferContext::RequestTimers</a></li>
<li class="toctree-l4"><a class="reference internal" href="../cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result.html">Class InferContext::Result</a></li>
<li class="toctree-l4"><a class="reference internal" href="../cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferGrpcContext.html">Class InferGrpcContext</a></li>
<li class="toctree-l4"><a class="reference internal" href="../cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferGrpcStreamContext.html">Class InferGrpcStreamContext</a></li>
<li class="toctree-l4"><a class="reference internal" href="../cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferHttpContext.html">Class InferHttpContext</a></li>
<li class="toctree-l4"><a class="reference internal" href="../cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ProfileContext.html">Class ProfileContext</a></li>
<li class="toctree-l4"><a class="reference internal" href="../cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ProfileGrpcContext.html">Class ProfileGrpcContext</a></li>
<li class="toctree-l4"><a class="reference internal" href="../cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ProfileHttpContext.html">Class ProfileHttpContext</a></li>
<li class="toctree-l4"><a class="reference internal" href="../cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ServerHealthContext.html">Class ServerHealthContext</a></li>
<li class="toctree-l4"><a class="reference internal" href="../cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ServerHealthGrpcContext.html">Class ServerHealthGrpcContext</a></li>
<li class="toctree-l4"><a class="reference internal" href="../cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ServerHealthHttpContext.html">Class ServerHealthHttpContext</a></li>
<li class="toctree-l4"><a class="reference internal" href="../cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ServerStatusContext.html">Class ServerStatusContext</a></li>
<li class="toctree-l4"><a class="reference internal" href="../cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ServerStatusGrpcContext.html">Class ServerStatusGrpcContext</a></li>
<li class="toctree-l4"><a class="reference internal" href="../cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ServerStatusHttpContext.html">Class ServerStatusHttpContext</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../cpp_api/cpp_api_root.html#functions">Functions</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../cpp_api/function_custom_8h_1aedfdcb69f8eaab594173b197b10dc804.html">Function CustomErrorString</a></li>
<li class="toctree-l4"><a class="reference internal" href="../cpp_api/function_custom_8h_1afc75c0a4e1a169562586c2d4ff09ba40.html">Function CustomExecute</a></li>
<li class="toctree-l4"><a class="reference internal" href="../cpp_api/function_custom_8h_1a9cd799dfef77db82babc6ddd4a4be707.html">Function CustomFinalize</a></li>
<li class="toctree-l4"><a class="reference internal" href="../cpp_api/function_custom_8h_1ad384f6af9bfb11b82b2b4e411f799b6f.html">Function CustomInitialize</a></li>
<li class="toctree-l4"><a class="reference internal" href="../cpp_api/function_request_8h_1a19d352bb13848c14b5f03274b25ecee3.html">Function nvidia::inferenceserver::client::operator&lt;&lt;</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../cpp_api/cpp_api_root.html#defines">Defines</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../cpp_api/define_custom_8h_1aaf537da73bb55fda05fb4e39defe1949.html">Define CUSTOM_NO_GPU_DEVICE</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../cpp_api/cpp_api_root.html#typedefs">Typedefs</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../cpp_api/typedef_custom_8h_1aa6616d50105d3441ba277c6dbce66286.html">Typedef CustomErrorStringFn_t</a></li>
<li class="toctree-l4"><a class="reference internal" href="../cpp_api/typedef_custom_8h_1ac0c3508cd00e9db56912ff40deecf9d2.html">Typedef CustomExecuteFn_t</a></li>
<li class="toctree-l4"><a class="reference internal" href="../cpp_api/typedef_custom_8h_1aa32f51734df1f217cc7c2b07f31f8f3f.html">Typedef CustomFinalizeFn_t</a></li>
<li class="toctree-l4"><a class="reference internal" href="../cpp_api/typedef_custom_8h_1aee3723f1a17ff4664691655e056a309c.html">Typedef CustomGetNextInputFn_t</a></li>
<li class="toctree-l4"><a class="reference internal" href="../cpp_api/typedef_custom_8h_1a71cb419d6e7feee902115438973d02f8.html">Typedef CustomGetOutputFn_t</a></li>
<li class="toctree-l4"><a class="reference internal" href="../cpp_api/typedef_custom_8h_1a11fea108abb0b28f86a96e6b7cd92a20.html">Typedef CustomInitializeFn_t</a></li>
<li class="toctree-l4"><a class="reference internal" href="../cpp_api/typedef_custom_8h_1a99a7b9dcc756702784a869d9afb59df4.html">Typedef CustomPayload</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../cpp_api/cpp_api_root.html#directories">Directories</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../cpp_api/dir_src.html">Directory src</a></li>
<li class="toctree-l4"><a class="reference internal" href="../cpp_api/dir_src_clients.html">Directory clients</a></li>
<li class="toctree-l4"><a class="reference internal" href="../cpp_api/dir_src_clients_c++.html">Directory c++</a></li>
<li class="toctree-l4"><a class="reference internal" href="../cpp_api/dir_src_servables.html">Directory servables</a></li>
<li class="toctree-l4"><a class="reference internal" href="../cpp_api/dir_src_servables_custom.html">Directory custom</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../cpp_api/cpp_api_root.html#files">Files</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../cpp_api/file_src_servables_custom_custom.h.html">File custom.h</a></li>
<li class="toctree-l4"><a class="reference internal" href="../cpp_api/file_src_clients_c++_request.h.html">File request.h</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../python_api.html">Python API</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../python_api.html#client">Client</a></li>
</ul>
</li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">NVIDIA TensorRT Inference Server</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html">Docs</a> &raquo;</li>
        
      <li>api.proto</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../_sources/protobuf_api/api.proto.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  
<style>
/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast,
.nboutput.nblast {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast + .nbinput {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<div class="section" id="api-proto">
<h1>api.proto<a class="headerlink" href="#api-proto" title="Permalink to this headline">¶</a></h1>
<dl class="var">
<dt id="_CPPv4N6nvidia15inferenceserver18InferRequestHeaderE">
message <code class="descname">InferRequestHeader</code><a class="headerlink" href="#_CPPv4N6nvidia15inferenceserver18InferRequestHeaderE" title="Permalink to this definition">¶</a><br /></dt>
<dd><blockquote>
<div>Meta-data for an inferencing request. The actual input data is
delivered separate from this header, in the HTTP body for an HTTP
request, or in the <a class="reference internal" href="grpc_service.proto.html#_CPPv4N6nvidia15inferenceserver12InferRequestE" title="nvidia::inferenceserver::InferRequest"><code class="xref cpp cpp-var docutils literal notranslate"><span class="pre">InferRequest</span></code></a> message for a gRPC request.</div></blockquote>
<dl class="enum">
<dt id="_CPPv4N6nvidia15inferenceserver18InferRequestHeader4FlagE">
<em class="property">enum </em><code class="descname">Flag</code><a class="headerlink" href="#_CPPv4N6nvidia15inferenceserver18InferRequestHeader4FlagE" title="Permalink to this definition">¶</a><br /></dt>
<dd><blockquote>
<div>Flags that can be associated with an inference request.
All flags are packed bitwise into the ‘flags’ field and
so the value of each must be a power-of-2.</div></blockquote>
<dl class="enumerator">
<dt id="_CPPv4N6nvidia15inferenceserver18InferRequestHeader4Flag4Flag9FLAG_NONEE">
<em class="property">enumerator </em><code class="descclassname">Flag<code class="descclassname">::</code></code><code class="descname">FLAG_NONE</code> = 0<a class="headerlink" href="#_CPPv4N6nvidia15inferenceserver18InferRequestHeader4Flag4Flag9FLAG_NONEE" title="Permalink to this definition">¶</a><br /></dt>
<dd><p>Value indicating no flags are enabled.</p>
</dd></dl>

<dl class="enumerator">
<dt id="_CPPv4N6nvidia15inferenceserver18InferRequestHeader4Flag4Flag17FLAG_SEQUENCE_ENDE">
<em class="property">enumerator </em><code class="descclassname">Flag<code class="descclassname">::</code></code><code class="descname">FLAG_SEQUENCE_END</code> = 1<a class="headerlink" href="#_CPPv4N6nvidia15inferenceserver18InferRequestHeader4Flag4Flag17FLAG_SEQUENCE_ENDE" title="Permalink to this definition">¶</a><br /></dt>
<dd><p>This request is the end of a related sequence of requests.</p>
</dd></dl>

</dd></dl>

<dl class="var">
<dt id="_CPPv4N6nvidia15inferenceserver18InferRequestHeader5InputE">
message <code class="descname">Input</code><a class="headerlink" href="#_CPPv4N6nvidia15inferenceserver18InferRequestHeader5InputE" title="Permalink to this definition">¶</a><br /></dt>
<dd><blockquote>
<div>Meta-data for an input tensor provided as part of an inferencing
request.</div></blockquote>
<dl class="var">
<dt id="_CPPv4N6nvidia15inferenceserver18InferRequestHeader5Input4nameE">
string <code class="descname">name</code><a class="headerlink" href="#_CPPv4N6nvidia15inferenceserver18InferRequestHeader5Input4nameE" title="Permalink to this definition">¶</a><br /></dt>
<dd><p>The name of the input tensor.</p>
</dd></dl>

<dl class="var">
<dt id="_CPPv4N6nvidia15inferenceserver18InferRequestHeader5Input4dimsE">
int64 <code class="descname">dims</code><span class="sig-paren">(</span>repeated<span class="sig-paren">)</span><a class="headerlink" href="#_CPPv4N6nvidia15inferenceserver18InferRequestHeader5Input4dimsE" title="Permalink to this definition">¶</a><br /></dt>
<dd><p>The shape of the input tensor, not including the batch dimension.
Optional if the model configuration for this input explicitly
specifies all dimensions of the shape. Required if the model
configuration for this input has any wildcard dimensions (-1).</p>
</dd></dl>

<dl class="var">
<dt id="_CPPv4N6nvidia15inferenceserver18InferRequestHeader5Input15batch_byte_sizeE">
uint64 <code class="descname">batch_byte_size</code><a class="headerlink" href="#_CPPv4N6nvidia15inferenceserver18InferRequestHeader5Input15batch_byte_sizeE" title="Permalink to this definition">¶</a><br /></dt>
<dd><p>The size of the full batch of the input tensor, in bytes.
Optional for tensors with fixed-sized datatypes. Required
for tensors with a non-fixed-size datatype (like STRING).</p>
</dd></dl>

</dd></dl>

<dl class="var">
<dt id="_CPPv4N6nvidia15inferenceserver18InferRequestHeader6OutputE">
message <code class="descname">Output</code><a class="headerlink" href="#_CPPv4N6nvidia15inferenceserver18InferRequestHeader6OutputE" title="Permalink to this definition">¶</a><br /></dt>
<dd><blockquote>
<div>Meta-data for a requested output tensor as part of an inferencing
request.</div></blockquote>
<dl class="var">
<dt id="_CPPv4N6nvidia15inferenceserver18InferRequestHeader6Output4nameE">
string <code class="descname">name</code><a class="headerlink" href="#_CPPv4N6nvidia15inferenceserver18InferRequestHeader6Output4nameE" title="Permalink to this definition">¶</a><br /></dt>
<dd><p>The name of the output tensor.</p>
</dd></dl>

<dl class="var">
<dt id="_CPPv4N6nvidia15inferenceserver18InferRequestHeader6Output5ClassE">
message <code class="descname">Class</code><a class="headerlink" href="#_CPPv4N6nvidia15inferenceserver18InferRequestHeader6Output5ClassE" title="Permalink to this definition">¶</a><br /></dt>
<dd><blockquote>
<div>Options for an output returned as a classification.</div></blockquote>
<dl class="var">
<dt id="_CPPv4N6nvidia15inferenceserver18InferRequestHeader6Output5Class5countE">
uint32 <code class="descname">count</code><a class="headerlink" href="#_CPPv4N6nvidia15inferenceserver18InferRequestHeader6Output5Class5countE" title="Permalink to this definition">¶</a><br /></dt>
<dd><p>Indicates how many classification values should be returned
for the output. The ‘count’ highest priority values are
returned.</p>
</dd></dl>

</dd></dl>

<dl class="var">
<dt id="_CPPv4N6nvidia15inferenceserver18InferRequestHeader6Output3clsE">
<a class="reference internal" href="#_CPPv4N6nvidia15inferenceserver18InferRequestHeader6Output5ClassE" title="nvidia::inferenceserver::InferRequestHeader::Output::Class">Class</a> <code class="descname">cls</code><a class="headerlink" href="#_CPPv4N6nvidia15inferenceserver18InferRequestHeader6Output3clsE" title="Permalink to this definition">¶</a><br /></dt>
<dd><p>Optional. If defined return this output as a classification
instead of raw data. The output tensor will be interpreted as
probabilities and the classifications associated with the
highest probabilities will be returned.</p>
</dd></dl>

</dd></dl>

<dl class="var">
<dt id="_CPPv4N6nvidia15inferenceserver18InferRequestHeader2idE">
uint64 <code class="descname">id</code><a class="headerlink" href="#_CPPv4N6nvidia15inferenceserver18InferRequestHeader2idE" title="Permalink to this definition">¶</a><br /></dt>
<dd><p>The ID of the inference request. The response of the request will
have the same ID in InferResponseHeader. The request sender can use
the ID to correlate the response to corresponding request if needed.</p>
</dd></dl>

<dl class="var">
<dt id="_CPPv4N6nvidia15inferenceserver18InferRequestHeader5flagsE">
uint32 <code class="descname">flags</code><a class="headerlink" href="#_CPPv4N6nvidia15inferenceserver18InferRequestHeader5flagsE" title="Permalink to this definition">¶</a><br /></dt>
<dd><p>The flags associated with this request. This field holds a bitwise-or
of all flag values.</p>
</dd></dl>

<dl class="var">
<dt id="_CPPv4N6nvidia15inferenceserver18InferRequestHeader14correlation_idE">
uint64 <code class="descname">correlation_id</code><a class="headerlink" href="#_CPPv4N6nvidia15inferenceserver18InferRequestHeader14correlation_idE" title="Permalink to this definition">¶</a><br /></dt>
<dd><p>The correlation ID of the inference request. Default is 0, which
indictes that the request has no correlation ID. The correlation ID
is used to indicate two or more inference request are related to
each other. How this relationship is handled by the inference
server is determined by the model’s scheduling policy.</p>
</dd></dl>

<dl class="var">
<dt id="_CPPv4N6nvidia15inferenceserver18InferRequestHeader10batch_sizeE">
uint32 <code class="descname">batch_size</code><a class="headerlink" href="#_CPPv4N6nvidia15inferenceserver18InferRequestHeader10batch_sizeE" title="Permalink to this definition">¶</a><br /></dt>
<dd><p>The batch size of the inference request. This must be &gt;= 1. For
models that don’t support batching, batch_size must be 1.</p>
</dd></dl>

<dl class="var">
<dt id="_CPPv4N6nvidia15inferenceserver18InferRequestHeader5inputE">
<a class="reference internal" href="#_CPPv4N6nvidia15inferenceserver18InferRequestHeader5InputE" title="nvidia::inferenceserver::InferRequestHeader::Input">Input</a> <code class="descname">input</code><span class="sig-paren">(</span>repeated<span class="sig-paren">)</span><a class="headerlink" href="#_CPPv4N6nvidia15inferenceserver18InferRequestHeader5inputE" title="Permalink to this definition">¶</a><br /></dt>
<dd><p>The input meta-data for the inputs provided with the the inference
request.</p>
</dd></dl>

<dl class="var">
<dt id="_CPPv4N6nvidia15inferenceserver18InferRequestHeader6outputE">
<a class="reference internal" href="#_CPPv4N6nvidia15inferenceserver18InferRequestHeader6OutputE" title="nvidia::inferenceserver::InferRequestHeader::Output">Output</a> <code class="descname">output</code><span class="sig-paren">(</span>repeated<span class="sig-paren">)</span><a class="headerlink" href="#_CPPv4N6nvidia15inferenceserver18InferRequestHeader6outputE" title="Permalink to this definition">¶</a><br /></dt>
<dd><p>The output meta-data for the inputs provided with the the inference
request.</p>
</dd></dl>

</dd></dl>

<dl class="var">
<dt id="_CPPv4N6nvidia15inferenceserver19InferResponseHeaderE">
message <code class="descname">InferResponseHeader</code><a class="headerlink" href="#_CPPv4N6nvidia15inferenceserver19InferResponseHeaderE" title="Permalink to this definition">¶</a><br /></dt>
<dd><blockquote>
<div>Meta-data for the response to an inferencing request. The actual output
data is delivered separate from this header, in the HTTP body for an HTTP
request, or in the <a class="reference internal" href="grpc_service.proto.html#_CPPv4N6nvidia15inferenceserver13InferResponseE" title="nvidia::inferenceserver::InferResponse"><code class="xref cpp cpp-var docutils literal notranslate"><span class="pre">InferResponse</span></code></a> message for a gRPC request.</div></blockquote>
<dl class="var">
<dt id="_CPPv4N6nvidia15inferenceserver19InferResponseHeader6OutputE">
message <code class="descname">Output</code><a class="headerlink" href="#_CPPv4N6nvidia15inferenceserver19InferResponseHeader6OutputE" title="Permalink to this definition">¶</a><br /></dt>
<dd><blockquote>
<div>Meta-data for an output tensor requested as part of an inferencing
request.</div></blockquote>
<dl class="var">
<dt id="_CPPv4N6nvidia15inferenceserver19InferResponseHeader6Output4nameE">
string <code class="descname">name</code><a class="headerlink" href="#_CPPv4N6nvidia15inferenceserver19InferResponseHeader6Output4nameE" title="Permalink to this definition">¶</a><br /></dt>
<dd><p>The name of the output tensor.</p>
</dd></dl>

<dl class="var">
<dt id="_CPPv4N6nvidia15inferenceserver19InferResponseHeader6Output3RawE">
message <code class="descname">Raw</code><a class="headerlink" href="#_CPPv4N6nvidia15inferenceserver19InferResponseHeader6Output3RawE" title="Permalink to this definition">¶</a><br /></dt>
<dd><blockquote>
<div>Meta-data for an output tensor being returned as raw data.</div></blockquote>
<dl class="var">
<dt id="_CPPv4N6nvidia15inferenceserver19InferResponseHeader6Output3Raw4dimsE">
int64 <code class="descname">dims</code><span class="sig-paren">(</span>repeated<span class="sig-paren">)</span><a class="headerlink" href="#_CPPv4N6nvidia15inferenceserver19InferResponseHeader6Output3Raw4dimsE" title="Permalink to this definition">¶</a><br /></dt>
<dd><p>The shape of the output tensor, not including the batch
dimension.</p>
</dd></dl>

<dl class="var">
<dt id="_CPPv4N6nvidia15inferenceserver19InferResponseHeader6Output3Raw15batch_byte_sizeE">
uint64 <code class="descname">batch_byte_size</code><a class="headerlink" href="#_CPPv4N6nvidia15inferenceserver19InferResponseHeader6Output3Raw15batch_byte_sizeE" title="Permalink to this definition">¶</a><br /></dt>
<dd><p>The full size of the output tensor, in bytes. For a
batch output, this is the size of the entire batch.</p>
</dd></dl>

</dd></dl>

<dl class="var">
<dt id="_CPPv4N6nvidia15inferenceserver19InferResponseHeader6Output5ClassE">
message <code class="descname">Class</code><a class="headerlink" href="#_CPPv4N6nvidia15inferenceserver19InferResponseHeader6Output5ClassE" title="Permalink to this definition">¶</a><br /></dt>
<dd><blockquote>
<div>Information about each classification for this output.</div></blockquote>
<dl class="var">
<dt id="_CPPv4N6nvidia15inferenceserver19InferResponseHeader6Output5Class3idxE">
int32 <code class="descname">idx</code><a class="headerlink" href="#_CPPv4N6nvidia15inferenceserver19InferResponseHeader6Output5Class3idxE" title="Permalink to this definition">¶</a><br /></dt>
<dd><p>The classification index.</p>
</dd></dl>

<dl class="var">
<dt id="_CPPv4N6nvidia15inferenceserver19InferResponseHeader6Output5Class5valueE">
float <code class="descname">value</code><a class="headerlink" href="#_CPPv4N6nvidia15inferenceserver19InferResponseHeader6Output5Class5valueE" title="Permalink to this definition">¶</a><br /></dt>
<dd><p>The classification value as a float (typically a
probability).</p>
</dd></dl>

<dl class="var">
<dt id="_CPPv4N6nvidia15inferenceserver19InferResponseHeader6Output5Class5labelE">
string <code class="descname">label</code><a class="headerlink" href="#_CPPv4N6nvidia15inferenceserver19InferResponseHeader6Output5Class5labelE" title="Permalink to this definition">¶</a><br /></dt>
<dd><p>The label for the class (optional, only available if provided
by the model).</p>
</dd></dl>

</dd></dl>

<dl class="var">
<dt id="_CPPv4N6nvidia15inferenceserver19InferResponseHeader6Output7ClassesE">
message <code class="descname">Classes</code><a class="headerlink" href="#_CPPv4N6nvidia15inferenceserver19InferResponseHeader6Output7ClassesE" title="Permalink to this definition">¶</a><br /></dt>
<dd><blockquote>
<div>Meta-data for an output tensor being returned as classifications.</div></blockquote>
<dl class="var">
<dt id="_CPPv4N6nvidia15inferenceserver19InferResponseHeader6Output7Classes3clsE">
<a class="reference internal" href="#_CPPv4N6nvidia15inferenceserver19InferResponseHeader6Output5ClassE" title="nvidia::inferenceserver::InferResponseHeader::Output::Class">Class</a> <code class="descname">cls</code><span class="sig-paren">(</span>repeated<span class="sig-paren">)</span><a class="headerlink" href="#_CPPv4N6nvidia15inferenceserver19InferResponseHeader6Output7Classes3clsE" title="Permalink to this definition">¶</a><br /></dt>
<dd><p>The topk classes for this output.</p>
</dd></dl>

</dd></dl>

<dl class="var">
<dt id="_CPPv4N6nvidia15inferenceserver19InferResponseHeader6Output3rawE">
<a class="reference internal" href="#_CPPv4N6nvidia15inferenceserver19InferResponseHeader6Output3RawE" title="nvidia::inferenceserver::InferResponseHeader::Output::Raw">Raw</a> <code class="descname">raw</code><a class="headerlink" href="#_CPPv4N6nvidia15inferenceserver19InferResponseHeader6Output3rawE" title="Permalink to this definition">¶</a><br /></dt>
<dd><p>If specified deliver results for this output as raw tensor data.
The actual output data is delivered in the HTTP body for an HTTP
request, or in the <a class="reference internal" href="grpc_service.proto.html#_CPPv4N6nvidia15inferenceserver13InferResponseE" title="nvidia::inferenceserver::InferResponse"><code class="xref cpp cpp-var docutils literal notranslate"><span class="pre">InferResponse</span></code></a> message for a gRPC
request. Only one of ‘raw’ and ‘batch_classes’ may be specified.</p>
</dd></dl>

<dl class="var">
<dt id="_CPPv4N6nvidia15inferenceserver19InferResponseHeader6Output13batch_classesE">
<a class="reference internal" href="#_CPPv4N6nvidia15inferenceserver19InferResponseHeader6Output7ClassesE" title="nvidia::inferenceserver::InferResponseHeader::Output::Classes">Classes</a> <code class="descname">batch_classes</code><span class="sig-paren">(</span>repeated<span class="sig-paren">)</span><a class="headerlink" href="#_CPPv4N6nvidia15inferenceserver19InferResponseHeader6Output13batch_classesE" title="Permalink to this definition">¶</a><br /></dt>
<dd><p>If specified deliver results for this output as classifications.
There is one <a class="reference internal" href="#_CPPv4N6nvidia15inferenceserver19InferResponseHeader6Output7ClassesE" title="nvidia::inferenceserver::InferResponseHeader::Output::Classes"><code class="xref cpp cpp-var docutils literal notranslate"><span class="pre">Classes</span></code></a> object for each batch entry in
the output. Only one of ‘raw’ and ‘batch_classes’ may be
specified.</p>
</dd></dl>

</dd></dl>

<dl class="var">
<dt id="_CPPv4N6nvidia15inferenceserver19InferResponseHeader2idE">
uint64 <code class="descname">id</code><a class="headerlink" href="#_CPPv4N6nvidia15inferenceserver19InferResponseHeader2idE" title="Permalink to this definition">¶</a><br /></dt>
<dd><p>The ID of the inference response. The response will have the same ID
as the ID of its originated request. The request sender can use
the ID to correlate the response to corresponding request if needed.</p>
</dd></dl>

<dl class="var">
<dt id="_CPPv4N6nvidia15inferenceserver19InferResponseHeader10model_nameE">
string <code class="descname">model_name</code><a class="headerlink" href="#_CPPv4N6nvidia15inferenceserver19InferResponseHeader10model_nameE" title="Permalink to this definition">¶</a><br /></dt>
<dd><p>The name of the model that produced the outputs.</p>
</dd></dl>

<dl class="var">
<dt id="_CPPv4N6nvidia15inferenceserver19InferResponseHeader13model_versionE">
int64 <code class="descname">model_version</code><a class="headerlink" href="#_CPPv4N6nvidia15inferenceserver19InferResponseHeader13model_versionE" title="Permalink to this definition">¶</a><br /></dt>
<dd><p>The version of the model that produced the outputs.</p>
</dd></dl>

<dl class="var">
<dt id="_CPPv4N6nvidia15inferenceserver19InferResponseHeader10batch_sizeE">
uint32 <code class="descname">batch_size</code><a class="headerlink" href="#_CPPv4N6nvidia15inferenceserver19InferResponseHeader10batch_sizeE" title="Permalink to this definition">¶</a><br /></dt>
<dd><p>The batch size of the outputs. This will always be equal to the
batch size of the inputs. For models that don’t support
batching the batch_size will be 1.</p>
</dd></dl>

<dl class="var">
<dt id="_CPPv4N6nvidia15inferenceserver19InferResponseHeader6outputE">
<a class="reference internal" href="#_CPPv4N6nvidia15inferenceserver19InferResponseHeader6OutputE" title="nvidia::inferenceserver::InferResponseHeader::Output">Output</a> <code class="descname">output</code><span class="sig-paren">(</span>repeated<span class="sig-paren">)</span><a class="headerlink" href="#_CPPv4N6nvidia15inferenceserver19InferResponseHeader6outputE" title="Permalink to this definition">¶</a><br /></dt>
<dd><p>The outputs, in the same order as they were requested in
<a class="reference internal" href="#_CPPv4N6nvidia15inferenceserver18InferRequestHeaderE" title="nvidia::inferenceserver::InferRequestHeader"><code class="xref cpp cpp-var docutils literal notranslate"><span class="pre">InferRequestHeader</span></code></a>.</p>
</dd></dl>

</dd></dl>

</div>


           </div>
           
          </div>
          <footer>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2018, NVIDIA Corporation

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
    

  <style>
  a:link, a:visited {
    color: #76b900;
  }

  a:hover {
    color: #8c0;
  }

  .rst-content dl:not(.docutils) dt {
    background: rgba(118, 185, 0, 0.1);
    color: rgba(59,93,0,1);
    border-top: solid 3px rgba(59,93,0,1);
  }
  </style>
  

</body>
</html>