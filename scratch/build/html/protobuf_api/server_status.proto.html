<!--
# Copyright (c) 2018, NVIDIA CORPORATION. All rights reserved.
#
# Redistribution and use in source and binary forms, with or without
# modification, are permitted provided that the following conditions
# are met:
#  * Redistributions of source code must retain the above copyright
#    notice, this list of conditions and the following disclaimer.
#  * Redistributions in binary form must reproduce the above copyright
#    notice, this list of conditions and the following disclaimer in the
#    documentation and/or other materials provided with the distribution.
#  * Neither the name of NVIDIA CORPORATION nor the names of its
#    contributors may be used to endorse or promote products derived
#    from this software without specific prior written permission.
#
# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
# EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
# PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
# PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
# OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
# (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
# OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
-->


<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>server_status.proto &mdash; NVIDIA TensorRT Inference Server 0.12.0dev
 documentation</title>
  

  
  
  
  
    <link rel="canonical" href="https://docs.nvidia.com/deeplearning/sdk/tensorrt-inference-server-guide/docs/index.htmlprotobuf_api/server_status.proto.html"/>
  

  
  <script type="text/javascript" src="../_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script type="text/javascript" src="../_static/jquery.js"></script>
        <script type="text/javascript" src="../_static/underscore.js"></script>
        <script type="text/javascript" src="../_static/doctools.js"></script>
        <script type="text/javascript" src="../_static/language_data.js"></script>
        <script type="text/javascript" src="../_static/collapsible-lists/js/CollapsibleLists.compressed.js"></script>
        <script type="text/javascript" src="../_static/collapsible-lists/js/apply-collapsible-lists.js"></script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/collapsible-lists/css/tree_view.css" type="text/css" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
           

          
            <a href="../index.html" class="icon icon-home"> NVIDIA TensorRT Inference Server
          

          
          </a>

          
            
            
              <div class="version">
                0.12.0dev
-ad20b82<br/>
Version select: <select onChange="window.location.href = this.value" onFocus="this.selectedIndex = -1">
    <option value="https://docs.nvidia.com/deeplearning/sdk/tensorrt-inference-server-guide/docs/index.html">Current release</option>
    <option value="https://docs.nvidia.com/deeplearning/sdk/tensorrt-inference-server-master-branch-guide/docs/index.html">master (unstable)</option>
    <option value="https://docs.nvidia.com/deeplearning/sdk/inference-server-archived/index.html">Older releases</option>
</select>
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          

  <style>
    /* Sidebar header (and topbar for mobile) */
    .wy-side-nav-search, .wy-nav-top {
      background: #76b900;
    }

    .wy-side-nav-search a:link, .wy-nav-top a:link {
      color: #fff;
    }
    .wy-side-nav-search a:visited, .wy-nav-top a:visited {
      color: #fff;
    }
    .wy-side-nav-search a:hover, .wy-nav-top a:hover {
      color: #fff;
    }

    .wy-menu-vertical a:link, .wy-menu-vertical a:visited {
      color: #d9d9d9
    }

    .wy-menu-vertical a:active {
      background-color: #76b900
    }

    .wy-side-nav-search>div.version {
      color: rgba(0, 0, 0, 0.3)
    }
  </style>
  
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul>
<li class="toctree-l1"><a class="reference internal" href="../index.html">Documentation home</a></li>
</ul>
<p class="caption"><span class="caption-text">User Guide</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../quickstart.html">Quickstart</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../quickstart.html#prerequisites">Prerequisites</a></li>
<li class="toctree-l2"><a class="reference internal" href="../quickstart.html#using-a-prebuilt-docker-container">Using A Prebuilt Docker Container</a></li>
<li class="toctree-l2"><a class="reference internal" href="../quickstart.html#building-from-source-code">Building From Source Code</a></li>
<li class="toctree-l2"><a class="reference internal" href="../quickstart.html#run-tensorrt-inference-server">Run TensorRT Inference Server</a></li>
<li class="toctree-l2"><a class="reference internal" href="../quickstart.html#verify-inference-server-is-running-correctly">Verify Inference Server Is Running Correctly</a></li>
<li class="toctree-l2"><a class="reference internal" href="../quickstart.html#building-the-client-examples">Building The Client Examples</a></li>
<li class="toctree-l2"><a class="reference internal" href="../quickstart.html#running-the-image-classification-example">Running The Image Classification Example</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../install.html">Installing the Server</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../install.html#installing-prebuilt-containers">Installing Prebuilt Containers</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../run.html">Running the Server</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../run.html#example-model-repository">Example Model Repository</a></li>
<li class="toctree-l2"><a class="reference internal" href="../run.html#running-the-inference-server">Running The Inference Server</a></li>
<li class="toctree-l2"><a class="reference internal" href="../run.html#running-the-inference-server-on-a-system-without-a-gpu">Running The Inference Server On A System Without A GPU</a></li>
<li class="toctree-l2"><a class="reference internal" href="../run.html#checking-inference-server-status">Checking Inference Server Status</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../client.html">Client Libraries and Examples</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../client.html#building-the-client-libraries-and-examples">Building the Client Libraries and Examples</a></li>
<li class="toctree-l2"><a class="reference internal" href="../client.html#image-classification-example-application">Image Classification Example Application</a></li>
<li class="toctree-l2"><a class="reference internal" href="../client.html#performance-example-application">Performance Example Application</a></li>
<li class="toctree-l2"><a class="reference internal" href="../client.html#client-api">Client API</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../client.html#string-datatype">String Datatype</a></li>
<li class="toctree-l3"><a class="reference internal" href="../client.html#stream-inference">Stream Inference</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../model_repository.html">Model Repository</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../model_repository.html#modifying-the-model-repository">Modifying the Model Repository</a></li>
<li class="toctree-l2"><a class="reference internal" href="../model_repository.html#model-versions">Model Versions</a></li>
<li class="toctree-l2"><a class="reference internal" href="../model_repository.html#framework-model-definition">Framework Model Definition</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../model_repository.html#tensorrt-models">TensorRT Models</a></li>
<li class="toctree-l3"><a class="reference internal" href="../model_repository.html#tensorflow-models">TensorFlow Models</a></li>
<li class="toctree-l3"><a class="reference internal" href="../model_repository.html#caffe2-models">Caffe2 Models</a></li>
<li class="toctree-l3"><a class="reference internal" href="../model_repository.html#tensorrt-tensorflow-models">TensorRT/TensorFlow Models</a></li>
<li class="toctree-l3"><a class="reference internal" href="../model_repository.html#onnx-models">ONNX Models</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../model_repository.html#custom-backends">Custom Backends</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../model_repository.html#custom-backend-api">Custom Backend API</a></li>
<li class="toctree-l3"><a class="reference internal" href="../model_repository.html#example-custom-backend">Example Custom Backend</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../model_configuration.html">Model Configuration</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../model_configuration.html#generated-model-configuration">Generated Model Configuration</a></li>
<li class="toctree-l2"><a class="reference internal" href="../model_configuration.html#datatypes">Datatypes</a></li>
<li class="toctree-l2"><a class="reference internal" href="../model_configuration.html#version-policy">Version Policy</a></li>
<li class="toctree-l2"><a class="reference internal" href="../model_configuration.html#instance-groups">Instance Groups</a></li>
<li class="toctree-l2"><a class="reference internal" href="../model_configuration.html#dynamic-batching">Dynamic Batching</a></li>
<li class="toctree-l2"><a class="reference internal" href="../model_configuration.html#optimization-policy">Optimization Policy</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../http_grpc_api.html">Inference Server API</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../http_grpc_api.html#health">Health</a></li>
<li class="toctree-l2"><a class="reference internal" href="../http_grpc_api.html#status">Status</a></li>
<li class="toctree-l2"><a class="reference internal" href="../http_grpc_api.html#inference">Inference</a></li>
<li class="toctree-l2"><a class="reference internal" href="../http_grpc_api.html#stream-inference">Stream Inference</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../metrics.html">Metrics</a></li>
</ul>
<p class="caption"><span class="caption-text">Developer Guide</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../architecture.html">Architecture</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../architecture.html#concurrent-model-execution">Concurrent Model Execution</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../contribute.html">Contributing</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../contribute.html#coding-convention">Coding Convention</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../build.html">Building</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../build.html#building-the-server">Building the Server</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../build.html#incremental-builds">Incremental Builds</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../build.html#building-the-client-libraries-and-examples">Building the Client Libraries and Examples</a></li>
<li class="toctree-l2"><a class="reference internal" href="../build.html#building-the-documentation">Building the Documentation</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../test.html">Testing</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../test.html#generate-qa-model-repositories">Generate QA Model Repositories</a></li>
<li class="toctree-l2"><a class="reference internal" href="../test.html#build-qa-container">Build QA Container</a></li>
<li class="toctree-l2"><a class="reference internal" href="../test.html#run-qa-container">Run QA Container</a></li>
</ul>
</li>
</ul>
<p class="caption"><span class="caption-text">API Reference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="protobuf_api_root.html">Protobuf API</a><ul>
<li class="toctree-l2"><a class="reference internal" href="protobuf_api_root.html#http-grpc-api">HTTP/GRPC API</a></li>
<li class="toctree-l2"><a class="reference internal" href="protobuf_api_root.html#model-configuration">Model Configuration</a></li>
<li class="toctree-l2"><a class="reference internal" href="protobuf_api_root.html#status">Status</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../cpp_api/cpp_api_root.html">C++ API</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../cpp_api/cpp_api_root.html#class-hierarchy">Class Hierarchy</a></li>
<li class="toctree-l2"><a class="reference internal" href="../cpp_api/cpp_api_root.html#file-hierarchy">File Hierarchy</a></li>
<li class="toctree-l2"><a class="reference internal" href="../cpp_api/cpp_api_root.html#full-api">Full API</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../cpp_api/cpp_api_root.html#namespaces">Namespaces</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../cpp_api/namespace_nvidia.html">Namespace nvidia</a></li>
<li class="toctree-l4"><a class="reference internal" href="../cpp_api/namespace_nvidia__inferenceserver.html">Namespace nvidia::inferenceserver</a></li>
<li class="toctree-l4"><a class="reference internal" href="../cpp_api/namespace_nvidia__inferenceserver__client.html">Namespace nvidia::inferenceserver::client</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../cpp_api/cpp_api_root.html#classes-and-structs">Classes and Structs</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../cpp_api/structcustom__payload__struct.html">Struct custom_payload_struct</a></li>
<li class="toctree-l4"><a class="reference internal" href="../cpp_api/structnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result_1_1ClassResult.html">Struct Result::ClassResult</a></li>
<li class="toctree-l4"><a class="reference internal" href="../cpp_api/structnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Stat.html">Struct InferContext::Stat</a></li>
<li class="toctree-l4"><a class="reference internal" href="../cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1Error.html">Class Error</a></li>
<li class="toctree-l4"><a class="reference internal" href="../cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext.html">Class InferContext</a></li>
<li class="toctree-l4"><a class="reference internal" href="../cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Input.html">Class InferContext::Input</a></li>
<li class="toctree-l4"><a class="reference internal" href="../cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Options.html">Class InferContext::Options</a></li>
<li class="toctree-l4"><a class="reference internal" href="../cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Output.html">Class InferContext::Output</a></li>
<li class="toctree-l4"><a class="reference internal" href="../cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Request.html">Class InferContext::Request</a></li>
<li class="toctree-l4"><a class="reference internal" href="../cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1RequestTimers.html">Class InferContext::RequestTimers</a></li>
<li class="toctree-l4"><a class="reference internal" href="../cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result.html">Class InferContext::Result</a></li>
<li class="toctree-l4"><a class="reference internal" href="../cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferGrpcContext.html">Class InferGrpcContext</a></li>
<li class="toctree-l4"><a class="reference internal" href="../cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferGrpcStreamContext.html">Class InferGrpcStreamContext</a></li>
<li class="toctree-l4"><a class="reference internal" href="../cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferHttpContext.html">Class InferHttpContext</a></li>
<li class="toctree-l4"><a class="reference internal" href="../cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ProfileContext.html">Class ProfileContext</a></li>
<li class="toctree-l4"><a class="reference internal" href="../cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ProfileGrpcContext.html">Class ProfileGrpcContext</a></li>
<li class="toctree-l4"><a class="reference internal" href="../cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ProfileHttpContext.html">Class ProfileHttpContext</a></li>
<li class="toctree-l4"><a class="reference internal" href="../cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ServerHealthContext.html">Class ServerHealthContext</a></li>
<li class="toctree-l4"><a class="reference internal" href="../cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ServerHealthGrpcContext.html">Class ServerHealthGrpcContext</a></li>
<li class="toctree-l4"><a class="reference internal" href="../cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ServerHealthHttpContext.html">Class ServerHealthHttpContext</a></li>
<li class="toctree-l4"><a class="reference internal" href="../cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ServerStatusContext.html">Class ServerStatusContext</a></li>
<li class="toctree-l4"><a class="reference internal" href="../cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ServerStatusGrpcContext.html">Class ServerStatusGrpcContext</a></li>
<li class="toctree-l4"><a class="reference internal" href="../cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ServerStatusHttpContext.html">Class ServerStatusHttpContext</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../cpp_api/cpp_api_root.html#functions">Functions</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../cpp_api/function_custom_8h_1aedfdcb69f8eaab594173b197b10dc804.html">Function CustomErrorString</a></li>
<li class="toctree-l4"><a class="reference internal" href="../cpp_api/function_custom_8h_1afc75c0a4e1a169562586c2d4ff09ba40.html">Function CustomExecute</a></li>
<li class="toctree-l4"><a class="reference internal" href="../cpp_api/function_custom_8h_1a9cd799dfef77db82babc6ddd4a4be707.html">Function CustomFinalize</a></li>
<li class="toctree-l4"><a class="reference internal" href="../cpp_api/function_custom_8h_1ad384f6af9bfb11b82b2b4e411f799b6f.html">Function CustomInitialize</a></li>
<li class="toctree-l4"><a class="reference internal" href="../cpp_api/function_request_8h_1a19d352bb13848c14b5f03274b25ecee3.html">Function nvidia::inferenceserver::client::operator&lt;&lt;</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../cpp_api/cpp_api_root.html#defines">Defines</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../cpp_api/define_custom_8h_1aaf537da73bb55fda05fb4e39defe1949.html">Define CUSTOM_NO_GPU_DEVICE</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../cpp_api/cpp_api_root.html#typedefs">Typedefs</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../cpp_api/typedef_custom_8h_1aa6616d50105d3441ba277c6dbce66286.html">Typedef CustomErrorStringFn_t</a></li>
<li class="toctree-l4"><a class="reference internal" href="../cpp_api/typedef_custom_8h_1ac0c3508cd00e9db56912ff40deecf9d2.html">Typedef CustomExecuteFn_t</a></li>
<li class="toctree-l4"><a class="reference internal" href="../cpp_api/typedef_custom_8h_1aa32f51734df1f217cc7c2b07f31f8f3f.html">Typedef CustomFinalizeFn_t</a></li>
<li class="toctree-l4"><a class="reference internal" href="../cpp_api/typedef_custom_8h_1aee3723f1a17ff4664691655e056a309c.html">Typedef CustomGetNextInputFn_t</a></li>
<li class="toctree-l4"><a class="reference internal" href="../cpp_api/typedef_custom_8h_1a71cb419d6e7feee902115438973d02f8.html">Typedef CustomGetOutputFn_t</a></li>
<li class="toctree-l4"><a class="reference internal" href="../cpp_api/typedef_custom_8h_1a11fea108abb0b28f86a96e6b7cd92a20.html">Typedef CustomInitializeFn_t</a></li>
<li class="toctree-l4"><a class="reference internal" href="../cpp_api/typedef_custom_8h_1a99a7b9dcc756702784a869d9afb59df4.html">Typedef CustomPayload</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../cpp_api/cpp_api_root.html#directories">Directories</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../cpp_api/dir_src.html">Directory src</a></li>
<li class="toctree-l4"><a class="reference internal" href="../cpp_api/dir_src_clients.html">Directory clients</a></li>
<li class="toctree-l4"><a class="reference internal" href="../cpp_api/dir_src_clients_c++.html">Directory c++</a></li>
<li class="toctree-l4"><a class="reference internal" href="../cpp_api/dir_src_servables.html">Directory servables</a></li>
<li class="toctree-l4"><a class="reference internal" href="../cpp_api/dir_src_servables_custom.html">Directory custom</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../cpp_api/cpp_api_root.html#files">Files</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../cpp_api/file_src_servables_custom_custom.h.html">File custom.h</a></li>
<li class="toctree-l4"><a class="reference internal" href="../cpp_api/file_src_clients_c++_request.h.html">File request.h</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../python_api.html">Python API</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../python_api.html#client">Client</a></li>
</ul>
</li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">NVIDIA TensorRT Inference Server</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html">Docs</a> &raquo;</li>
        
      <li>server_status.proto</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../_sources/protobuf_api/server_status.proto.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  
<style>
/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast,
.nboutput.nblast {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast + .nbinput {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<div class="section" id="server-status-proto">
<h1>server_status.proto<a class="headerlink" href="#server-status-proto" title="Permalink to this headline">¶</a></h1>
<dl class="var">
<dt id="_CPPv4N6nvidia15inferenceserver12StatDurationE">
message <code class="descname">StatDuration</code><a class="headerlink" href="#_CPPv4N6nvidia15inferenceserver12StatDurationE" title="Permalink to this definition">¶</a><br /></dt>
<dd><blockquote>
<div>Statistic collecting a duration metric.</div></blockquote>
<dl class="var">
<dt id="_CPPv4N6nvidia15inferenceserver12StatDuration5countE">
uint64 <code class="descname">count</code><a class="headerlink" href="#_CPPv4N6nvidia15inferenceserver12StatDuration5countE" title="Permalink to this definition">¶</a><br /></dt>
<dd><p>Cumulative number of times this metric occurred.</p>
</dd></dl>

<dl class="var">
<dt id="_CPPv4N6nvidia15inferenceserver12StatDuration13total_time_nsE">
uint64 <code class="descname">total_time_ns</code><a class="headerlink" href="#_CPPv4N6nvidia15inferenceserver12StatDuration13total_time_nsE" title="Permalink to this definition">¶</a><br /></dt>
<dd><p>Total collected duration of this metric in nanoseconds.</p>
</dd></dl>

</dd></dl>

<dl class="var">
<dt id="_CPPv4N6nvidia15inferenceserver18StatusRequestStatsE">
message <code class="descname">StatusRequestStats</code><a class="headerlink" href="#_CPPv4N6nvidia15inferenceserver18StatusRequestStatsE" title="Permalink to this definition">¶</a><br /></dt>
<dd><blockquote>
<div>Statistics collected for Status requests.</div></blockquote>
<dl class="var">
<dt id="_CPPv4N6nvidia15inferenceserver18StatusRequestStats7successE">
<a class="reference internal" href="#_CPPv4N6nvidia15inferenceserver12StatDurationE" title="nvidia::inferenceserver::StatDuration">StatDuration</a> <code class="descname">success</code><a class="headerlink" href="#_CPPv4N6nvidia15inferenceserver18StatusRequestStats7successE" title="Permalink to this definition">¶</a><br /></dt>
<dd><p>Total time required to handle successful Status requests, not
including HTTP or gRPC endpoint termination time.</p>
</dd></dl>

</dd></dl>

<dl class="var">
<dt id="_CPPv4N6nvidia15inferenceserver19ProfileRequestStatsE">
message <code class="descname">ProfileRequestStats</code><a class="headerlink" href="#_CPPv4N6nvidia15inferenceserver19ProfileRequestStatsE" title="Permalink to this definition">¶</a><br /></dt>
<dd><blockquote>
<div>Statistics collected for Profile requests.</div></blockquote>
<dl class="var">
<dt id="_CPPv4N6nvidia15inferenceserver19ProfileRequestStats7successE">
<a class="reference internal" href="#_CPPv4N6nvidia15inferenceserver12StatDurationE" title="nvidia::inferenceserver::StatDuration">StatDuration</a> <code class="descname">success</code><a class="headerlink" href="#_CPPv4N6nvidia15inferenceserver19ProfileRequestStats7successE" title="Permalink to this definition">¶</a><br /></dt>
<dd><p>Total time required to handle successful Profile requests, not
including HTTP or gRPC endpoint termination time.</p>
</dd></dl>

</dd></dl>

<dl class="var">
<dt id="_CPPv4N6nvidia15inferenceserver18HealthRequestStatsE">
message <code class="descname">HealthRequestStats</code><a class="headerlink" href="#_CPPv4N6nvidia15inferenceserver18HealthRequestStatsE" title="Permalink to this definition">¶</a><br /></dt>
<dd><blockquote>
<div>Statistics collected for Health requests.</div></blockquote>
<dl class="var">
<dt id="_CPPv4N6nvidia15inferenceserver18HealthRequestStats7successE">
<a class="reference internal" href="#_CPPv4N6nvidia15inferenceserver12StatDurationE" title="nvidia::inferenceserver::StatDuration">StatDuration</a> <code class="descname">success</code><a class="headerlink" href="#_CPPv4N6nvidia15inferenceserver18HealthRequestStats7successE" title="Permalink to this definition">¶</a><br /></dt>
<dd><p>Total time required to handle successful Health requests, not
including HTTP or gRPC endpoint termination time.</p>
</dd></dl>

</dd></dl>

<dl class="var">
<dt id="_CPPv4N6nvidia15inferenceserver17InferRequestStatsE">
message <code class="descname">InferRequestStats</code><a class="headerlink" href="#_CPPv4N6nvidia15inferenceserver17InferRequestStatsE" title="Permalink to this definition">¶</a><br /></dt>
<dd><blockquote>
<div>Statistics collected for Infer requests.</div></blockquote>
<dl class="var">
<dt id="_CPPv4N6nvidia15inferenceserver17InferRequestStats7successE">
<a class="reference internal" href="#_CPPv4N6nvidia15inferenceserver12StatDurationE" title="nvidia::inferenceserver::StatDuration">StatDuration</a> <code class="descname">success</code><a class="headerlink" href="#_CPPv4N6nvidia15inferenceserver17InferRequestStats7successE" title="Permalink to this definition">¶</a><br /></dt>
<dd><p>Total time required to handle successful Infer requests, not
including HTTP or gRPC endpoint termination time.</p>
</dd></dl>

<dl class="var">
<dt id="_CPPv4N6nvidia15inferenceserver17InferRequestStats6failedE">
<a class="reference internal" href="#_CPPv4N6nvidia15inferenceserver12StatDurationE" title="nvidia::inferenceserver::StatDuration">StatDuration</a> <code class="descname">failed</code><a class="headerlink" href="#_CPPv4N6nvidia15inferenceserver17InferRequestStats6failedE" title="Permalink to this definition">¶</a><br /></dt>
<dd><p>Total time required to handle failed Infer requests, not
including HTTP or gRPC endpoint termination time.</p>
</dd></dl>

<dl class="var">
<dt id="_CPPv4N6nvidia15inferenceserver17InferRequestStats7computeE">
<a class="reference internal" href="#_CPPv4N6nvidia15inferenceserver12StatDurationE" title="nvidia::inferenceserver::StatDuration">StatDuration</a> <code class="descname">compute</code><a class="headerlink" href="#_CPPv4N6nvidia15inferenceserver17InferRequestStats7computeE" title="Permalink to this definition">¶</a><br /></dt>
<dd><p>Time required to run inferencing for an inference request;
including time copying input tensors to GPU memory, time
executing the model, and time copying output tensors from GPU
memory.</p>
</dd></dl>

<dl class="var">
<dt id="_CPPv4N6nvidia15inferenceserver17InferRequestStats5queueE">
<a class="reference internal" href="#_CPPv4N6nvidia15inferenceserver12StatDurationE" title="nvidia::inferenceserver::StatDuration">StatDuration</a> <code class="descname">queue</code><a class="headerlink" href="#_CPPv4N6nvidia15inferenceserver17InferRequestStats5queueE" title="Permalink to this definition">¶</a><br /></dt>
<dd><p>Time an inference request waits in scheduling queue for an
available model instance.</p>
</dd></dl>

</dd></dl>

<dl class="enum">
<dt id="_CPPv4N6nvidia15inferenceserver15ModelReadyStateE">
<em class="property">enum </em><code class="descname">ModelReadyState</code><a class="headerlink" href="#_CPPv4N6nvidia15inferenceserver15ModelReadyStateE" title="Permalink to this definition">¶</a><br /></dt>
<dd><blockquote>
<div>Readiness status for models.</div></blockquote>
<dl class="enumerator">
<dt id="_CPPv4N6nvidia15inferenceserver15ModelReadyState15ModelReadyState13MODEL_UNKNOWNE">
<em class="property">enumerator </em><code class="descclassname">ModelReadyState<code class="descclassname">::</code></code><code class="descname">MODEL_UNKNOWN</code> = 0<a class="headerlink" href="#_CPPv4N6nvidia15inferenceserver15ModelReadyState15ModelReadyState13MODEL_UNKNOWNE" title="Permalink to this definition">¶</a><br /></dt>
<dd><p>The model is in an unknown state. The model is not available for
inferencing.</p>
</dd></dl>

<dl class="enumerator">
<dt id="_CPPv4N6nvidia15inferenceserver15ModelReadyState15ModelReadyState11MODEL_READYE">
<em class="property">enumerator </em><code class="descclassname">ModelReadyState<code class="descclassname">::</code></code><code class="descname">MODEL_READY</code> = 1<a class="headerlink" href="#_CPPv4N6nvidia15inferenceserver15ModelReadyState15ModelReadyState11MODEL_READYE" title="Permalink to this definition">¶</a><br /></dt>
<dd><p>The model is ready and available for inferencing.</p>
</dd></dl>

<dl class="enumerator">
<dt id="_CPPv4N6nvidia15inferenceserver15ModelReadyState15ModelReadyState17MODEL_UNAVAILABLEE">
<em class="property">enumerator </em><code class="descclassname">ModelReadyState<code class="descclassname">::</code></code><code class="descname">MODEL_UNAVAILABLE</code> = 2<a class="headerlink" href="#_CPPv4N6nvidia15inferenceserver15ModelReadyState15ModelReadyState17MODEL_UNAVAILABLEE" title="Permalink to this definition">¶</a><br /></dt>
<dd><p>The model is unavailable, indicating that the model failed to
load or has been implicitly or explicitly unloaded. The model is
not available for inferencing.</p>
</dd></dl>

<dl class="enumerator">
<dt id="_CPPv4N6nvidia15inferenceserver15ModelReadyState15ModelReadyState13MODEL_LOADINGE">
<em class="property">enumerator </em><code class="descclassname">ModelReadyState<code class="descclassname">::</code></code><code class="descname">MODEL_LOADING</code> = 3<a class="headerlink" href="#_CPPv4N6nvidia15inferenceserver15ModelReadyState15ModelReadyState13MODEL_LOADINGE" title="Permalink to this definition">¶</a><br /></dt>
<dd><p>The model is being loaded by the inference server. The model is
not available for inferencing.</p>
</dd></dl>

<dl class="enumerator">
<dt id="_CPPv4N6nvidia15inferenceserver15ModelReadyState15ModelReadyState15MODEL_UNLOADINGE">
<em class="property">enumerator </em><code class="descclassname">ModelReadyState<code class="descclassname">::</code></code><code class="descname">MODEL_UNLOADING</code> = 4<a class="headerlink" href="#_CPPv4N6nvidia15inferenceserver15ModelReadyState15ModelReadyState15MODEL_UNLOADINGE" title="Permalink to this definition">¶</a><br /></dt>
<dd><p>The model is being unloaded by the inference server. The model is
not available for inferencing.</p>
</dd></dl>

</dd></dl>

<dl class="var">
<dt id="_CPPv4N6nvidia15inferenceserver18ModelVersionStatusE">
message <code class="descname">ModelVersionStatus</code><a class="headerlink" href="#_CPPv4N6nvidia15inferenceserver18ModelVersionStatusE" title="Permalink to this definition">¶</a><br /></dt>
<dd><blockquote>
<div>Status for a version of a model.</div></blockquote>
<dl class="var">
<dt id="_CPPv4N6nvidia15inferenceserver18ModelVersionStatus12ready_statueE">
<a class="reference internal" href="#_CPPv4N6nvidia15inferenceserver15ModelReadyStateE" title="nvidia::inferenceserver::ModelReadyState">ModelReadyState</a> <code class="descname">ready_statue</code><a class="headerlink" href="#_CPPv4N6nvidia15inferenceserver18ModelVersionStatus12ready_statueE" title="Permalink to this definition">¶</a><br /></dt>
<dd><p>Current readiness state for the model.</p>
</dd></dl>

<dl class="var">
<dt id="_CPPv4N6nvidia15inferenceserver18ModelVersionStatus11infer_statsE">
map&lt;uint32, <a class="reference internal" href="#_CPPv4N6nvidia15inferenceserver17InferRequestStatsE" title="nvidia::inferenceserver::InferRequestStats">InferRequestStats</a>&gt; <code class="descname">infer_stats</code><a class="headerlink" href="#_CPPv4N6nvidia15inferenceserver18ModelVersionStatus11infer_statsE" title="Permalink to this definition">¶</a><br /></dt>
<dd><p>Inference statistics for the model, as a map from batch size
to the statistics. A batch size will not occur in the map
unless there has been at least one inference request of
that batch size.</p>
</dd></dl>

<dl class="var">
<dt id="_CPPv4N6nvidia15inferenceserver18ModelVersionStatus21model_execution_countE">
uint64 <code class="descname">model_execution_count</code><a class="headerlink" href="#_CPPv4N6nvidia15inferenceserver18ModelVersionStatus21model_execution_countE" title="Permalink to this definition">¶</a><br /></dt>
<dd><p>Cumulative number of model executions performed for the
model. A single model execution performs inferencing for
the entire request batch and can perform inferencing for multiple
requests if dynamic batching is enabled.</p>
</dd></dl>

<dl class="var">
<dt id="_CPPv4N6nvidia15inferenceserver18ModelVersionStatus21model_inference_countE">
uint64 <code class="descname">model_inference_count</code><a class="headerlink" href="#_CPPv4N6nvidia15inferenceserver18ModelVersionStatus21model_inference_countE" title="Permalink to this definition">¶</a><br /></dt>
<dd><p>Cumulative number of model inferences performed for the
model. Each inference in a batched request is counted as
an individual inference.</p>
</dd></dl>

</dd></dl>

<dl class="var">
<dt id="_CPPv4N6nvidia15inferenceserver11ModelStatusE">
message <code class="descname">ModelStatus</code><a class="headerlink" href="#_CPPv4N6nvidia15inferenceserver11ModelStatusE" title="Permalink to this definition">¶</a><br /></dt>
<dd><blockquote>
<div>Status for a model.</div></blockquote>
<dl class="var">
<dt id="_CPPv4N6nvidia15inferenceserver11ModelStatus6configE">
<a class="reference internal" href="model_config.proto.html#_CPPv4N6nvidia15inferenceserver11ModelConfigE" title="nvidia::inferenceserver::ModelConfig">ModelConfig</a> <code class="descname">config</code><a class="headerlink" href="#_CPPv4N6nvidia15inferenceserver11ModelStatus6configE" title="Permalink to this definition">¶</a><br /></dt>
<dd><p>The configuration for the model.</p>
</dd></dl>

<dl class="var">
<dt id="_CPPv4N6nvidia15inferenceserver11ModelStatus14version_statusE">
map&lt;int64, <a class="reference internal" href="#_CPPv4N6nvidia15inferenceserver18ModelVersionStatusE" title="nvidia::inferenceserver::ModelVersionStatus">ModelVersionStatus</a>&gt; <code class="descname">version_status</code><a class="headerlink" href="#_CPPv4N6nvidia15inferenceserver11ModelStatus14version_statusE" title="Permalink to this definition">¶</a><br /></dt>
<dd><p>Duration statistics for each version of the model, as a map
from version to the status. A version will not occur in the map
unless there has been at least one inference request of
that model version. A version of -1 indicates the status is
for requests for which the version could not be determined.</p>
</dd></dl>

</dd></dl>

<dl class="enum">
<dt id="_CPPv4N6nvidia15inferenceserver16ServerReadyStateE">
<em class="property">enum </em><code class="descname">ServerReadyState</code><a class="headerlink" href="#_CPPv4N6nvidia15inferenceserver16ServerReadyStateE" title="Permalink to this definition">¶</a><br /></dt>
<dd><blockquote>
<div>Readiness status for the inference server.</div></blockquote>
<dl class="enumerator">
<dt id="_CPPv4N6nvidia15inferenceserver16ServerReadyState16ServerReadyState14SERVER_INVALIDE">
<em class="property">enumerator </em><code class="descclassname">ServerReadyState<code class="descclassname">::</code></code><code class="descname">SERVER_INVALID</code> = 0<a class="headerlink" href="#_CPPv4N6nvidia15inferenceserver16ServerReadyState16ServerReadyState14SERVER_INVALIDE" title="Permalink to this definition">¶</a><br /></dt>
<dd><p>The server is in an invalid state and will likely not
response correctly to any requests.</p>
</dd></dl>

<dl class="enumerator">
<dt id="_CPPv4N6nvidia15inferenceserver16ServerReadyState16ServerReadyState19SERVER_INITIALIZINGE">
<em class="property">enumerator </em><code class="descclassname">ServerReadyState<code class="descclassname">::</code></code><code class="descname">SERVER_INITIALIZING</code> = 1<a class="headerlink" href="#_CPPv4N6nvidia15inferenceserver16ServerReadyState16ServerReadyState19SERVER_INITIALIZINGE" title="Permalink to this definition">¶</a><br /></dt>
<dd><p>The server is initializing.</p>
</dd></dl>

<dl class="enumerator">
<dt id="_CPPv4N6nvidia15inferenceserver16ServerReadyState16ServerReadyState12SERVER_READYE">
<em class="property">enumerator </em><code class="descclassname">ServerReadyState<code class="descclassname">::</code></code><code class="descname">SERVER_READY</code> = 2<a class="headerlink" href="#_CPPv4N6nvidia15inferenceserver16ServerReadyState16ServerReadyState12SERVER_READYE" title="Permalink to this definition">¶</a><br /></dt>
<dd><p>The server is ready and accepting requests.</p>
</dd></dl>

<dl class="enumerator">
<dt id="_CPPv4N6nvidia15inferenceserver16ServerReadyState16ServerReadyState14SERVER_EXITINGE">
<em class="property">enumerator </em><code class="descclassname">ServerReadyState<code class="descclassname">::</code></code><code class="descname">SERVER_EXITING</code> = 3<a class="headerlink" href="#_CPPv4N6nvidia15inferenceserver16ServerReadyState16ServerReadyState14SERVER_EXITINGE" title="Permalink to this definition">¶</a><br /></dt>
<dd><p>The server is exiting and will not respond to requests.</p>
</dd></dl>

<dl class="enumerator">
<dt id="_CPPv4N6nvidia15inferenceserver16ServerReadyState16ServerReadyState27SERVER_FAILED_TO_INITIALIZEE">
<em class="property">enumerator </em><code class="descclassname">ServerReadyState<code class="descclassname">::</code></code><code class="descname">SERVER_FAILED_TO_INITIALIZE</code> = 10<a class="headerlink" href="#_CPPv4N6nvidia15inferenceserver16ServerReadyState16ServerReadyState27SERVER_FAILED_TO_INITIALIZEE" title="Permalink to this definition">¶</a><br /></dt>
<dd><p>The server did not initialize correctly. Most requests will fail.</p>
</dd></dl>

</dd></dl>

<dl class="var">
<dt id="_CPPv4N6nvidia15inferenceserver12ServerStatusE">
message <code class="descname">ServerStatus</code><a class="headerlink" href="#_CPPv4N6nvidia15inferenceserver12ServerStatusE" title="Permalink to this definition">¶</a><br /></dt>
<dd><blockquote>
<div>Status for the inference server.</div></blockquote>
<dl class="var">
<dt id="_CPPv4N6nvidia15inferenceserver12ServerStatus2idE">
string <code class="descname">id</code><a class="headerlink" href="#_CPPv4N6nvidia15inferenceserver12ServerStatus2idE" title="Permalink to this definition">¶</a><br /></dt>
<dd><p>The server’s ID.</p>
</dd></dl>

<dl class="var">
<dt id="_CPPv4N6nvidia15inferenceserver12ServerStatus7versionE">
string <code class="descname">version</code><a class="headerlink" href="#_CPPv4N6nvidia15inferenceserver12ServerStatus7versionE" title="Permalink to this definition">¶</a><br /></dt>
<dd><p>The server’s version.</p>
</dd></dl>

<dl class="var">
<dt id="_CPPv4N6nvidia15inferenceserver12ServerStatus11ready_stateE">
<a class="reference internal" href="#_CPPv4N6nvidia15inferenceserver16ServerReadyStateE" title="nvidia::inferenceserver::ServerReadyState">ServerReadyState</a> <code class="descname">ready_state</code><a class="headerlink" href="#_CPPv4N6nvidia15inferenceserver12ServerStatus11ready_stateE" title="Permalink to this definition">¶</a><br /></dt>
<dd><p>Current readiness state for the server.</p>
</dd></dl>

<dl class="var">
<dt id="_CPPv4N6nvidia15inferenceserver12ServerStatus9uptime_nsE">
uint64 <code class="descname">uptime_ns</code><a class="headerlink" href="#_CPPv4N6nvidia15inferenceserver12ServerStatus9uptime_nsE" title="Permalink to this definition">¶</a><br /></dt>
<dd><p>Server uptime in nanoseconds.</p>
</dd></dl>

<dl class="var">
<dt id="_CPPv4N6nvidia15inferenceserver12ServerStatus12model_statusE">
map&lt;string, <a class="reference internal" href="#_CPPv4N6nvidia15inferenceserver11ModelStatusE" title="nvidia::inferenceserver::ModelStatus">ModelStatus</a>&gt; <code class="descname">model_status</code><a class="headerlink" href="#_CPPv4N6nvidia15inferenceserver12ServerStatus12model_statusE" title="Permalink to this definition">¶</a><br /></dt>
<dd><p>Status for each model, as a map from model name to the
status.</p>
</dd></dl>

<dl class="var">
<dt id="_CPPv4N6nvidia15inferenceserver12ServerStatus12status_statsE">
<a class="reference internal" href="#_CPPv4N6nvidia15inferenceserver18StatusRequestStatsE" title="nvidia::inferenceserver::StatusRequestStats">StatusRequestStats</a> <code class="descname">status_stats</code><a class="headerlink" href="#_CPPv4N6nvidia15inferenceserver12ServerStatus12status_statsE" title="Permalink to this definition">¶</a><br /></dt>
<dd><p>Statistics for Status requests.</p>
</dd></dl>

<dl class="var">
<dt id="_CPPv4N6nvidia15inferenceserver12ServerStatus13profile_statsE">
<a class="reference internal" href="#_CPPv4N6nvidia15inferenceserver19ProfileRequestStatsE" title="nvidia::inferenceserver::ProfileRequestStats">ProfileRequestStats</a> <code class="descname">profile_stats</code><a class="headerlink" href="#_CPPv4N6nvidia15inferenceserver12ServerStatus13profile_statsE" title="Permalink to this definition">¶</a><br /></dt>
<dd><p>Statistics for Profile requests.</p>
</dd></dl>

<dl class="var">
<dt id="_CPPv4N6nvidia15inferenceserver12ServerStatus12health_statsE">
<a class="reference internal" href="#_CPPv4N6nvidia15inferenceserver18HealthRequestStatsE" title="nvidia::inferenceserver::HealthRequestStats">HealthRequestStats</a> <code class="descname">health_stats</code><a class="headerlink" href="#_CPPv4N6nvidia15inferenceserver12ServerStatus12health_statsE" title="Permalink to this definition">¶</a><br /></dt>
<dd><p>Statistics for Health requests.</p>
</dd></dl>

</dd></dl>

</div>


           </div>
           
          </div>
          <footer>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2018, NVIDIA Corporation

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
    

  <style>
  a:link, a:visited {
    color: #76b900;
  }

  a:hover {
    color: #8c0;
  }

  .rst-content dl:not(.docutils) dt {
    background: rgba(118, 185, 0, 0.1);
    color: rgba(59,93,0,1);
    border-top: solid 3px rgba(59,93,0,1);
  }
  </style>
  

</body>
</html>