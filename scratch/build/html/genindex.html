
<!--
# Copyright (c) 2018, NVIDIA CORPORATION. All rights reserved.
#
# Redistribution and use in source and binary forms, with or without
# modification, are permitted provided that the following conditions
# are met:
#  * Redistributions of source code must retain the above copyright
#    notice, this list of conditions and the following disclaimer.
#  * Redistributions in binary form must reproduce the above copyright
#    notice, this list of conditions and the following disclaimer in the
#    documentation and/or other materials provided with the distribution.
#  * Neither the name of NVIDIA CORPORATION nor the names of its
#    contributors may be used to endorse or promote products derived
#    from this software without specific prior written permission.
#
# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
# EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
# PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
# PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
# OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
# (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
# OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
-->


<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Index &mdash; NVIDIA TensorRT Inference Server 0.12.0dev
 documentation</title>
  

  
  
  
  
    <link rel="canonical" href="https://docs.nvidia.com/deeplearning/sdk/tensorrt-inference-server-guide/docs/index.htmlgenindex.html"/>
  

  
  <script type="text/javascript" src="_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
        <script type="text/javascript" src="_static/jquery.js"></script>
        <script type="text/javascript" src="_static/underscore.js"></script>
        <script type="text/javascript" src="_static/doctools.js"></script>
        <script type="text/javascript" src="_static/language_data.js"></script>
        <script type="text/javascript" src="_static/collapsible-lists/js/CollapsibleLists.compressed.js"></script>
        <script type="text/javascript" src="_static/collapsible-lists/js/apply-collapsible-lists.js"></script>
    
    <script type="text/javascript" src="_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="_static/collapsible-lists/css/tree_view.css" type="text/css" />
    <link rel="index" title="Index" href="#" />
    <link rel="search" title="Search" href="search.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
           

          
            <a href="index.html" class="icon icon-home"> NVIDIA TensorRT Inference Server
          

          
          </a>

          
            
            
              <div class="version">
                0.12.0dev
-ad20b82<br/>
Version select: <select onChange="window.location.href = this.value" onFocus="this.selectedIndex = -1">
    <option value="https://docs.nvidia.com/deeplearning/sdk/tensorrt-inference-server-guide/docs/index.html">Current release</option>
    <option value="https://docs.nvidia.com/deeplearning/sdk/tensorrt-inference-server-master-branch-guide/docs/index.html">master (unstable)</option>
    <option value="https://docs.nvidia.com/deeplearning/sdk/inference-server-archived/index.html">Older releases</option>
</select>
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          

  <style>
    /* Sidebar header (and topbar for mobile) */
    .wy-side-nav-search, .wy-nav-top {
      background: #76b900;
    }

    .wy-side-nav-search a:link, .wy-nav-top a:link {
      color: #fff;
    }
    .wy-side-nav-search a:visited, .wy-nav-top a:visited {
      color: #fff;
    }
    .wy-side-nav-search a:hover, .wy-nav-top a:hover {
      color: #fff;
    }

    .wy-menu-vertical a:link, .wy-menu-vertical a:visited {
      color: #d9d9d9
    }

    .wy-menu-vertical a:active {
      background-color: #76b900
    }

    .wy-side-nav-search>div.version {
      color: rgba(0, 0, 0, 0.3)
    }
  </style>
  
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul>
<li class="toctree-l1"><a class="reference internal" href="index.html">Documentation home</a></li>
</ul>
<p class="caption"><span class="caption-text">User Guide</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="quickstart.html">Quickstart</a><ul>
<li class="toctree-l2"><a class="reference internal" href="quickstart.html#prerequisites">Prerequisites</a></li>
<li class="toctree-l2"><a class="reference internal" href="quickstart.html#using-a-prebuilt-docker-container">Using A Prebuilt Docker Container</a></li>
<li class="toctree-l2"><a class="reference internal" href="quickstart.html#building-from-source-code">Building From Source Code</a></li>
<li class="toctree-l2"><a class="reference internal" href="quickstart.html#run-tensorrt-inference-server">Run TensorRT Inference Server</a></li>
<li class="toctree-l2"><a class="reference internal" href="quickstart.html#verify-inference-server-is-running-correctly">Verify Inference Server Is Running Correctly</a></li>
<li class="toctree-l2"><a class="reference internal" href="quickstart.html#building-the-client-examples">Building The Client Examples</a></li>
<li class="toctree-l2"><a class="reference internal" href="quickstart.html#running-the-image-classification-example">Running The Image Classification Example</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="install.html">Installing the Server</a><ul>
<li class="toctree-l2"><a class="reference internal" href="install.html#installing-prebuilt-containers">Installing Prebuilt Containers</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="run.html">Running the Server</a><ul>
<li class="toctree-l2"><a class="reference internal" href="run.html#example-model-repository">Example Model Repository</a></li>
<li class="toctree-l2"><a class="reference internal" href="run.html#running-the-inference-server">Running The Inference Server</a></li>
<li class="toctree-l2"><a class="reference internal" href="run.html#running-the-inference-server-on-a-system-without-a-gpu">Running The Inference Server On A System Without A GPU</a></li>
<li class="toctree-l2"><a class="reference internal" href="run.html#checking-inference-server-status">Checking Inference Server Status</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="client.html">Client Libraries and Examples</a><ul>
<li class="toctree-l2"><a class="reference internal" href="client.html#building-the-client-libraries-and-examples">Building the Client Libraries and Examples</a></li>
<li class="toctree-l2"><a class="reference internal" href="client.html#image-classification-example-application">Image Classification Example Application</a></li>
<li class="toctree-l2"><a class="reference internal" href="client.html#performance-example-application">Performance Example Application</a></li>
<li class="toctree-l2"><a class="reference internal" href="client.html#client-api">Client API</a><ul>
<li class="toctree-l3"><a class="reference internal" href="client.html#string-datatype">String Datatype</a></li>
<li class="toctree-l3"><a class="reference internal" href="client.html#stream-inference">Stream Inference</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="model_repository.html">Model Repository</a><ul>
<li class="toctree-l2"><a class="reference internal" href="model_repository.html#modifying-the-model-repository">Modifying the Model Repository</a></li>
<li class="toctree-l2"><a class="reference internal" href="model_repository.html#model-versions">Model Versions</a></li>
<li class="toctree-l2"><a class="reference internal" href="model_repository.html#framework-model-definition">Framework Model Definition</a><ul>
<li class="toctree-l3"><a class="reference internal" href="model_repository.html#tensorrt-models">TensorRT Models</a></li>
<li class="toctree-l3"><a class="reference internal" href="model_repository.html#tensorflow-models">TensorFlow Models</a></li>
<li class="toctree-l3"><a class="reference internal" href="model_repository.html#caffe2-models">Caffe2 Models</a></li>
<li class="toctree-l3"><a class="reference internal" href="model_repository.html#tensorrt-tensorflow-models">TensorRT/TensorFlow Models</a></li>
<li class="toctree-l3"><a class="reference internal" href="model_repository.html#onnx-models">ONNX Models</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="model_repository.html#custom-backends">Custom Backends</a><ul>
<li class="toctree-l3"><a class="reference internal" href="model_repository.html#custom-backend-api">Custom Backend API</a></li>
<li class="toctree-l3"><a class="reference internal" href="model_repository.html#example-custom-backend">Example Custom Backend</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="model_configuration.html">Model Configuration</a><ul>
<li class="toctree-l2"><a class="reference internal" href="model_configuration.html#generated-model-configuration">Generated Model Configuration</a></li>
<li class="toctree-l2"><a class="reference internal" href="model_configuration.html#datatypes">Datatypes</a></li>
<li class="toctree-l2"><a class="reference internal" href="model_configuration.html#version-policy">Version Policy</a></li>
<li class="toctree-l2"><a class="reference internal" href="model_configuration.html#instance-groups">Instance Groups</a></li>
<li class="toctree-l2"><a class="reference internal" href="model_configuration.html#dynamic-batching">Dynamic Batching</a></li>
<li class="toctree-l2"><a class="reference internal" href="model_configuration.html#optimization-policy">Optimization Policy</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="http_grpc_api.html">Inference Server API</a><ul>
<li class="toctree-l2"><a class="reference internal" href="http_grpc_api.html#health">Health</a></li>
<li class="toctree-l2"><a class="reference internal" href="http_grpc_api.html#status">Status</a></li>
<li class="toctree-l2"><a class="reference internal" href="http_grpc_api.html#inference">Inference</a></li>
<li class="toctree-l2"><a class="reference internal" href="http_grpc_api.html#stream-inference">Stream Inference</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="metrics.html">Metrics</a></li>
</ul>
<p class="caption"><span class="caption-text">Developer Guide</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="architecture.html">Architecture</a><ul>
<li class="toctree-l2"><a class="reference internal" href="architecture.html#concurrent-model-execution">Concurrent Model Execution</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="contribute.html">Contributing</a><ul>
<li class="toctree-l2"><a class="reference internal" href="contribute.html#coding-convention">Coding Convention</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="build.html">Building</a><ul>
<li class="toctree-l2"><a class="reference internal" href="build.html#building-the-server">Building the Server</a><ul>
<li class="toctree-l3"><a class="reference internal" href="build.html#incremental-builds">Incremental Builds</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="build.html#building-the-client-libraries-and-examples">Building the Client Libraries and Examples</a></li>
<li class="toctree-l2"><a class="reference internal" href="build.html#building-the-documentation">Building the Documentation</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="test.html">Testing</a><ul>
<li class="toctree-l2"><a class="reference internal" href="test.html#generate-qa-model-repositories">Generate QA Model Repositories</a></li>
<li class="toctree-l2"><a class="reference internal" href="test.html#build-qa-container">Build QA Container</a></li>
<li class="toctree-l2"><a class="reference internal" href="test.html#run-qa-container">Run QA Container</a></li>
</ul>
</li>
</ul>
<p class="caption"><span class="caption-text">API Reference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="protobuf_api/protobuf_api_root.html">Protobuf API</a><ul>
<li class="toctree-l2"><a class="reference internal" href="protobuf_api/protobuf_api_root.html#http-grpc-api">HTTP/GRPC API</a></li>
<li class="toctree-l2"><a class="reference internal" href="protobuf_api/protobuf_api_root.html#model-configuration">Model Configuration</a></li>
<li class="toctree-l2"><a class="reference internal" href="protobuf_api/protobuf_api_root.html#status">Status</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="cpp_api/cpp_api_root.html">C++ API</a><ul>
<li class="toctree-l2"><a class="reference internal" href="cpp_api/cpp_api_root.html#class-hierarchy">Class Hierarchy</a></li>
<li class="toctree-l2"><a class="reference internal" href="cpp_api/cpp_api_root.html#file-hierarchy">File Hierarchy</a></li>
<li class="toctree-l2"><a class="reference internal" href="cpp_api/cpp_api_root.html#full-api">Full API</a><ul>
<li class="toctree-l3"><a class="reference internal" href="cpp_api/cpp_api_root.html#namespaces">Namespaces</a><ul>
<li class="toctree-l4"><a class="reference internal" href="cpp_api/namespace_nvidia.html">Namespace nvidia</a></li>
<li class="toctree-l4"><a class="reference internal" href="cpp_api/namespace_nvidia__inferenceserver.html">Namespace nvidia::inferenceserver</a></li>
<li class="toctree-l4"><a class="reference internal" href="cpp_api/namespace_nvidia__inferenceserver__client.html">Namespace nvidia::inferenceserver::client</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="cpp_api/cpp_api_root.html#classes-and-structs">Classes and Structs</a><ul>
<li class="toctree-l4"><a class="reference internal" href="cpp_api/structcustom__payload__struct.html">Struct custom_payload_struct</a></li>
<li class="toctree-l4"><a class="reference internal" href="cpp_api/structnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result_1_1ClassResult.html">Struct Result::ClassResult</a></li>
<li class="toctree-l4"><a class="reference internal" href="cpp_api/structnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Stat.html">Struct InferContext::Stat</a></li>
<li class="toctree-l4"><a class="reference internal" href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1Error.html">Class Error</a></li>
<li class="toctree-l4"><a class="reference internal" href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext.html">Class InferContext</a></li>
<li class="toctree-l4"><a class="reference internal" href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Input.html">Class InferContext::Input</a></li>
<li class="toctree-l4"><a class="reference internal" href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Options.html">Class InferContext::Options</a></li>
<li class="toctree-l4"><a class="reference internal" href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Output.html">Class InferContext::Output</a></li>
<li class="toctree-l4"><a class="reference internal" href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Request.html">Class InferContext::Request</a></li>
<li class="toctree-l4"><a class="reference internal" href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1RequestTimers.html">Class InferContext::RequestTimers</a></li>
<li class="toctree-l4"><a class="reference internal" href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result.html">Class InferContext::Result</a></li>
<li class="toctree-l4"><a class="reference internal" href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferGrpcContext.html">Class InferGrpcContext</a></li>
<li class="toctree-l4"><a class="reference internal" href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferGrpcStreamContext.html">Class InferGrpcStreamContext</a></li>
<li class="toctree-l4"><a class="reference internal" href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferHttpContext.html">Class InferHttpContext</a></li>
<li class="toctree-l4"><a class="reference internal" href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ProfileContext.html">Class ProfileContext</a></li>
<li class="toctree-l4"><a class="reference internal" href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ProfileGrpcContext.html">Class ProfileGrpcContext</a></li>
<li class="toctree-l4"><a class="reference internal" href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ProfileHttpContext.html">Class ProfileHttpContext</a></li>
<li class="toctree-l4"><a class="reference internal" href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ServerHealthContext.html">Class ServerHealthContext</a></li>
<li class="toctree-l4"><a class="reference internal" href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ServerHealthGrpcContext.html">Class ServerHealthGrpcContext</a></li>
<li class="toctree-l4"><a class="reference internal" href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ServerHealthHttpContext.html">Class ServerHealthHttpContext</a></li>
<li class="toctree-l4"><a class="reference internal" href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ServerStatusContext.html">Class ServerStatusContext</a></li>
<li class="toctree-l4"><a class="reference internal" href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ServerStatusGrpcContext.html">Class ServerStatusGrpcContext</a></li>
<li class="toctree-l4"><a class="reference internal" href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ServerStatusHttpContext.html">Class ServerStatusHttpContext</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="cpp_api/cpp_api_root.html#functions">Functions</a><ul>
<li class="toctree-l4"><a class="reference internal" href="cpp_api/function_custom_8h_1aedfdcb69f8eaab594173b197b10dc804.html">Function CustomErrorString</a></li>
<li class="toctree-l4"><a class="reference internal" href="cpp_api/function_custom_8h_1afc75c0a4e1a169562586c2d4ff09ba40.html">Function CustomExecute</a></li>
<li class="toctree-l4"><a class="reference internal" href="cpp_api/function_custom_8h_1a9cd799dfef77db82babc6ddd4a4be707.html">Function CustomFinalize</a></li>
<li class="toctree-l4"><a class="reference internal" href="cpp_api/function_custom_8h_1ad384f6af9bfb11b82b2b4e411f799b6f.html">Function CustomInitialize</a></li>
<li class="toctree-l4"><a class="reference internal" href="cpp_api/function_request_8h_1a19d352bb13848c14b5f03274b25ecee3.html">Function nvidia::inferenceserver::client::operator&lt;&lt;</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="cpp_api/cpp_api_root.html#defines">Defines</a><ul>
<li class="toctree-l4"><a class="reference internal" href="cpp_api/define_custom_8h_1aaf537da73bb55fda05fb4e39defe1949.html">Define CUSTOM_NO_GPU_DEVICE</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="cpp_api/cpp_api_root.html#typedefs">Typedefs</a><ul>
<li class="toctree-l4"><a class="reference internal" href="cpp_api/typedef_custom_8h_1aa6616d50105d3441ba277c6dbce66286.html">Typedef CustomErrorStringFn_t</a></li>
<li class="toctree-l4"><a class="reference internal" href="cpp_api/typedef_custom_8h_1ac0c3508cd00e9db56912ff40deecf9d2.html">Typedef CustomExecuteFn_t</a></li>
<li class="toctree-l4"><a class="reference internal" href="cpp_api/typedef_custom_8h_1aa32f51734df1f217cc7c2b07f31f8f3f.html">Typedef CustomFinalizeFn_t</a></li>
<li class="toctree-l4"><a class="reference internal" href="cpp_api/typedef_custom_8h_1aee3723f1a17ff4664691655e056a309c.html">Typedef CustomGetNextInputFn_t</a></li>
<li class="toctree-l4"><a class="reference internal" href="cpp_api/typedef_custom_8h_1a71cb419d6e7feee902115438973d02f8.html">Typedef CustomGetOutputFn_t</a></li>
<li class="toctree-l4"><a class="reference internal" href="cpp_api/typedef_custom_8h_1a11fea108abb0b28f86a96e6b7cd92a20.html">Typedef CustomInitializeFn_t</a></li>
<li class="toctree-l4"><a class="reference internal" href="cpp_api/typedef_custom_8h_1a99a7b9dcc756702784a869d9afb59df4.html">Typedef CustomPayload</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="cpp_api/cpp_api_root.html#directories">Directories</a><ul>
<li class="toctree-l4"><a class="reference internal" href="cpp_api/dir_src.html">Directory src</a></li>
<li class="toctree-l4"><a class="reference internal" href="cpp_api/dir_src_clients.html">Directory clients</a></li>
<li class="toctree-l4"><a class="reference internal" href="cpp_api/dir_src_clients_c++.html">Directory c++</a></li>
<li class="toctree-l4"><a class="reference internal" href="cpp_api/dir_src_servables.html">Directory servables</a></li>
<li class="toctree-l4"><a class="reference internal" href="cpp_api/dir_src_servables_custom.html">Directory custom</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="cpp_api/cpp_api_root.html#files">Files</a><ul>
<li class="toctree-l4"><a class="reference internal" href="cpp_api/file_src_servables_custom_custom.h.html">File custom.h</a></li>
<li class="toctree-l4"><a class="reference internal" href="cpp_api/file_src_clients_c++_request.h.html">File request.h</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="python_api.html">Python API</a><ul>
<li class="toctree-l2"><a class="reference internal" href="python_api.html#client">Client</a></li>
</ul>
</li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">NVIDIA TensorRT Inference Server</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html">Docs</a> &raquo;</li>
        
      <li></li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            

<h1 id="index">Index</h1>

<div class="genindex-jumpbox">
 <a href="#C"><strong>C</strong></a>
 | <a href="#N"><strong>N</strong></a>
 
</div>
<h2 id="C">C</h2>
<table style="width: 100%" class="indextable genindextable"><tr>
  <td style="width: 33%; vertical-align: top;"><ul>
      <li><a href="cpp_api/define_custom_8h_1aaf537da73bb55fda05fb4e39defe1949.html#c.CUSTOM_NO_GPU_DEVICE">CUSTOM_NO_GPU_DEVICE (C macro)</a>
</li>
      <li><a href="cpp_api/structcustom__payload__struct.html#_CPPv421custom_payload_struct">custom_payload_struct (C++ class)</a>
</li>
      <li><a href="cpp_api/structcustom__payload__struct.html#_CPPv4N21custom_payload_struct10batch_sizeE">custom_payload_struct::batch_size (C++ member)</a>
</li>
      <li><a href="cpp_api/structcustom__payload__struct.html#_CPPv4N21custom_payload_struct10error_codeE">custom_payload_struct::error_code (C++ member)</a>
</li>
      <li><a href="cpp_api/structcustom__payload__struct.html#_CPPv4N21custom_payload_struct9input_cntE">custom_payload_struct::input_cnt (C++ member)</a>
</li>
      <li><a href="cpp_api/structcustom__payload__struct.html#_CPPv4N21custom_payload_struct13input_contextE">custom_payload_struct::input_context (C++ member)</a>
</li>
      <li><a href="cpp_api/structcustom__payload__struct.html#_CPPv4N21custom_payload_struct11input_namesE">custom_payload_struct::input_names (C++ member)</a>
</li>
      <li><a href="cpp_api/structcustom__payload__struct.html#_CPPv4N21custom_payload_struct20input_shape_dim_cntsE">custom_payload_struct::input_shape_dim_cnts (C++ member)</a>
</li>
      <li><a href="cpp_api/structcustom__payload__struct.html#_CPPv4N21custom_payload_struct16input_shape_dimsE">custom_payload_struct::input_shape_dims (C++ member)</a>
</li>
      <li><a href="cpp_api/structcustom__payload__struct.html#_CPPv4N21custom_payload_struct10output_cntE">custom_payload_struct::output_cnt (C++ member)</a>
</li>
      <li><a href="cpp_api/structcustom__payload__struct.html#_CPPv4N21custom_payload_struct14output_contextE">custom_payload_struct::output_context (C++ member)</a>
</li>
  </ul></td>
  <td style="width: 33%; vertical-align: top;"><ul>
      <li><a href="cpp_api/structcustom__payload__struct.html#_CPPv4N21custom_payload_struct21required_output_namesE">custom_payload_struct::required_output_names (C++ member)</a>
</li>
      <li><a href="cpp_api/function_custom_8h_1aedfdcb69f8eaab594173b197b10dc804.html#_CPPv417CustomErrorStringPvi">CustomErrorString (C++ function)</a>
</li>
      <li><a href="cpp_api/typedef_custom_8h_1aa6616d50105d3441ba277c6dbce66286.html#_CPPv421CustomErrorStringFn_t">CustomErrorStringFn_t (C++ type)</a>
</li>
      <li><a href="cpp_api/function_custom_8h_1afc75c0a4e1a169562586c2d4ff09ba40.html#_CPPv413CustomExecutePv8uint32_tP13CustomPayload22CustomGetNextInputFn_t19CustomGetOutputFn_t">CustomExecute (C++ function)</a>
</li>
      <li><a href="cpp_api/typedef_custom_8h_1ac0c3508cd00e9db56912ff40deecf9d2.html#_CPPv417CustomExecuteFn_t">CustomExecuteFn_t (C++ type)</a>
</li>
      <li><a href="cpp_api/function_custom_8h_1a9cd799dfef77db82babc6ddd4a4be707.html#_CPPv414CustomFinalizePv">CustomFinalize (C++ function)</a>
</li>
      <li><a href="cpp_api/typedef_custom_8h_1aa32f51734df1f217cc7c2b07f31f8f3f.html#_CPPv418CustomFinalizeFn_t">CustomFinalizeFn_t (C++ type)</a>
</li>
      <li><a href="cpp_api/typedef_custom_8h_1aee3723f1a17ff4664691655e056a309c.html#_CPPv422CustomGetNextInputFn_t">CustomGetNextInputFn_t (C++ type)</a>
</li>
      <li><a href="cpp_api/typedef_custom_8h_1a71cb419d6e7feee902115438973d02f8.html#_CPPv419CustomGetOutputFn_t">CustomGetOutputFn_t (C++ type)</a>
</li>
      <li><a href="cpp_api/function_custom_8h_1ad384f6af9bfb11b82b2b4e411f799b6f.html#_CPPv416CustomInitializePKc6size_tiPPv">CustomInitialize (C++ function)</a>
</li>
      <li><a href="cpp_api/typedef_custom_8h_1a11fea108abb0b28f86a96e6b7cd92a20.html#_CPPv420CustomInitializeFn_t">CustomInitializeFn_t (C++ type)</a>
</li>
      <li><a href="cpp_api/typedef_custom_8h_1a99a7b9dcc756702784a869d9afb59df4.html#_CPPv413CustomPayload">CustomPayload (C++ type)</a>
</li>
  </ul></td>
</tr></table>

<h2 id="N">N</h2>
<table style="width: 100%" class="indextable genindextable"><tr>
  <td style="width: 33%; vertical-align: top;"><ul>
      <li><a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1Error.html#_CPPv4N6nvidia15inferenceserver6client5ErrorE">nvidia::inferenceserver::client::Error (C++ class)</a>
</li>
      <li><a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1Error.html#_CPPv4NK6nvidia15inferenceserver6client5Error4CodeEv">nvidia::inferenceserver::client::Error::Code (C++ function)</a>
</li>
      <li><a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1Error.html#_CPPv4N6nvidia15inferenceserver6client5Error5ErrorE17RequestStatusCode">nvidia::inferenceserver::client::Error::Error (C++ function)</a>, <a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1Error.html#_CPPv4N6nvidia15inferenceserver6client5Error5ErrorE17RequestStatusCodeRKNSt6stringE">[1]</a>, <a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1Error.html#_CPPv4N6nvidia15inferenceserver6client5Error5ErrorERK13RequestStatus">[2]</a>
</li>
      <li><a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1Error.html#_CPPv4NK6nvidia15inferenceserver6client5Error4IsOkEv">nvidia::inferenceserver::client::Error::IsOk (C++ function)</a>
</li>
      <li><a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1Error.html#_CPPv4NK6nvidia15inferenceserver6client5Error7MessageEv">nvidia::inferenceserver::client::Error::Message (C++ function)</a>
</li>
      <li><a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1Error.html#_CPPv4NK6nvidia15inferenceserver6client5Error9RequestIdEv">nvidia::inferenceserver::client::Error::RequestId (C++ function)</a>
</li>
      <li><a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1Error.html#_CPPv4NK6nvidia15inferenceserver6client5Error8ServerIdEv">nvidia::inferenceserver::client::Error::ServerId (C++ function)</a>
</li>
      <li><a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1Error.html#_CPPv4N6nvidia15inferenceserver6client5Error7SuccessE">nvidia::inferenceserver::client::Error::Success (C++ member)</a>
</li>
      <li><a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext.html#_CPPv4N6nvidia15inferenceserver6client12InferContextE">nvidia::inferenceserver::client::InferContext (C++ class)</a>
</li>
      <li><a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext.html#_CPPv4N6nvidia15inferenceserver6client12InferContext17async_request_id_E">nvidia::inferenceserver::client::InferContext::async_request_id_ (C++ member)</a>
</li>
      <li><a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext.html#_CPPv4N6nvidia15inferenceserver6client12InferContext11AsyncReqMapE">nvidia::inferenceserver::client::InferContext::AsyncReqMap (C++ type)</a>
</li>
      <li><a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext.html#_CPPv4N6nvidia15inferenceserver6client12InferContext8AsyncRunEPNSt10shared_ptrI7RequestEE">nvidia::inferenceserver::client::InferContext::AsyncRun (C++ function)</a>
</li>
      <li><a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext.html#_CPPv4N6nvidia15inferenceserver6client12InferContext13AsyncTransferEv">nvidia::inferenceserver::client::InferContext::AsyncTransfer (C++ function)</a>
</li>
      <li><a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext.html#_CPPv4N6nvidia15inferenceserver6client12InferContext11batch_size_E">nvidia::inferenceserver::client::InferContext::batch_size_ (C++ member)</a>
</li>
      <li><a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext.html#_CPPv4N6nvidia15inferenceserver6client12InferContext5CLASSE">nvidia::inferenceserver::client::InferContext::CLASS (C++ enumerator)</a>
</li>
      <li><a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext.html#_CPPv4N6nvidia15inferenceserver6client12InferContext13context_stat_E">nvidia::inferenceserver::client::InferContext::context_stat_ (C++ member)</a>
</li>
      <li><a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext.html#_CPPv4N6nvidia15inferenceserver6client12InferContext15correlation_id_E">nvidia::inferenceserver::client::InferContext::correlation_id_ (C++ member)</a>
</li>
      <li><a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext.html#_CPPv4N6nvidia15inferenceserver6client12InferContext3cv_E">nvidia::inferenceserver::client::InferContext::cv_ (C++ member)</a>
</li>
      <li><a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext.html#_CPPv4N6nvidia15inferenceserver6client12InferContext8exiting_E">nvidia::inferenceserver::client::InferContext::exiting_ (C++ member)</a>
</li>
      <li><a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext.html#_CPPv4N6nvidia15inferenceserver6client12InferContext18GetAsyncRunResultsEP9ResultMapRKNSt10shared_ptrI7RequestEEb">nvidia::inferenceserver::client::InferContext::GetAsyncRunResults (C++ function)</a>
</li>
      <li><a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext.html#_CPPv4NK6nvidia15inferenceserver6client12InferContext8GetInputERKNSt6stringEPNSt10shared_ptrI5InputEE">nvidia::inferenceserver::client::InferContext::GetInput (C++ function)</a>
</li>
      <li><a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext.html#_CPPv4NK6nvidia15inferenceserver6client12InferContext9GetOutputERKNSt6stringEPNSt10shared_ptrI6OutputEE">nvidia::inferenceserver::client::InferContext::GetOutput (C++ function)</a>
</li>
      <li><a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext.html#_CPPv4N6nvidia15inferenceserver6client12InferContext20GetReadyAsyncRequestEPNSt10shared_ptrI7RequestEEb">nvidia::inferenceserver::client::InferContext::GetReadyAsyncRequest (C++ function)</a>
</li>
      <li><a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext.html#_CPPv4N6nvidia15inferenceserver6client12InferContext7GetStatEP4Stat">nvidia::inferenceserver::client::InferContext::GetStat (C++ function)</a>
</li>
      <li><a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext.html#_CPPv4N6nvidia15inferenceserver6client12InferContext14infer_request_E">nvidia::inferenceserver::client::InferContext::infer_request_ (C++ member)</a>
</li>
      <li><a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext.html#_CPPv4N6nvidia15inferenceserver6client12InferContext12InferContextERKNSt6stringE7int64_t13CorrelationIDb">nvidia::inferenceserver::client::InferContext::InferContext (C++ function)</a>
</li>
      <li><a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext.html#_CPPv4N6nvidia15inferenceserver6client12InferContext5InputE">nvidia::inferenceserver::client::InferContext::Input (C++ class)</a>, <a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Input.html#_CPPv4N6nvidia15inferenceserver6client12InferContext5InputE">[1]</a>
</li>
      <li><a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext.html#_CPPv4NK6nvidia15inferenceserver6client12InferContext5Input8ByteSizeEv">nvidia::inferenceserver::client::InferContext::Input::ByteSize (C++ function)</a>, <a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Input.html#_CPPv4NK6nvidia15inferenceserver6client12InferContext5Input8ByteSizeEv">[1]</a>
</li>
      <li><a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext.html#_CPPv4NK6nvidia15inferenceserver6client12InferContext5Input4DimsEv">nvidia::inferenceserver::client::InferContext::Input::Dims (C++ function)</a>, <a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Input.html#_CPPv4NK6nvidia15inferenceserver6client12InferContext5Input4DimsEv">[1]</a>
</li>
      <li><a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext.html#_CPPv4NK6nvidia15inferenceserver6client12InferContext5Input5DTypeEv">nvidia::inferenceserver::client::InferContext::Input::DType (C++ function)</a>, <a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Input.html#_CPPv4NK6nvidia15inferenceserver6client12InferContext5Input5DTypeEv">[1]</a>
</li>
      <li><a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext.html#_CPPv4NK6nvidia15inferenceserver6client12InferContext5Input6FormatEv">nvidia::inferenceserver::client::InferContext::Input::Format (C++ function)</a>, <a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Input.html#_CPPv4NK6nvidia15inferenceserver6client12InferContext5Input6FormatEv">[1]</a>
</li>
      <li><a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext.html#_CPPv4NK6nvidia15inferenceserver6client12InferContext5Input4NameEv">nvidia::inferenceserver::client::InferContext::Input::Name (C++ function)</a>, <a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Input.html#_CPPv4NK6nvidia15inferenceserver6client12InferContext5Input4NameEv">[1]</a>
</li>
      <li><a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext.html#_CPPv4N6nvidia15inferenceserver6client12InferContext5Input5ResetEv">nvidia::inferenceserver::client::InferContext::Input::Reset (C++ function)</a>, <a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Input.html#_CPPv4N6nvidia15inferenceserver6client12InferContext5Input5ResetEv">[1]</a>
</li>
      <li><a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext.html#_CPPv4N6nvidia15inferenceserver6client12InferContext5Input13SetFromStringERKNSt6vectorINSt6stringEEE">nvidia::inferenceserver::client::InferContext::Input::SetFromString (C++ function)</a>, <a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Input.html#_CPPv4N6nvidia15inferenceserver6client12InferContext5Input13SetFromStringERKNSt6vectorINSt6stringEEE">[1]</a>
</li>
      <li><a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext.html#_CPPv4N6nvidia15inferenceserver6client12InferContext5Input6SetRawEPK7uint8_t6size_t">nvidia::inferenceserver::client::InferContext::Input::SetRaw (C++ function)</a>, <a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext.html#_CPPv4N6nvidia15inferenceserver6client12InferContext5Input6SetRawERKNSt6vectorI7uint8_tEE">[1]</a>, <a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Input.html#_CPPv4N6nvidia15inferenceserver6client12InferContext5Input6SetRawEPK7uint8_t6size_t">[2]</a>, <a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Input.html#_CPPv4N6nvidia15inferenceserver6client12InferContext5Input6SetRawERKNSt6vectorI7uint8_tEE">[3]</a>
</li>
      <li><a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext.html#_CPPv4N6nvidia15inferenceserver6client12InferContext5Input8SetShapeERKNSt6vectorI7int64_tEE">nvidia::inferenceserver::client::InferContext::Input::SetShape (C++ function)</a>, <a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Input.html#_CPPv4N6nvidia15inferenceserver6client12InferContext5Input8SetShapeERKNSt6vectorI7int64_tEE">[1]</a>
</li>
      <li><a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext.html#_CPPv4NK6nvidia15inferenceserver6client12InferContext5Input5ShapeEv">nvidia::inferenceserver::client::InferContext::Input::Shape (C++ function)</a>, <a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Input.html#_CPPv4NK6nvidia15inferenceserver6client12InferContext5Input5ShapeEv">[1]</a>
</li>
      <li><a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext.html#_CPPv4NK6nvidia15inferenceserver6client12InferContext5Input13TotalByteSizeEv">nvidia::inferenceserver::client::InferContext::Input::TotalByteSize (C++ function)</a>, <a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Input.html#_CPPv4NK6nvidia15inferenceserver6client12InferContext5Input13TotalByteSizeEv">[1]</a>
</li>
      <li><a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext.html#_CPPv4N6nvidia15inferenceserver6client12InferContext5InputD0Ev">nvidia::inferenceserver::client::InferContext::Input::~Input (C++ function)</a>, <a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Input.html#_CPPv4N6nvidia15inferenceserver6client12InferContext5InputD0Ev">[1]</a>
</li>
      <li><a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext.html#_CPPv4NK6nvidia15inferenceserver6client12InferContext6InputsEv">nvidia::inferenceserver::client::InferContext::Inputs (C++ function)</a>
</li>
      <li><a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext.html#_CPPv4N6nvidia15inferenceserver6client12InferContext7inputs_E">nvidia::inferenceserver::client::InferContext::inputs_ (C++ member)</a>
</li>
      <li><a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext.html#_CPPv4N6nvidia15inferenceserver6client12InferContext14IsRequestReadyERKNSt10shared_ptrI7RequestEEb">nvidia::inferenceserver::client::InferContext::IsRequestReady (C++ function)</a>
</li>
      <li><a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext.html#_CPPv4N6nvidia15inferenceserver6client12InferContext4KindE">nvidia::inferenceserver::client::InferContext::Kind (C++ type)</a>
</li>
      <li><a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext.html#_CPPv4N6nvidia15inferenceserver6client12InferContext15max_batch_size_E">nvidia::inferenceserver::client::InferContext::max_batch_size_ (C++ member)</a>
</li>
      <li><a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext.html#_CPPv4NK6nvidia15inferenceserver6client12InferContext12MaxBatchSizeEv">nvidia::inferenceserver::client::InferContext::MaxBatchSize (C++ function)</a>
</li>
      <li><a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext.html#_CPPv4N6nvidia15inferenceserver6client12InferContext11model_name_E">nvidia::inferenceserver::client::InferContext::model_name_ (C++ member)</a>
</li>
      <li><a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext.html#_CPPv4N6nvidia15inferenceserver6client12InferContext14model_version_E">nvidia::inferenceserver::client::InferContext::model_version_ (C++ member)</a>
</li>
      <li><a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext.html#_CPPv4NK6nvidia15inferenceserver6client12InferContext9ModelNameEv">nvidia::inferenceserver::client::InferContext::ModelName (C++ function)</a>
</li>
      <li><a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext.html#_CPPv4NK6nvidia15inferenceserver6client12InferContext12ModelVersionEv">nvidia::inferenceserver::client::InferContext::ModelVersion (C++ function)</a>
</li>
      <li><a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext.html#_CPPv4N6nvidia15inferenceserver6client12InferContext6mutex_E">nvidia::inferenceserver::client::InferContext::mutex_ (C++ member)</a>
</li>
      <li><a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext.html#_CPPv4N6nvidia15inferenceserver6client12InferContext23ongoing_async_requests_E">nvidia::inferenceserver::client::InferContext::ongoing_async_requests_ (C++ member)</a>
</li>
      <li><a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext.html#_CPPv4N6nvidia15inferenceserver6client12InferContext7OptionsE">nvidia::inferenceserver::client::InferContext::Options (C++ class)</a>, <a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Options.html#_CPPv4N6nvidia15inferenceserver6client12InferContext7OptionsE">[1]</a>
</li>
      <li><a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext.html#_CPPv4N6nvidia15inferenceserver6client12InferContext7Options14AddClassResultERKNSt10shared_ptrIN12InferContext6OutputEEE8uint64_t">nvidia::inferenceserver::client::InferContext::Options::AddClassResult (C++ function)</a>, <a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Options.html#_CPPv4N6nvidia15inferenceserver6client12InferContext7Options14AddClassResultERKNSt10shared_ptrIN12InferContext6OutputEEE8uint64_t">[1]</a>
</li>
      <li><a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext.html#_CPPv4N6nvidia15inferenceserver6client12InferContext7Options12AddRawResultERKNSt10shared_ptrIN12InferContext6OutputEEE">nvidia::inferenceserver::client::InferContext::Options::AddRawResult (C++ function)</a>, <a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Options.html#_CPPv4N6nvidia15inferenceserver6client12InferContext7Options12AddRawResultERKNSt10shared_ptrIN12InferContext6OutputEEE">[1]</a>
</li>
      <li><a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext.html#_CPPv4NK6nvidia15inferenceserver6client12InferContext7Options9BatchSizeEv">nvidia::inferenceserver::client::InferContext::Options::BatchSize (C++ function)</a>, <a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Options.html#_CPPv4NK6nvidia15inferenceserver6client12InferContext7Options9BatchSizeEv">[1]</a>
</li>
      <li><a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext.html#_CPPv4N6nvidia15inferenceserver6client12InferContext7Options6CreateEPNSt10unique_ptrI7OptionsEE">nvidia::inferenceserver::client::InferContext::Options::Create (C++ function)</a>, <a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Options.html#_CPPv4N6nvidia15inferenceserver6client12InferContext7Options6CreateEPNSt10unique_ptrI7OptionsEE">[1]</a>
</li>
      <li><a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext.html#_CPPv4NK6nvidia15inferenceserver6client12InferContext7Options4FlagEN18InferRequestHeader4FlagE">nvidia::inferenceserver::client::InferContext::Options::Flag (C++ function)</a>, <a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Options.html#_CPPv4NK6nvidia15inferenceserver6client12InferContext7Options4FlagEN18InferRequestHeader4FlagE">[1]</a>
</li>
      <li><a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext.html#_CPPv4NK6nvidia15inferenceserver6client12InferContext7Options5FlagsEv">nvidia::inferenceserver::client::InferContext::Options::Flags (C++ function)</a>, <a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Options.html#_CPPv4NK6nvidia15inferenceserver6client12InferContext7Options5FlagsEv">[1]</a>
</li>
      <li><a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext.html#_CPPv4N6nvidia15inferenceserver6client12InferContext7Options12SetBatchSizeE6size_t">nvidia::inferenceserver::client::InferContext::Options::SetBatchSize (C++ function)</a>, <a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Options.html#_CPPv4N6nvidia15inferenceserver6client12InferContext7Options12SetBatchSizeE6size_t">[1]</a>
</li>
      <li><a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext.html#_CPPv4N6nvidia15inferenceserver6client12InferContext7Options7SetFlagEN18InferRequestHeader4FlagEb">nvidia::inferenceserver::client::InferContext::Options::SetFlag (C++ function)</a>, <a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Options.html#_CPPv4N6nvidia15inferenceserver6client12InferContext7Options7SetFlagEN18InferRequestHeader4FlagEb">[1]</a>
</li>
      <li><a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext.html#_CPPv4N6nvidia15inferenceserver6client12InferContext7Options8SetFlagsE8uint32_t">nvidia::inferenceserver::client::InferContext::Options::SetFlags (C++ function)</a>, <a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Options.html#_CPPv4N6nvidia15inferenceserver6client12InferContext7Options8SetFlagsE8uint32_t">[1]</a>
</li>
      <li><a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext.html#_CPPv4N6nvidia15inferenceserver6client12InferContext7OptionsD0Ev">nvidia::inferenceserver::client::InferContext::Options::~Options (C++ function)</a>, <a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Options.html#_CPPv4N6nvidia15inferenceserver6client12InferContext7OptionsD0Ev">[1]</a>
</li>
      <li><a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext.html#_CPPv4N6nvidia15inferenceserver6client12InferContext6OutputE">nvidia::inferenceserver::client::InferContext::Output (C++ class)</a>, <a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Output.html#_CPPv4N6nvidia15inferenceserver6client12InferContext6OutputE">[1]</a>
</li>
      <li><a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext.html#_CPPv4NK6nvidia15inferenceserver6client12InferContext6Output4DimsEv">nvidia::inferenceserver::client::InferContext::Output::Dims (C++ function)</a>, <a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Output.html#_CPPv4NK6nvidia15inferenceserver6client12InferContext6Output4DimsEv">[1]</a>
</li>
      <li><a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext.html#_CPPv4NK6nvidia15inferenceserver6client12InferContext6Output5DTypeEv">nvidia::inferenceserver::client::InferContext::Output::DType (C++ function)</a>, <a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Output.html#_CPPv4NK6nvidia15inferenceserver6client12InferContext6Output5DTypeEv">[1]</a>
</li>
      <li><a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext.html#_CPPv4NK6nvidia15inferenceserver6client12InferContext6Output4NameEv">nvidia::inferenceserver::client::InferContext::Output::Name (C++ function)</a>, <a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Output.html#_CPPv4NK6nvidia15inferenceserver6client12InferContext6Output4NameEv">[1]</a>
</li>
      <li><a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext.html#_CPPv4N6nvidia15inferenceserver6client12InferContext6OutputD0Ev">nvidia::inferenceserver::client::InferContext::Output::~Output (C++ function)</a>, <a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Output.html#_CPPv4N6nvidia15inferenceserver6client12InferContext6OutputD0Ev">[1]</a>
</li>
      <li><a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext.html#_CPPv4NK6nvidia15inferenceserver6client12InferContext7OutputsEv">nvidia::inferenceserver::client::InferContext::Outputs (C++ function)</a>
</li>
      <li><a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext.html#_CPPv4N6nvidia15inferenceserver6client12InferContext8outputs_E">nvidia::inferenceserver::client::InferContext::outputs_ (C++ member)</a>
</li>
      <li><a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext.html#_CPPv4N6nvidia15inferenceserver6client12InferContext16PreRunProcessingERNSt10shared_ptrI7RequestEE">nvidia::inferenceserver::client::InferContext::PreRunProcessing (C++ function)</a>
</li>
      <li><a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext.html#_CPPv4N6nvidia15inferenceserver6client12InferContext3RAWE">nvidia::inferenceserver::client::InferContext::RAW (C++ enumerator)</a>
</li>
      <li><a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext.html#_CPPv4N6nvidia15inferenceserver6client12InferContext11RECEIVE_ENDE">nvidia::inferenceserver::client::InferContext::RECEIVE_END (C++ enumerator)</a>
</li>
      <li><a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext.html#_CPPv4N6nvidia15inferenceserver6client12InferContext13RECEIVE_STARTE">nvidia::inferenceserver::client::InferContext::RECEIVE_START (C++ enumerator)</a>
</li>
      <li><a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext.html#_CPPv4N6nvidia15inferenceserver6client12InferContext7RequestE">nvidia::inferenceserver::client::InferContext::Request (C++ class)</a>, <a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Request.html#_CPPv4N6nvidia15inferenceserver6client12InferContext7RequestE">[1]</a>
</li>
      <li><a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext.html#_CPPv4NK6nvidia15inferenceserver6client12InferContext7Request2IdEv">nvidia::inferenceserver::client::InferContext::Request::Id (C++ function)</a>, <a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Request.html#_CPPv4NK6nvidia15inferenceserver6client12InferContext7Request2IdEv">[1]</a>
</li>
      <li><a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext.html#_CPPv4N6nvidia15inferenceserver6client12InferContext7RequestD0Ev">nvidia::inferenceserver::client::InferContext::Request::~Request (C++ function)</a>, <a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Request.html#_CPPv4N6nvidia15inferenceserver6client12InferContext7RequestD0Ev">[1]</a>
</li>
      <li><a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext.html#_CPPv4N6nvidia15inferenceserver6client12InferContext11REQUEST_ENDE">nvidia::inferenceserver::client::InferContext::REQUEST_END (C++ enumerator)</a>
</li>
      <li><a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext.html#_CPPv4N6nvidia15inferenceserver6client12InferContext13REQUEST_STARTE">nvidia::inferenceserver::client::InferContext::REQUEST_START (C++ enumerator)</a>
</li>
      <li><a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext.html#_CPPv4N6nvidia15inferenceserver6client12InferContext13RequestTimersE">nvidia::inferenceserver::client::InferContext::RequestTimers (C++ class)</a>, <a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1RequestTimers.html#_CPPv4N6nvidia15inferenceserver6client12InferContext13RequestTimersE">[1]</a>
</li>
      <li><a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1RequestTimers.html#_CPPv4N6nvidia15inferenceserver6client12InferContext13RequestTimers4KindE">nvidia::inferenceserver::client::InferContext::RequestTimers::Kind (C++ type)</a>
</li>
      <li><a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1RequestTimers.html#_CPPv4N6nvidia15inferenceserver6client12InferContext13RequestTimers11RECEIVE_ENDE">nvidia::inferenceserver::client::InferContext::RequestTimers::RECEIVE_END (C++ enumerator)</a>
</li>
      <li><a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1RequestTimers.html#_CPPv4N6nvidia15inferenceserver6client12InferContext13RequestTimers13RECEIVE_STARTE">nvidia::inferenceserver::client::InferContext::RequestTimers::RECEIVE_START (C++ enumerator)</a>
</li>
      <li><a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext.html#_CPPv4N6nvidia15inferenceserver6client12InferContext13RequestTimers6RecordE4Kind">nvidia::inferenceserver::client::InferContext::RequestTimers::Record (C++ function)</a>, <a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1RequestTimers.html#_CPPv4N6nvidia15inferenceserver6client12InferContext13RequestTimers6RecordE4Kind">[1]</a>
</li>
      <li><a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1RequestTimers.html#_CPPv4N6nvidia15inferenceserver6client12InferContext13RequestTimers11REQUEST_ENDE">nvidia::inferenceserver::client::InferContext::RequestTimers::REQUEST_END (C++ enumerator)</a>
</li>
      <li><a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1RequestTimers.html#_CPPv4N6nvidia15inferenceserver6client12InferContext13RequestTimers13REQUEST_STARTE">nvidia::inferenceserver::client::InferContext::RequestTimers::REQUEST_START (C++ enumerator)</a>
</li>
      <li><a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext.html#_CPPv4N6nvidia15inferenceserver6client12InferContext13RequestTimers13RequestTimersEv">nvidia::inferenceserver::client::InferContext::RequestTimers::RequestTimers (C++ function)</a>, <a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1RequestTimers.html#_CPPv4N6nvidia15inferenceserver6client12InferContext13RequestTimers13RequestTimersEv">[1]</a>
</li>
      <li><a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext.html#_CPPv4N6nvidia15inferenceserver6client12InferContext13RequestTimers5ResetEv">nvidia::inferenceserver::client::InferContext::RequestTimers::Reset (C++ function)</a>, <a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1RequestTimers.html#_CPPv4N6nvidia15inferenceserver6client12InferContext13RequestTimers5ResetEv">[1]</a>
</li>
      <li><a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1RequestTimers.html#_CPPv4N6nvidia15inferenceserver6client12InferContext13RequestTimers8SEND_ENDE">nvidia::inferenceserver::client::InferContext::RequestTimers::SEND_END (C++ enumerator)</a>
</li>
      <li><a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1RequestTimers.html#_CPPv4N6nvidia15inferenceserver6client12InferContext13RequestTimers10SEND_STARTE">nvidia::inferenceserver::client::InferContext::RequestTimers::SEND_START (C++ enumerator)</a>
</li>
      <li><a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext.html#_CPPv4N6nvidia15inferenceserver6client12InferContext6ResultE">nvidia::inferenceserver::client::InferContext::Result (C++ class)</a>, <a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result.html#_CPPv4N6nvidia15inferenceserver6client12InferContext6ResultE">[1]</a>
</li>
      <li><a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result.html#_CPPv4N6nvidia15inferenceserver6client12InferContext6Result5CLASSE">nvidia::inferenceserver::client::InferContext::Result::CLASS (C++ enumerator)</a>
</li>
      <li><a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext.html#_CPPv4N6nvidia15inferenceserver6client12InferContext6Result11ClassResultE">nvidia::inferenceserver::client::InferContext::Result::ClassResult (C++ class)</a>, <a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result.html#_CPPv4N6nvidia15inferenceserver6client12InferContext6Result11ClassResultE">[1]</a>, <a href="cpp_api/structnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result_1_1ClassResult.html#_CPPv4N6nvidia15inferenceserver6client12InferContext6Result11ClassResultE">[2]</a>
</li>
      <li><a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext.html#_CPPv4N6nvidia15inferenceserver6client12InferContext6Result11ClassResult3idxE">nvidia::inferenceserver::client::InferContext::Result::ClassResult::idx (C++ member)</a>, <a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result.html#_CPPv4N6nvidia15inferenceserver6client12InferContext6Result11ClassResult3idxE">[1]</a>, <a href="cpp_api/structnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result_1_1ClassResult.html#_CPPv4N6nvidia15inferenceserver6client12InferContext6Result11ClassResult3idxE">[2]</a>
</li>
      <li><a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext.html#_CPPv4N6nvidia15inferenceserver6client12InferContext6Result11ClassResult5labelE">nvidia::inferenceserver::client::InferContext::Result::ClassResult::label (C++ member)</a>, <a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result.html#_CPPv4N6nvidia15inferenceserver6client12InferContext6Result11ClassResult5labelE">[1]</a>, <a href="cpp_api/structnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result_1_1ClassResult.html#_CPPv4N6nvidia15inferenceserver6client12InferContext6Result11ClassResult5labelE">[2]</a>
</li>
      <li><a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext.html#_CPPv4N6nvidia15inferenceserver6client12InferContext6Result11ClassResult5valueE">nvidia::inferenceserver::client::InferContext::Result::ClassResult::value (C++ member)</a>, <a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result.html#_CPPv4N6nvidia15inferenceserver6client12InferContext6Result11ClassResult5valueE">[1]</a>, <a href="cpp_api/structnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result_1_1ClassResult.html#_CPPv4N6nvidia15inferenceserver6client12InferContext6Result11ClassResult5valueE">[2]</a>
</li>
      <li><a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext.html#_CPPv4N6nvidia15inferenceserver6client12InferContext6Result16GetClassAtCursorE6size_tP11ClassResult">nvidia::inferenceserver::client::InferContext::Result::GetClassAtCursor (C++ function)</a>, <a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result.html#_CPPv4N6nvidia15inferenceserver6client12InferContext6Result16GetClassAtCursorE6size_tP11ClassResult">[1]</a>
</li>
      <li><a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext.html#_CPPv4NK6nvidia15inferenceserver6client12InferContext6Result13GetClassCountE6size_tP6size_t">nvidia::inferenceserver::client::InferContext::Result::GetClassCount (C++ function)</a>, <a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result.html#_CPPv4NK6nvidia15inferenceserver6client12InferContext6Result13GetClassCountE6size_tP6size_t">[1]</a>
</li>
      <li><a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext.html#_CPPv4NK6nvidia15inferenceserver6client12InferContext6Result9GetOutputEv">nvidia::inferenceserver::client::InferContext::Result::GetOutput (C++ function)</a>, <a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result.html#_CPPv4NK6nvidia15inferenceserver6client12InferContext6Result9GetOutputEv">[1]</a>
</li>
      <li><a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext.html#_CPPv4NK6nvidia15inferenceserver6client12InferContext6Result6GetRawE6size_tPPKNSt6vectorI7uint8_tEE">nvidia::inferenceserver::client::InferContext::Result::GetRaw (C++ function)</a>, <a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result.html#_CPPv4NK6nvidia15inferenceserver6client12InferContext6Result6GetRawE6size_tPPKNSt6vectorI7uint8_tEE">[1]</a>
</li>
      <li><a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext.html#_CPPv4N6nvidia15inferenceserver6client12InferContext6Result14GetRawAtCursorE6size_tP1T">nvidia::inferenceserver::client::InferContext::Result::GetRawAtCursor (C++ function)</a>, <a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext.html#_CPPv4N6nvidia15inferenceserver6client12InferContext6Result14GetRawAtCursorE6size_tPNSt6stringE">[1]</a>, <a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext.html#_CPPv4N6nvidia15inferenceserver6client12InferContext6Result14GetRawAtCursorE6size_tPPK7uint8_t6size_t">[2]</a>, <a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result.html#_CPPv4N6nvidia15inferenceserver6client12InferContext6Result14GetRawAtCursorE6size_tP1T">[3]</a>, <a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result.html#_CPPv4N6nvidia15inferenceserver6client12InferContext6Result14GetRawAtCursorE6size_tPNSt6stringE">[4]</a>, <a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result.html#_CPPv4N6nvidia15inferenceserver6client12InferContext6Result14GetRawAtCursorE6size_tPPK7uint8_t6size_t">[5]</a>
</li>
      <li><a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext.html#_CPPv4NK6nvidia15inferenceserver6client12InferContext6Result11GetRawShapeEPNSt6vectorI7int64_tEE">nvidia::inferenceserver::client::InferContext::Result::GetRawShape (C++ function)</a>, <a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result.html#_CPPv4NK6nvidia15inferenceserver6client12InferContext6Result11GetRawShapeEPNSt6vectorI7int64_tEE">[1]</a>
</li>
      <li><a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext.html#_CPPv4NK6nvidia15inferenceserver6client12InferContext6Result9ModelNameEv">nvidia::inferenceserver::client::InferContext::Result::ModelName (C++ function)</a>, <a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result.html#_CPPv4NK6nvidia15inferenceserver6client12InferContext6Result9ModelNameEv">[1]</a>
</li>
      <li><a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext.html#_CPPv4NK6nvidia15inferenceserver6client12InferContext6Result12ModelVersionEv">nvidia::inferenceserver::client::InferContext::Result::ModelVersion (C++ function)</a>, <a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result.html#_CPPv4NK6nvidia15inferenceserver6client12InferContext6Result12ModelVersionEv">[1]</a>
</li>
      <li><a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result.html#_CPPv4N6nvidia15inferenceserver6client12InferContext6Result3RAWE">nvidia::inferenceserver::client::InferContext::Result::RAW (C++ enumerator)</a>
</li>
      <li><a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext.html#_CPPv4N6nvidia15inferenceserver6client12InferContext6Result11ResetCursorE6size_t">nvidia::inferenceserver::client::InferContext::Result::ResetCursor (C++ function)</a>, <a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result.html#_CPPv4N6nvidia15inferenceserver6client12InferContext6Result11ResetCursorE6size_t">[1]</a>
</li>
      <li><a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext.html#_CPPv4N6nvidia15inferenceserver6client12InferContext6Result12ResetCursorsEv">nvidia::inferenceserver::client::InferContext::Result::ResetCursors (C++ function)</a>, <a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result.html#_CPPv4N6nvidia15inferenceserver6client12InferContext6Result12ResetCursorsEv">[1]</a>
</li>
      <li><a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result.html#_CPPv4N6nvidia15inferenceserver6client12InferContext6Result12ResultFormatE">nvidia::inferenceserver::client::InferContext::Result::ResultFormat (C++ type)</a>
</li>
      <li><a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext.html#_CPPv4N6nvidia15inferenceserver6client12InferContext6ResultD0Ev">nvidia::inferenceserver::client::InferContext::Result::~Result (C++ function)</a>, <a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result.html#_CPPv4N6nvidia15inferenceserver6client12InferContext6ResultD0Ev">[1]</a>
</li>
      <li><a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext.html#_CPPv4N6nvidia15inferenceserver6client12InferContext12ResultFormatE">nvidia::inferenceserver::client::InferContext::ResultFormat (C++ type)</a>
</li>
      <li><a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext.html#_CPPv4N6nvidia15inferenceserver6client12InferContext9ResultMapE">nvidia::inferenceserver::client::InferContext::ResultMap (C++ type)</a>
</li>
      <li><a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext.html#_CPPv4N6nvidia15inferenceserver6client12InferContext3RunEP9ResultMap">nvidia::inferenceserver::client::InferContext::Run (C++ function)</a>
</li>
      <li><a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext.html#_CPPv4N6nvidia15inferenceserver6client12InferContext8SEND_ENDE">nvidia::inferenceserver::client::InferContext::SEND_END (C++ enumerator)</a>
</li>
      <li><a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext.html#_CPPv4N6nvidia15inferenceserver6client12InferContext10SEND_STARTE">nvidia::inferenceserver::client::InferContext::SEND_START (C++ enumerator)</a>
</li>
      <li><a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext.html#_CPPv4N6nvidia15inferenceserver6client12InferContext13SetRunOptionsERK7Options">nvidia::inferenceserver::client::InferContext::SetRunOptions (C++ function)</a>
</li>
      <li><a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext.html#_CPPv4N6nvidia15inferenceserver6client12InferContext4StatE">nvidia::inferenceserver::client::InferContext::Stat (C++ class)</a>, <a href="cpp_api/structnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Stat.html#_CPPv4N6nvidia15inferenceserver6client12InferContext4StatE">[1]</a>
</li>
      <li><a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext.html#_CPPv4N6nvidia15inferenceserver6client12InferContext4Stat23completed_request_countE">nvidia::inferenceserver::client::InferContext::Stat::completed_request_count (C++ member)</a>, <a href="cpp_api/structnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Stat.html#_CPPv4N6nvidia15inferenceserver6client12InferContext4Stat23completed_request_countE">[1]</a>
</li>
      <li><a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext.html#_CPPv4N6nvidia15inferenceserver6client12InferContext4Stat26cumulative_receive_time_nsE">nvidia::inferenceserver::client::InferContext::Stat::cumulative_receive_time_ns (C++ member)</a>, <a href="cpp_api/structnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Stat.html#_CPPv4N6nvidia15inferenceserver6client12InferContext4Stat26cumulative_receive_time_nsE">[1]</a>
</li>
      <li><a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext.html#_CPPv4N6nvidia15inferenceserver6client12InferContext4Stat23cumulative_send_time_nsE">nvidia::inferenceserver::client::InferContext::Stat::cumulative_send_time_ns (C++ member)</a>, <a href="cpp_api/structnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Stat.html#_CPPv4N6nvidia15inferenceserver6client12InferContext4Stat23cumulative_send_time_nsE">[1]</a>
</li>
      <li><a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext.html#_CPPv4N6nvidia15inferenceserver6client12InferContext4Stat32cumulative_total_request_time_nsE">nvidia::inferenceserver::client::InferContext::Stat::cumulative_total_request_time_ns (C++ member)</a>, <a href="cpp_api/structnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Stat.html#_CPPv4N6nvidia15inferenceserver6client12InferContext4Stat32cumulative_total_request_time_nsE">[1]</a>
</li>
      <li><a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext.html#_CPPv4N6nvidia15inferenceserver6client12InferContext4Stat4StatEv">nvidia::inferenceserver::client::InferContext::Stat::Stat (C++ function)</a>, <a href="cpp_api/structnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Stat.html#_CPPv4N6nvidia15inferenceserver6client12InferContext4Stat4StatEv">[1]</a>
</li>
      <li><a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext.html#_CPPv4N6nvidia15inferenceserver6client12InferContext13sync_request_E">nvidia::inferenceserver::client::InferContext::sync_request_ (C++ member)</a>
</li>
      <li><a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext.html#_CPPv4N6nvidia15inferenceserver6client12InferContext10UpdateStatERK13RequestTimers">nvidia::inferenceserver::client::InferContext::UpdateStat (C++ function)</a>
</li>
      <li><a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext.html#_CPPv4N6nvidia15inferenceserver6client12InferContext8verbose_E">nvidia::inferenceserver::client::InferContext::verbose_ (C++ member)</a>
</li>
      <li><a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext.html#_CPPv4N6nvidia15inferenceserver6client12InferContext7worker_E">nvidia::inferenceserver::client::InferContext::worker_ (C++ member)</a>
</li>
      <li><a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext.html#_CPPv4N6nvidia15inferenceserver6client12InferContextD0Ev">nvidia::inferenceserver::client::InferContext::~InferContext (C++ function)</a>
</li>
      <li><a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferGrpcContext.html#_CPPv4N6nvidia15inferenceserver6client16InferGrpcContextE">nvidia::inferenceserver::client::InferGrpcContext (C++ class)</a>
</li>
      <li><a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferGrpcContext.html#_CPPv4N6nvidia15inferenceserver6client16InferGrpcContext31async_request_completion_queue_E">nvidia::inferenceserver::client::InferGrpcContext::async_request_completion_queue_ (C++ member)</a>
</li>
      <li><a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferGrpcContext.html#_CPPv4N6nvidia15inferenceserver6client16InferGrpcContext8AsyncRunEPNSt10shared_ptrI7RequestEE">nvidia::inferenceserver::client::InferGrpcContext::AsyncRun (C++ function)</a>
</li>
      <li><a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferGrpcContext.html#_CPPv4N6nvidia15inferenceserver6client16InferGrpcContext13AsyncTransferEv">nvidia::inferenceserver::client::InferGrpcContext::AsyncTransfer (C++ function)</a>
</li>
      <li><a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferGrpcContext.html#_CPPv4N6nvidia15inferenceserver6client16InferGrpcContext6CreateEPNSt10unique_ptrI12InferContextEE13CorrelationIDRKNSt6stringERKNSt6stringE7int64_tb">nvidia::inferenceserver::client::InferGrpcContext::Create (C++ function)</a>, <a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferGrpcContext.html#_CPPv4N6nvidia15inferenceserver6client16InferGrpcContext6CreateEPNSt10unique_ptrI12InferContextEERKNSt6stringERKNSt6stringE7int64_tb">[1]</a>
</li>
      <li><a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferGrpcContext.html#_CPPv4N6nvidia15inferenceserver6client16InferGrpcContext18GetAsyncRunResultsEP9ResultMapRKNSt10shared_ptrI7RequestEEb">nvidia::inferenceserver::client::InferGrpcContext::GetAsyncRunResults (C++ function)</a>
</li>
      <li><a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferGrpcContext.html#_CPPv4N6nvidia15inferenceserver6client16InferGrpcContext16InferGrpcContextERKNSt6stringERKNSt6stringE7int64_t13CorrelationIDb">nvidia::inferenceserver::client::InferGrpcContext::InferGrpcContext (C++ function)</a>
</li>
      <li><a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferGrpcContext.html#_CPPv4N6nvidia15inferenceserver6client16InferGrpcContext10InitHelperERKNSt6stringERKNSt6stringEb">nvidia::inferenceserver::client::InferGrpcContext::InitHelper (C++ function)</a>
</li>
      <li><a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferGrpcContext.html#_CPPv4N6nvidia15inferenceserver6client16InferGrpcContext16PreRunProcessingERNSt10shared_ptrI7RequestEE">nvidia::inferenceserver::client::InferGrpcContext::PreRunProcessing (C++ function)</a>
</li>
      <li><a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferGrpcContext.html#_CPPv4N6nvidia15inferenceserver6client16InferGrpcContext8request_E">nvidia::inferenceserver::client::InferGrpcContext::request_ (C++ member)</a>
</li>
      <li><a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferGrpcContext.html#_CPPv4N6nvidia15inferenceserver6client16InferGrpcContext3RunEP9ResultMap">nvidia::inferenceserver::client::InferGrpcContext::Run (C++ function)</a>
</li>
      <li><a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferGrpcContext.html#_CPPv4N6nvidia15inferenceserver6client16InferGrpcContext5stub_E">nvidia::inferenceserver::client::InferGrpcContext::stub_ (C++ member)</a>
</li>
      <li><a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferGrpcContext.html#_CPPv4N6nvidia15inferenceserver6client16InferGrpcContextD0Ev">nvidia::inferenceserver::client::InferGrpcContext::~InferGrpcContext (C++ function)</a>
</li>
      <li><a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferGrpcStreamContext.html#_CPPv4N6nvidia15inferenceserver6client22InferGrpcStreamContextE">nvidia::inferenceserver::client::InferGrpcStreamContext (C++ class)</a>
</li>
      <li><a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferGrpcStreamContext.html#_CPPv4N6nvidia15inferenceserver6client22InferGrpcStreamContext8AsyncRunEPNSt10shared_ptrI7RequestEE">nvidia::inferenceserver::client::InferGrpcStreamContext::AsyncRun (C++ function)</a>
</li>
      <li><a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferGrpcStreamContext.html#_CPPv4N6nvidia15inferenceserver6client22InferGrpcStreamContext6CreateEPNSt10unique_ptrI12InferContextEE13CorrelationIDRKNSt6stringERKNSt6stringE7int64_tb">nvidia::inferenceserver::client::InferGrpcStreamContext::Create (C++ function)</a>, <a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferGrpcStreamContext.html#_CPPv4N6nvidia15inferenceserver6client22InferGrpcStreamContext6CreateEPNSt10unique_ptrI12InferContextEERKNSt6stringERKNSt6stringE7int64_tb">[1]</a>
</li>
      <li><a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferGrpcStreamContext.html#_CPPv4N6nvidia15inferenceserver6client22InferGrpcStreamContext3RunEP9ResultMap">nvidia::inferenceserver::client::InferGrpcStreamContext::Run (C++ function)</a>
</li>
      <li><a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferGrpcStreamContext.html#_CPPv4N6nvidia15inferenceserver6client22InferGrpcStreamContextD0Ev">nvidia::inferenceserver::client::InferGrpcStreamContext::~InferGrpcStreamContext (C++ function)</a>
</li>
      <li><a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferHttpContext.html#_CPPv4N6nvidia15inferenceserver6client16InferHttpContextE">nvidia::inferenceserver::client::InferHttpContext (C++ class)</a>
</li>
      <li><a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferHttpContext.html#_CPPv4N6nvidia15inferenceserver6client16InferHttpContext8AsyncRunEPNSt10shared_ptrI7RequestEE">nvidia::inferenceserver::client::InferHttpContext::AsyncRun (C++ function)</a>
</li>
      <li><a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferHttpContext.html#_CPPv4N6nvidia15inferenceserver6client16InferHttpContext6CreateEPNSt10unique_ptrI12InferContextEE13CorrelationIDRKNSt6stringERKNSt6stringE7int64_tb">nvidia::inferenceserver::client::InferHttpContext::Create (C++ function)</a>, <a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferHttpContext.html#_CPPv4N6nvidia15inferenceserver6client16InferHttpContext6CreateEPNSt10unique_ptrI12InferContextEERKNSt6stringERKNSt6stringE7int64_tb">[1]</a>
</li>
      <li><a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferHttpContext.html#_CPPv4N6nvidia15inferenceserver6client16InferHttpContext18GetAsyncRunResultsEP9ResultMapRKNSt10shared_ptrI7RequestEEb">nvidia::inferenceserver::client::InferHttpContext::GetAsyncRunResults (C++ function)</a>
</li>
      <li><a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferHttpContext.html#_CPPv4N6nvidia15inferenceserver6client16InferHttpContext3RunEP9ResultMap">nvidia::inferenceserver::client::InferHttpContext::Run (C++ function)</a>
</li>
      <li><a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferHttpContext.html#_CPPv4N6nvidia15inferenceserver6client16InferHttpContextD0Ev">nvidia::inferenceserver::client::InferHttpContext::~InferHttpContext (C++ function)</a>
</li>
      <li><a href="cpp_api/function_request_8h_1a19d352bb13848c14b5f03274b25ecee3.html#_CPPv4N6nvidia15inferenceserver6clientlsERNSt7ostreamERK5Error">nvidia::inferenceserver::client::operator&lt;&lt; (C++ function)</a>
</li>
      <li><a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ProfileContext.html#_CPPv4N6nvidia15inferenceserver6client14ProfileContextE">nvidia::inferenceserver::client::ProfileContext (C++ class)</a>
</li>
      <li><a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ProfileContext.html#_CPPv4N6nvidia15inferenceserver6client14ProfileContext14ProfileContextEb">nvidia::inferenceserver::client::ProfileContext::ProfileContext (C++ function)</a>
</li>
      <li><a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ProfileContext.html#_CPPv4N6nvidia15inferenceserver6client14ProfileContext11SendCommandERKNSt6stringE">nvidia::inferenceserver::client::ProfileContext::SendCommand (C++ function)</a>
</li>
      <li><a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ProfileContext.html#_CPPv4N6nvidia15inferenceserver6client14ProfileContext12StartProfileEv">nvidia::inferenceserver::client::ProfileContext::StartProfile (C++ function)</a>
</li>
      <li><a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ProfileContext.html#_CPPv4N6nvidia15inferenceserver6client14ProfileContext11StopProfileEv">nvidia::inferenceserver::client::ProfileContext::StopProfile (C++ function)</a>
</li>
      <li><a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ProfileContext.html#_CPPv4N6nvidia15inferenceserver6client14ProfileContext8verbose_E">nvidia::inferenceserver::client::ProfileContext::verbose_ (C++ member)</a>
</li>
      <li><a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ProfileGrpcContext.html#_CPPv4N6nvidia15inferenceserver6client18ProfileGrpcContextE">nvidia::inferenceserver::client::ProfileGrpcContext (C++ class)</a>
</li>
      <li><a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ProfileGrpcContext.html#_CPPv4N6nvidia15inferenceserver6client18ProfileGrpcContext6CreateEPNSt10unique_ptrI14ProfileContextEERKNSt6stringEb">nvidia::inferenceserver::client::ProfileGrpcContext::Create (C++ function)</a>
</li>
      <li><a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ProfileHttpContext.html#_CPPv4N6nvidia15inferenceserver6client18ProfileHttpContextE">nvidia::inferenceserver::client::ProfileHttpContext (C++ class)</a>
</li>
      <li><a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ProfileHttpContext.html#_CPPv4N6nvidia15inferenceserver6client18ProfileHttpContext6CreateEPNSt10unique_ptrI14ProfileContextEERKNSt6stringEb">nvidia::inferenceserver::client::ProfileHttpContext::Create (C++ function)</a>
</li>
      <li><a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ServerHealthContext.html#_CPPv4N6nvidia15inferenceserver6client19ServerHealthContextE">nvidia::inferenceserver::client::ServerHealthContext (C++ class)</a>
</li>
      <li><a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ServerHealthContext.html#_CPPv4N6nvidia15inferenceserver6client19ServerHealthContext7GetLiveEPb">nvidia::inferenceserver::client::ServerHealthContext::GetLive (C++ function)</a>
</li>
      <li><a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ServerHealthContext.html#_CPPv4N6nvidia15inferenceserver6client19ServerHealthContext8GetReadyEPb">nvidia::inferenceserver::client::ServerHealthContext::GetReady (C++ function)</a>
</li>
      <li><a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ServerHealthContext.html#_CPPv4N6nvidia15inferenceserver6client19ServerHealthContext19ServerHealthContextEb">nvidia::inferenceserver::client::ServerHealthContext::ServerHealthContext (C++ function)</a>
</li>
      <li><a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ServerHealthContext.html#_CPPv4N6nvidia15inferenceserver6client19ServerHealthContext8verbose_E">nvidia::inferenceserver::client::ServerHealthContext::verbose_ (C++ member)</a>
</li>
      <li><a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ServerHealthGrpcContext.html#_CPPv4N6nvidia15inferenceserver6client23ServerHealthGrpcContextE">nvidia::inferenceserver::client::ServerHealthGrpcContext (C++ class)</a>
</li>
      <li><a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ServerHealthGrpcContext.html#_CPPv4N6nvidia15inferenceserver6client23ServerHealthGrpcContext6CreateEPNSt10unique_ptrI19ServerHealthContextEERKNSt6stringEb">nvidia::inferenceserver::client::ServerHealthGrpcContext::Create (C++ function)</a>
</li>
      <li><a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ServerHealthGrpcContext.html#_CPPv4N6nvidia15inferenceserver6client23ServerHealthGrpcContext7GetLiveEPb">nvidia::inferenceserver::client::ServerHealthGrpcContext::GetLive (C++ function)</a>
</li>
      <li><a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ServerHealthGrpcContext.html#_CPPv4N6nvidia15inferenceserver6client23ServerHealthGrpcContext8GetReadyEPb">nvidia::inferenceserver::client::ServerHealthGrpcContext::GetReady (C++ function)</a>
</li>
      <li><a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ServerHealthHttpContext.html#_CPPv4N6nvidia15inferenceserver6client23ServerHealthHttpContextE">nvidia::inferenceserver::client::ServerHealthHttpContext (C++ class)</a>
</li>
      <li><a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ServerHealthHttpContext.html#_CPPv4N6nvidia15inferenceserver6client23ServerHealthHttpContext6CreateEPNSt10unique_ptrI19ServerHealthContextEERKNSt6stringEb">nvidia::inferenceserver::client::ServerHealthHttpContext::Create (C++ function)</a>
</li>
      <li><a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ServerHealthHttpContext.html#_CPPv4N6nvidia15inferenceserver6client23ServerHealthHttpContext7GetLiveEPb">nvidia::inferenceserver::client::ServerHealthHttpContext::GetLive (C++ function)</a>
</li>
      <li><a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ServerHealthHttpContext.html#_CPPv4N6nvidia15inferenceserver6client23ServerHealthHttpContext8GetReadyEPb">nvidia::inferenceserver::client::ServerHealthHttpContext::GetReady (C++ function)</a>
</li>
      <li><a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ServerStatusContext.html#_CPPv4N6nvidia15inferenceserver6client19ServerStatusContextE">nvidia::inferenceserver::client::ServerStatusContext (C++ class)</a>
</li>
      <li><a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ServerStatusContext.html#_CPPv4N6nvidia15inferenceserver6client19ServerStatusContext15GetServerStatusEP12ServerStatus">nvidia::inferenceserver::client::ServerStatusContext::GetServerStatus (C++ function)</a>
</li>
      <li><a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ServerStatusContext.html#_CPPv4N6nvidia15inferenceserver6client19ServerStatusContext19ServerStatusContextEb">nvidia::inferenceserver::client::ServerStatusContext::ServerStatusContext (C++ function)</a>
</li>
      <li><a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ServerStatusContext.html#_CPPv4N6nvidia15inferenceserver6client19ServerStatusContext8verbose_E">nvidia::inferenceserver::client::ServerStatusContext::verbose_ (C++ member)</a>
</li>
      <li><a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ServerStatusGrpcContext.html#_CPPv4N6nvidia15inferenceserver6client23ServerStatusGrpcContextE">nvidia::inferenceserver::client::ServerStatusGrpcContext (C++ class)</a>
</li>
      <li><a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ServerStatusGrpcContext.html#_CPPv4N6nvidia15inferenceserver6client23ServerStatusGrpcContext6CreateEPNSt10unique_ptrI19ServerStatusContextEERKNSt6stringERKNSt6stringEb">nvidia::inferenceserver::client::ServerStatusGrpcContext::Create (C++ function)</a>, <a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ServerStatusGrpcContext.html#_CPPv4N6nvidia15inferenceserver6client23ServerStatusGrpcContext6CreateEPNSt10unique_ptrI19ServerStatusContextEERKNSt6stringEb">[1]</a>
</li>
      <li><a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ServerStatusGrpcContext.html#_CPPv4N6nvidia15inferenceserver6client23ServerStatusGrpcContext15GetServerStatusEP12ServerStatus">nvidia::inferenceserver::client::ServerStatusGrpcContext::GetServerStatus (C++ function)</a>
</li>
      <li><a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ServerStatusHttpContext.html#_CPPv4N6nvidia15inferenceserver6client23ServerStatusHttpContextE">nvidia::inferenceserver::client::ServerStatusHttpContext (C++ class)</a>
</li>
      <li><a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ServerStatusHttpContext.html#_CPPv4N6nvidia15inferenceserver6client23ServerStatusHttpContext6CreateEPNSt10unique_ptrI19ServerStatusContextEERKNSt6stringERKNSt6stringEb">nvidia::inferenceserver::client::ServerStatusHttpContext::Create (C++ function)</a>, <a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ServerStatusHttpContext.html#_CPPv4N6nvidia15inferenceserver6client23ServerStatusHttpContext6CreateEPNSt10unique_ptrI19ServerStatusContextEERKNSt6stringEb">[1]</a>
</li>
      <li><a href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ServerStatusHttpContext.html#_CPPv4N6nvidia15inferenceserver6client23ServerStatusHttpContext15GetServerStatusEP12ServerStatus">nvidia::inferenceserver::client::ServerStatusHttpContext::GetServerStatus (C++ function)</a>
</li>
      <li><a href="protobuf_api/model_config.proto.html#_CPPv4N6nvidia15inferenceserver8DataTypeE">nvidia::inferenceserver::DataType (C++ enum)</a>
</li>
      <li><a href="protobuf_api/model_config.proto.html#_CPPv4N6nvidia15inferenceserver8DataType8DataType4BOOLE">nvidia::inferenceserver::DataType::DataType::BOOL (C++ enumerator)</a>
</li>
      <li><a href="protobuf_api/model_config.proto.html#_CPPv4N6nvidia15inferenceserver8DataType8DataType4FP16E">nvidia::inferenceserver::DataType::DataType::FP16 (C++ enumerator)</a>
</li>
      <li><a href="protobuf_api/model_config.proto.html#_CPPv4N6nvidia15inferenceserver8DataType8DataType4FP32E">nvidia::inferenceserver::DataType::DataType::FP32 (C++ enumerator)</a>
</li>
      <li><a href="protobuf_api/model_config.proto.html#_CPPv4N6nvidia15inferenceserver8DataType8DataType4FP64E">nvidia::inferenceserver::DataType::DataType::FP64 (C++ enumerator)</a>
</li>
      <li><a href="protobuf_api/model_config.proto.html#_CPPv4N6nvidia15inferenceserver8DataType8DataType5INT16E">nvidia::inferenceserver::DataType::DataType::INT16 (C++ enumerator)</a>
</li>
      <li><a href="protobuf_api/model_config.proto.html#_CPPv4N6nvidia15inferenceserver8DataType8DataType5INT32E">nvidia::inferenceserver::DataType::DataType::INT32 (C++ enumerator)</a>
</li>
      <li><a href="protobuf_api/model_config.proto.html#_CPPv4N6nvidia15inferenceserver8DataType8DataType5INT64E">nvidia::inferenceserver::DataType::DataType::INT64 (C++ enumerator)</a>
</li>
      <li><a href="protobuf_api/model_config.proto.html#_CPPv4N6nvidia15inferenceserver8DataType8DataType4INT8E">nvidia::inferenceserver::DataType::DataType::INT8 (C++ enumerator)</a>
</li>
      <li><a href="protobuf_api/model_config.proto.html#_CPPv4N6nvidia15inferenceserver8DataType8DataType7INVALIDE">nvidia::inferenceserver::DataType::DataType::INVALID (C++ enumerator)</a>
</li>
      <li><a href="protobuf_api/model_config.proto.html#_CPPv4N6nvidia15inferenceserver8DataType8DataType6STRINGE">nvidia::inferenceserver::DataType::DataType::STRING (C++ enumerator)</a>
</li>
  </ul></td>
  <td style="width: 33%; vertical-align: top;"><ul>
      <li><a href="protobuf_api/model_config.proto.html#_CPPv4N6nvidia15inferenceserver8DataType8DataType6UINT16E">nvidia::inferenceserver::DataType::DataType::UINT16 (C++ enumerator)</a>
</li>
      <li><a href="protobuf_api/model_config.proto.html#_CPPv4N6nvidia15inferenceserver8DataType8DataType6UINT32E">nvidia::inferenceserver::DataType::DataType::UINT32 (C++ enumerator)</a>
</li>
      <li><a href="protobuf_api/model_config.proto.html#_CPPv4N6nvidia15inferenceserver8DataType8DataType6UINT64E">nvidia::inferenceserver::DataType::DataType::UINT64 (C++ enumerator)</a>
</li>
      <li><a href="protobuf_api/model_config.proto.html#_CPPv4N6nvidia15inferenceserver8DataType8DataType5UINT8E">nvidia::inferenceserver::DataType::DataType::UINT8 (C++ enumerator)</a>
</li>
      <li><a href="protobuf_api/grpc_service.proto.html#_CPPv4N6nvidia15inferenceserver11GRPCServiceE">nvidia::inferenceserver::GRPCService (C++ member)</a>
</li>
      <li><a href="protobuf_api/grpc_service.proto.html#_CPPv4N6nvidia15inferenceserver13HealthRequestE">nvidia::inferenceserver::HealthRequest (C++ member)</a>
</li>
      <li><a href="protobuf_api/grpc_service.proto.html#_CPPv4N6nvidia15inferenceserver13HealthRequest4modeE">nvidia::inferenceserver::HealthRequest::mode (C++ member)</a>
</li>
      <li><a href="protobuf_api/server_status.proto.html#_CPPv4N6nvidia15inferenceserver18HealthRequestStatsE">nvidia::inferenceserver::HealthRequestStats (C++ member)</a>
</li>
      <li><a href="protobuf_api/server_status.proto.html#_CPPv4N6nvidia15inferenceserver18HealthRequestStats7successE">nvidia::inferenceserver::HealthRequestStats::success (C++ member)</a>
</li>
      <li><a href="protobuf_api/grpc_service.proto.html#_CPPv4N6nvidia15inferenceserver14HealthResponseE">nvidia::inferenceserver::HealthResponse (C++ member)</a>
</li>
      <li><a href="protobuf_api/grpc_service.proto.html#_CPPv4N6nvidia15inferenceserver14HealthResponse6healthE">nvidia::inferenceserver::HealthResponse::health (C++ member)</a>
</li>
      <li><a href="protobuf_api/grpc_service.proto.html#_CPPv4N6nvidia15inferenceserver14HealthResponse14request_statusE">nvidia::inferenceserver::HealthResponse::request_status (C++ member)</a>
</li>
      <li><a href="protobuf_api/grpc_service.proto.html#_CPPv4N6nvidia15inferenceserver12InferRequestE">nvidia::inferenceserver::InferRequest (C++ member)</a>
</li>
      <li><a href="protobuf_api/grpc_service.proto.html#_CPPv4N6nvidia15inferenceserver12InferRequest9meta_dataE">nvidia::inferenceserver::InferRequest::meta_data (C++ member)</a>
</li>
      <li><a href="protobuf_api/grpc_service.proto.html#_CPPv4N6nvidia15inferenceserver12InferRequest10model_nameE">nvidia::inferenceserver::InferRequest::model_name (C++ member)</a>
</li>
      <li><a href="protobuf_api/grpc_service.proto.html#_CPPv4N6nvidia15inferenceserver12InferRequest9raw_inputE">nvidia::inferenceserver::InferRequest::raw_input (C++ member)</a>
</li>
      <li><a href="protobuf_api/grpc_service.proto.html#_CPPv4N6nvidia15inferenceserver12InferRequest7versionE">nvidia::inferenceserver::InferRequest::version (C++ member)</a>
</li>
      <li><a href="protobuf_api/api.proto.html#_CPPv4N6nvidia15inferenceserver18InferRequestHeaderE">nvidia::inferenceserver::InferRequestHeader (C++ member)</a>
</li>
      <li><a href="protobuf_api/api.proto.html#_CPPv4N6nvidia15inferenceserver18InferRequestHeader10batch_sizeE">nvidia::inferenceserver::InferRequestHeader::batch_size (C++ member)</a>
</li>
      <li><a href="protobuf_api/api.proto.html#_CPPv4N6nvidia15inferenceserver18InferRequestHeader14correlation_idE">nvidia::inferenceserver::InferRequestHeader::correlation_id (C++ member)</a>
</li>
      <li><a href="protobuf_api/api.proto.html#_CPPv4N6nvidia15inferenceserver18InferRequestHeader4FlagE">nvidia::inferenceserver::InferRequestHeader::Flag (C++ enum)</a>
</li>
      <li><a href="protobuf_api/api.proto.html#_CPPv4N6nvidia15inferenceserver18InferRequestHeader4Flag4Flag9FLAG_NONEE">nvidia::inferenceserver::InferRequestHeader::Flag::Flag::FLAG_NONE (C++ enumerator)</a>
</li>
      <li><a href="protobuf_api/api.proto.html#_CPPv4N6nvidia15inferenceserver18InferRequestHeader4Flag4Flag17FLAG_SEQUENCE_ENDE">nvidia::inferenceserver::InferRequestHeader::Flag::Flag::FLAG_SEQUENCE_END (C++ enumerator)</a>
</li>
      <li><a href="protobuf_api/api.proto.html#_CPPv4N6nvidia15inferenceserver18InferRequestHeader5flagsE">nvidia::inferenceserver::InferRequestHeader::flags (C++ member)</a>
</li>
      <li><a href="protobuf_api/api.proto.html#_CPPv4N6nvidia15inferenceserver18InferRequestHeader2idE">nvidia::inferenceserver::InferRequestHeader::id (C++ member)</a>
</li>
      <li><a href="protobuf_api/api.proto.html#_CPPv4N6nvidia15inferenceserver18InferRequestHeader5InputE">nvidia::inferenceserver::InferRequestHeader::Input (C++ member)</a>
</li>
      <li><a href="protobuf_api/api.proto.html#_CPPv4N6nvidia15inferenceserver18InferRequestHeader5inputE">nvidia::inferenceserver::InferRequestHeader::input (C++ member)</a>
</li>
      <li><a href="protobuf_api/api.proto.html#_CPPv4N6nvidia15inferenceserver18InferRequestHeader5Input15batch_byte_sizeE">nvidia::inferenceserver::InferRequestHeader::Input::batch_byte_size (C++ member)</a>
</li>
      <li><a href="protobuf_api/api.proto.html#_CPPv4N6nvidia15inferenceserver18InferRequestHeader5Input4dimsE">nvidia::inferenceserver::InferRequestHeader::Input::dims (C++ member)</a>
</li>
      <li><a href="protobuf_api/api.proto.html#_CPPv4N6nvidia15inferenceserver18InferRequestHeader5Input4nameE">nvidia::inferenceserver::InferRequestHeader::Input::name (C++ member)</a>
</li>
      <li><a href="protobuf_api/api.proto.html#_CPPv4N6nvidia15inferenceserver18InferRequestHeader6OutputE">nvidia::inferenceserver::InferRequestHeader::Output (C++ member)</a>
</li>
      <li><a href="protobuf_api/api.proto.html#_CPPv4N6nvidia15inferenceserver18InferRequestHeader6outputE">nvidia::inferenceserver::InferRequestHeader::output (C++ member)</a>
</li>
      <li><a href="protobuf_api/api.proto.html#_CPPv4N6nvidia15inferenceserver18InferRequestHeader6Output5ClassE">nvidia::inferenceserver::InferRequestHeader::Output::Class (C++ member)</a>
</li>
      <li><a href="protobuf_api/api.proto.html#_CPPv4N6nvidia15inferenceserver18InferRequestHeader6Output5Class5countE">nvidia::inferenceserver::InferRequestHeader::Output::Class::count (C++ member)</a>
</li>
      <li><a href="protobuf_api/api.proto.html#_CPPv4N6nvidia15inferenceserver18InferRequestHeader6Output3clsE">nvidia::inferenceserver::InferRequestHeader::Output::cls (C++ member)</a>
</li>
      <li><a href="protobuf_api/api.proto.html#_CPPv4N6nvidia15inferenceserver18InferRequestHeader6Output4nameE">nvidia::inferenceserver::InferRequestHeader::Output::name (C++ member)</a>
</li>
      <li><a href="protobuf_api/server_status.proto.html#_CPPv4N6nvidia15inferenceserver17InferRequestStatsE">nvidia::inferenceserver::InferRequestStats (C++ member)</a>
</li>
      <li><a href="protobuf_api/server_status.proto.html#_CPPv4N6nvidia15inferenceserver17InferRequestStats7computeE">nvidia::inferenceserver::InferRequestStats::compute (C++ member)</a>
</li>
      <li><a href="protobuf_api/server_status.proto.html#_CPPv4N6nvidia15inferenceserver17InferRequestStats6failedE">nvidia::inferenceserver::InferRequestStats::failed (C++ member)</a>
</li>
      <li><a href="protobuf_api/server_status.proto.html#_CPPv4N6nvidia15inferenceserver17InferRequestStats5queueE">nvidia::inferenceserver::InferRequestStats::queue (C++ member)</a>
</li>
      <li><a href="protobuf_api/server_status.proto.html#_CPPv4N6nvidia15inferenceserver17InferRequestStats7successE">nvidia::inferenceserver::InferRequestStats::success (C++ member)</a>
</li>
      <li><a href="protobuf_api/grpc_service.proto.html#_CPPv4N6nvidia15inferenceserver13InferResponseE">nvidia::inferenceserver::InferResponse (C++ member)</a>
</li>
      <li><a href="protobuf_api/grpc_service.proto.html#_CPPv4N6nvidia15inferenceserver13InferResponse9meta_dataE">nvidia::inferenceserver::InferResponse::meta_data (C++ member)</a>
</li>
      <li><a href="protobuf_api/grpc_service.proto.html#_CPPv4N6nvidia15inferenceserver13InferResponse10raw_outputE">nvidia::inferenceserver::InferResponse::raw_output (C++ member)</a>
</li>
      <li><a href="protobuf_api/grpc_service.proto.html#_CPPv4N6nvidia15inferenceserver13InferResponse14request_statusE">nvidia::inferenceserver::InferResponse::request_status (C++ member)</a>
</li>
      <li><a href="protobuf_api/api.proto.html#_CPPv4N6nvidia15inferenceserver19InferResponseHeaderE">nvidia::inferenceserver::InferResponseHeader (C++ member)</a>
</li>
      <li><a href="protobuf_api/api.proto.html#_CPPv4N6nvidia15inferenceserver19InferResponseHeader10batch_sizeE">nvidia::inferenceserver::InferResponseHeader::batch_size (C++ member)</a>
</li>
      <li><a href="protobuf_api/api.proto.html#_CPPv4N6nvidia15inferenceserver19InferResponseHeader2idE">nvidia::inferenceserver::InferResponseHeader::id (C++ member)</a>
</li>
      <li><a href="protobuf_api/api.proto.html#_CPPv4N6nvidia15inferenceserver19InferResponseHeader10model_nameE">nvidia::inferenceserver::InferResponseHeader::model_name (C++ member)</a>
</li>
      <li><a href="protobuf_api/api.proto.html#_CPPv4N6nvidia15inferenceserver19InferResponseHeader13model_versionE">nvidia::inferenceserver::InferResponseHeader::model_version (C++ member)</a>
</li>
      <li><a href="protobuf_api/api.proto.html#_CPPv4N6nvidia15inferenceserver19InferResponseHeader6OutputE">nvidia::inferenceserver::InferResponseHeader::Output (C++ member)</a>
</li>
      <li><a href="protobuf_api/api.proto.html#_CPPv4N6nvidia15inferenceserver19InferResponseHeader6outputE">nvidia::inferenceserver::InferResponseHeader::output (C++ member)</a>
</li>
      <li><a href="protobuf_api/api.proto.html#_CPPv4N6nvidia15inferenceserver19InferResponseHeader6Output13batch_classesE">nvidia::inferenceserver::InferResponseHeader::Output::batch_classes (C++ member)</a>
</li>
      <li><a href="protobuf_api/api.proto.html#_CPPv4N6nvidia15inferenceserver19InferResponseHeader6Output5ClassE">nvidia::inferenceserver::InferResponseHeader::Output::Class (C++ member)</a>
</li>
      <li><a href="protobuf_api/api.proto.html#_CPPv4N6nvidia15inferenceserver19InferResponseHeader6Output5Class3idxE">nvidia::inferenceserver::InferResponseHeader::Output::Class::idx (C++ member)</a>
</li>
      <li><a href="protobuf_api/api.proto.html#_CPPv4N6nvidia15inferenceserver19InferResponseHeader6Output5Class5labelE">nvidia::inferenceserver::InferResponseHeader::Output::Class::label (C++ member)</a>
</li>
      <li><a href="protobuf_api/api.proto.html#_CPPv4N6nvidia15inferenceserver19InferResponseHeader6Output5Class5valueE">nvidia::inferenceserver::InferResponseHeader::Output::Class::value (C++ member)</a>
</li>
      <li><a href="protobuf_api/api.proto.html#_CPPv4N6nvidia15inferenceserver19InferResponseHeader6Output7ClassesE">nvidia::inferenceserver::InferResponseHeader::Output::Classes (C++ member)</a>
</li>
      <li><a href="protobuf_api/api.proto.html#_CPPv4N6nvidia15inferenceserver19InferResponseHeader6Output7Classes3clsE">nvidia::inferenceserver::InferResponseHeader::Output::Classes::cls (C++ member)</a>
</li>
      <li><a href="protobuf_api/api.proto.html#_CPPv4N6nvidia15inferenceserver19InferResponseHeader6Output4nameE">nvidia::inferenceserver::InferResponseHeader::Output::name (C++ member)</a>
</li>
      <li><a href="protobuf_api/api.proto.html#_CPPv4N6nvidia15inferenceserver19InferResponseHeader6Output3RawE">nvidia::inferenceserver::InferResponseHeader::Output::Raw (C++ member)</a>
</li>
      <li><a href="protobuf_api/api.proto.html#_CPPv4N6nvidia15inferenceserver19InferResponseHeader6Output3rawE">nvidia::inferenceserver::InferResponseHeader::Output::raw (C++ member)</a>
</li>
      <li><a href="protobuf_api/api.proto.html#_CPPv4N6nvidia15inferenceserver19InferResponseHeader6Output3Raw15batch_byte_sizeE">nvidia::inferenceserver::InferResponseHeader::Output::Raw::batch_byte_size (C++ member)</a>
</li>
      <li><a href="protobuf_api/api.proto.html#_CPPv4N6nvidia15inferenceserver19InferResponseHeader6Output3Raw4dimsE">nvidia::inferenceserver::InferResponseHeader::Output::Raw::dims (C++ member)</a>
</li>
      <li><a href="protobuf_api/model_config.proto.html#_CPPv4N6nvidia15inferenceserver11ModelConfigE">nvidia::inferenceserver::ModelConfig (C++ member)</a>
</li>
      <li><a href="protobuf_api/model_config.proto.html#_CPPv4N6nvidia15inferenceserver11ModelConfig18cc_model_filenamesE">nvidia::inferenceserver::ModelConfig::cc_model_filenames (C++ member)</a>
</li>
      <li><a href="protobuf_api/model_config.proto.html#_CPPv4N6nvidia15inferenceserver11ModelConfig22default_model_filenameE">nvidia::inferenceserver::ModelConfig::default_model_filename (C++ member)</a>
</li>
      <li><a href="protobuf_api/model_config.proto.html#_CPPv4N6nvidia15inferenceserver11ModelConfig5inputE">nvidia::inferenceserver::ModelConfig::input (C++ member)</a>
</li>
      <li><a href="protobuf_api/model_config.proto.html#_CPPv4N6nvidia15inferenceserver11ModelConfig14instance_groupE">nvidia::inferenceserver::ModelConfig::instance_group (C++ member)</a>
</li>
      <li><a href="protobuf_api/model_config.proto.html#_CPPv4N6nvidia15inferenceserver11ModelConfig14max_batch_sizeE">nvidia::inferenceserver::ModelConfig::max_batch_size (C++ member)</a>
</li>
      <li><a href="protobuf_api/model_config.proto.html#_CPPv4N6nvidia15inferenceserver11ModelConfig4nameE">nvidia::inferenceserver::ModelConfig::name (C++ member)</a>
</li>
      <li><a href="protobuf_api/model_config.proto.html#_CPPv4N6nvidia15inferenceserver11ModelConfig12optimizationE">nvidia::inferenceserver::ModelConfig::optimization (C++ member)</a>
</li>
      <li><a href="protobuf_api/model_config.proto.html#_CPPv4N6nvidia15inferenceserver11ModelConfig6outputE">nvidia::inferenceserver::ModelConfig::output (C++ member)</a>
</li>
      <li><a href="protobuf_api/model_config.proto.html#_CPPv4N6nvidia15inferenceserver11ModelConfig8platformE">nvidia::inferenceserver::ModelConfig::platform (C++ member)</a>
</li>
      <li><a href="protobuf_api/model_config.proto.html#_CPPv4N6nvidia15inferenceserver11ModelConfig17scheduling_choiceE">nvidia::inferenceserver::ModelConfig::scheduling_choice (C++ member)</a>
</li>
      <li><a href="protobuf_api/model_config.proto.html#_CPPv4N6nvidia15inferenceserver11ModelConfig17scheduling_choice16dynamic_batchingE">nvidia::inferenceserver::ModelConfig::scheduling_choice::dynamic_batching (C++ member)</a>
</li>
      <li><a href="protobuf_api/model_config.proto.html#_CPPv4N6nvidia15inferenceserver11ModelConfig17scheduling_choice17sequence_batchingE">nvidia::inferenceserver::ModelConfig::scheduling_choice::sequence_batching (C++ member)</a>
</li>
      <li><a href="protobuf_api/model_config.proto.html#_CPPv4N6nvidia15inferenceserver11ModelConfig4tagsE">nvidia::inferenceserver::ModelConfig::tags (C++ member)</a>
</li>
      <li><a href="protobuf_api/model_config.proto.html#_CPPv4N6nvidia15inferenceserver11ModelConfig14version_policyE">nvidia::inferenceserver::ModelConfig::version_policy (C++ member)</a>
</li>
      <li><a href="protobuf_api/model_config.proto.html#_CPPv4N6nvidia15inferenceserver20ModelDynamicBatchingE">nvidia::inferenceserver::ModelDynamicBatching (C++ member)</a>
</li>
      <li><a href="protobuf_api/model_config.proto.html#_CPPv4N6nvidia15inferenceserver20ModelDynamicBatching28max_queue_delay_microsecondsE">nvidia::inferenceserver::ModelDynamicBatching::max_queue_delay_microseconds (C++ member)</a>
</li>
      <li><a href="protobuf_api/model_config.proto.html#_CPPv4N6nvidia15inferenceserver20ModelDynamicBatching20preferred_batch_sizeE">nvidia::inferenceserver::ModelDynamicBatching::preferred_batch_size (C++ member)</a>
</li>
      <li><a href="protobuf_api/model_config.proto.html#_CPPv4N6nvidia15inferenceserver10ModelInputE">nvidia::inferenceserver::ModelInput (C++ member)</a>
</li>
      <li><a href="protobuf_api/model_config.proto.html#_CPPv4N6nvidia15inferenceserver10ModelInput9data_typeE">nvidia::inferenceserver::ModelInput::data_type (C++ member)</a>
</li>
      <li><a href="protobuf_api/model_config.proto.html#_CPPv4N6nvidia15inferenceserver10ModelInput4dimsE">nvidia::inferenceserver::ModelInput::dims (C++ member)</a>
</li>
      <li><a href="protobuf_api/model_config.proto.html#_CPPv4N6nvidia15inferenceserver10ModelInput6FormatE">nvidia::inferenceserver::ModelInput::Format (C++ enum)</a>
</li>
      <li><a href="protobuf_api/model_config.proto.html#_CPPv4N6nvidia15inferenceserver10ModelInput6formatE">nvidia::inferenceserver::ModelInput::format (C++ member)</a>
</li>
      <li><a href="protobuf_api/model_config.proto.html#_CPPv4N6nvidia15inferenceserver10ModelInput6Format6Format11FORMAT_NCHWE">nvidia::inferenceserver::ModelInput::Format::Format::FORMAT_NCHW (C++ enumerator)</a>
</li>
      <li><a href="protobuf_api/model_config.proto.html#_CPPv4N6nvidia15inferenceserver10ModelInput6Format6Format11FORMAT_NHWCE">nvidia::inferenceserver::ModelInput::Format::Format::FORMAT_NHWC (C++ enumerator)</a>
</li>
      <li><a href="protobuf_api/model_config.proto.html#_CPPv4N6nvidia15inferenceserver10ModelInput6Format6Format11FORMAT_NONEE">nvidia::inferenceserver::ModelInput::Format::Format::FORMAT_NONE (C++ enumerator)</a>
</li>
      <li><a href="protobuf_api/model_config.proto.html#_CPPv4N6nvidia15inferenceserver10ModelInput4nameE">nvidia::inferenceserver::ModelInput::name (C++ member)</a>
</li>
      <li><a href="protobuf_api/model_config.proto.html#_CPPv4N6nvidia15inferenceserver18ModelInstanceGroupE">nvidia::inferenceserver::ModelInstanceGroup (C++ member)</a>
</li>
      <li><a href="protobuf_api/model_config.proto.html#_CPPv4N6nvidia15inferenceserver18ModelInstanceGroup5countE">nvidia::inferenceserver::ModelInstanceGroup::count (C++ member)</a>
</li>
      <li><a href="protobuf_api/model_config.proto.html#_CPPv4N6nvidia15inferenceserver18ModelInstanceGroup4gpusE">nvidia::inferenceserver::ModelInstanceGroup::gpus (C++ member)</a>
</li>
      <li><a href="protobuf_api/model_config.proto.html#_CPPv4N6nvidia15inferenceserver18ModelInstanceGroup4KindE">nvidia::inferenceserver::ModelInstanceGroup::Kind (C++ enum)</a>
</li>
      <li><a href="protobuf_api/model_config.proto.html#_CPPv4N6nvidia15inferenceserver18ModelInstanceGroup4kindE">nvidia::inferenceserver::ModelInstanceGroup::kind (C++ member)</a>
</li>
      <li><a href="protobuf_api/model_config.proto.html#_CPPv4N6nvidia15inferenceserver18ModelInstanceGroup4Kind4Kind9KIND_AUTOE">nvidia::inferenceserver::ModelInstanceGroup::Kind::Kind::KIND_AUTO (C++ enumerator)</a>
</li>
      <li><a href="protobuf_api/model_config.proto.html#_CPPv4N6nvidia15inferenceserver18ModelInstanceGroup4Kind4Kind8KIND_CPUE">nvidia::inferenceserver::ModelInstanceGroup::Kind::Kind::KIND_CPU (C++ enumerator)</a>
</li>
      <li><a href="protobuf_api/model_config.proto.html#_CPPv4N6nvidia15inferenceserver18ModelInstanceGroup4Kind4Kind8KIND_GPUE">nvidia::inferenceserver::ModelInstanceGroup::Kind::Kind::KIND_GPU (C++ enumerator)</a>
</li>
      <li><a href="protobuf_api/model_config.proto.html#_CPPv4N6nvidia15inferenceserver18ModelInstanceGroup4nameE">nvidia::inferenceserver::ModelInstanceGroup::name (C++ member)</a>
</li>
      <li><a href="protobuf_api/model_config.proto.html#_CPPv4N6nvidia15inferenceserver23ModelOptimizationPolicyE">nvidia::inferenceserver::ModelOptimizationPolicy (C++ member)</a>
</li>
      <li><a href="protobuf_api/model_config.proto.html#_CPPv4N6nvidia15inferenceserver23ModelOptimizationPolicy5GraphE">nvidia::inferenceserver::ModelOptimizationPolicy::Graph (C++ member)</a>
</li>
      <li><a href="protobuf_api/model_config.proto.html#_CPPv4N6nvidia15inferenceserver23ModelOptimizationPolicy5graphE">nvidia::inferenceserver::ModelOptimizationPolicy::graph (C++ member)</a>
</li>
      <li><a href="protobuf_api/model_config.proto.html#_CPPv4N6nvidia15inferenceserver23ModelOptimizationPolicy5Graph5levelE">nvidia::inferenceserver::ModelOptimizationPolicy::Graph::level (C++ member)</a>
</li>
      <li><a href="protobuf_api/model_config.proto.html#_CPPv4N6nvidia15inferenceserver23ModelOptimizationPolicy13ModelPriorityE">nvidia::inferenceserver::ModelOptimizationPolicy::ModelPriority (C++ enum)</a>
</li>
      <li><a href="protobuf_api/model_config.proto.html#_CPPv4N6nvidia15inferenceserver23ModelOptimizationPolicy13ModelPriority13ModelPriority16PRIORITY_DEFAULTE">nvidia::inferenceserver::ModelOptimizationPolicy::ModelPriority::ModelPriority::PRIORITY_DEFAULT (C++ enumerator)</a>
</li>
      <li><a href="protobuf_api/model_config.proto.html#_CPPv4N6nvidia15inferenceserver23ModelOptimizationPolicy13ModelPriority13ModelPriority12PRIORITY_MAXE">nvidia::inferenceserver::ModelOptimizationPolicy::ModelPriority::ModelPriority::PRIORITY_MAX (C++ enumerator)</a>
</li>
      <li><a href="protobuf_api/model_config.proto.html#_CPPv4N6nvidia15inferenceserver23ModelOptimizationPolicy13ModelPriority13ModelPriority12PRIORITY_MINE">nvidia::inferenceserver::ModelOptimizationPolicy::ModelPriority::ModelPriority::PRIORITY_MIN (C++ enumerator)</a>
</li>
      <li><a href="protobuf_api/model_config.proto.html#_CPPv4N6nvidia15inferenceserver23ModelOptimizationPolicy8priorityE">nvidia::inferenceserver::ModelOptimizationPolicy::priority (C++ member)</a>
</li>
      <li><a href="protobuf_api/model_config.proto.html#_CPPv4N6nvidia15inferenceserver11ModelOutputE">nvidia::inferenceserver::ModelOutput (C++ member)</a>
</li>
      <li><a href="protobuf_api/model_config.proto.html#_CPPv4N6nvidia15inferenceserver11ModelOutput9data_typeE">nvidia::inferenceserver::ModelOutput::data_type (C++ member)</a>
</li>
      <li><a href="protobuf_api/model_config.proto.html#_CPPv4N6nvidia15inferenceserver11ModelOutput4dimsE">nvidia::inferenceserver::ModelOutput::dims (C++ member)</a>
</li>
      <li><a href="protobuf_api/model_config.proto.html#_CPPv4N6nvidia15inferenceserver11ModelOutput14label_filenameE">nvidia::inferenceserver::ModelOutput::label_filename (C++ member)</a>
</li>
      <li><a href="protobuf_api/model_config.proto.html#_CPPv4N6nvidia15inferenceserver11ModelOutput4nameE">nvidia::inferenceserver::ModelOutput::name (C++ member)</a>
</li>
      <li><a href="protobuf_api/server_status.proto.html#_CPPv4N6nvidia15inferenceserver15ModelReadyStateE">nvidia::inferenceserver::ModelReadyState (C++ enum)</a>
</li>
      <li><a href="protobuf_api/server_status.proto.html#_CPPv4N6nvidia15inferenceserver15ModelReadyState15ModelReadyState13MODEL_LOADINGE">nvidia::inferenceserver::ModelReadyState::ModelReadyState::MODEL_LOADING (C++ enumerator)</a>
</li>
      <li><a href="protobuf_api/server_status.proto.html#_CPPv4N6nvidia15inferenceserver15ModelReadyState15ModelReadyState11MODEL_READYE">nvidia::inferenceserver::ModelReadyState::ModelReadyState::MODEL_READY (C++ enumerator)</a>
</li>
      <li><a href="protobuf_api/server_status.proto.html#_CPPv4N6nvidia15inferenceserver15ModelReadyState15ModelReadyState17MODEL_UNAVAILABLEE">nvidia::inferenceserver::ModelReadyState::ModelReadyState::MODEL_UNAVAILABLE (C++ enumerator)</a>
</li>
      <li><a href="protobuf_api/server_status.proto.html#_CPPv4N6nvidia15inferenceserver15ModelReadyState15ModelReadyState13MODEL_UNKNOWNE">nvidia::inferenceserver::ModelReadyState::ModelReadyState::MODEL_UNKNOWN (C++ enumerator)</a>
</li>
      <li><a href="protobuf_api/server_status.proto.html#_CPPv4N6nvidia15inferenceserver15ModelReadyState15ModelReadyState15MODEL_UNLOADINGE">nvidia::inferenceserver::ModelReadyState::ModelReadyState::MODEL_UNLOADING (C++ enumerator)</a>
</li>
      <li><a href="protobuf_api/model_config.proto.html#_CPPv4N6nvidia15inferenceserver21ModelSequenceBatchingE">nvidia::inferenceserver::ModelSequenceBatching (C++ member)</a>
</li>
      <li><a href="protobuf_api/model_config.proto.html#_CPPv4N6nvidia15inferenceserver21ModelSequenceBatching7ControlE">nvidia::inferenceserver::ModelSequenceBatching::Control (C++ member)</a>
</li>
      <li><a href="protobuf_api/model_config.proto.html#_CPPv4N6nvidia15inferenceserver21ModelSequenceBatching7Control16int32_false_trueE">nvidia::inferenceserver::ModelSequenceBatching::Control::int32_false_true (C++ member)</a>
</li>
      <li><a href="protobuf_api/model_config.proto.html#_CPPv4N6nvidia15inferenceserver21ModelSequenceBatching7Control4KindE">nvidia::inferenceserver::ModelSequenceBatching::Control::Kind (C++ enum)</a>
</li>
      <li><a href="protobuf_api/model_config.proto.html#_CPPv4N6nvidia15inferenceserver21ModelSequenceBatching7Control4kindE">nvidia::inferenceserver::ModelSequenceBatching::Control::kind (C++ member)</a>
</li>
      <li><a href="protobuf_api/model_config.proto.html#_CPPv4N6nvidia15inferenceserver21ModelSequenceBatching7Control4Kind4Kind22CONTROL_SEQUENCE_READYE">nvidia::inferenceserver::ModelSequenceBatching::Control::Kind::Kind::CONTROL_SEQUENCE_READY (C++ enumerator)</a>
</li>
      <li><a href="protobuf_api/model_config.proto.html#_CPPv4N6nvidia15inferenceserver21ModelSequenceBatching7Control4Kind4Kind22CONTROL_SEQUENCE_STARTE">nvidia::inferenceserver::ModelSequenceBatching::Control::Kind::Kind::CONTROL_SEQUENCE_START (C++ enumerator)</a>
</li>
      <li><a href="protobuf_api/model_config.proto.html#_CPPv4N6nvidia15inferenceserver21ModelSequenceBatching13control_inputE">nvidia::inferenceserver::ModelSequenceBatching::control_input (C++ member)</a>
</li>
      <li><a href="protobuf_api/model_config.proto.html#_CPPv4N6nvidia15inferenceserver21ModelSequenceBatching12ControlInputE">nvidia::inferenceserver::ModelSequenceBatching::ControlInput (C++ member)</a>
</li>
      <li><a href="protobuf_api/model_config.proto.html#_CPPv4N6nvidia15inferenceserver21ModelSequenceBatching12ControlInput7controlE">nvidia::inferenceserver::ModelSequenceBatching::ControlInput::control (C++ member)</a>
</li>
      <li><a href="protobuf_api/model_config.proto.html#_CPPv4N6nvidia15inferenceserver21ModelSequenceBatching12ControlInput4nameE">nvidia::inferenceserver::ModelSequenceBatching::ControlInput::name (C++ member)</a>
</li>
      <li><a href="protobuf_api/model_config.proto.html#_CPPv4N6nvidia15inferenceserver21ModelSequenceBatching28max_queue_delay_microsecondsE">nvidia::inferenceserver::ModelSequenceBatching::max_queue_delay_microseconds (C++ member)</a>
</li>
      <li><a href="protobuf_api/server_status.proto.html#_CPPv4N6nvidia15inferenceserver11ModelStatusE">nvidia::inferenceserver::ModelStatus (C++ member)</a>
</li>
      <li><a href="protobuf_api/server_status.proto.html#_CPPv4N6nvidia15inferenceserver11ModelStatus6configE">nvidia::inferenceserver::ModelStatus::config (C++ member)</a>
</li>
      <li><a href="protobuf_api/server_status.proto.html#_CPPv4N6nvidia15inferenceserver11ModelStatus14version_statusE">nvidia::inferenceserver::ModelStatus::version_status (C++ member)</a>
</li>
      <li><a href="protobuf_api/model_config.proto.html#_CPPv4N6nvidia15inferenceserver18ModelVersionPolicyE">nvidia::inferenceserver::ModelVersionPolicy (C++ member)</a>
</li>
      <li><a href="protobuf_api/model_config.proto.html#_CPPv4N6nvidia15inferenceserver18ModelVersionPolicy3AllE">nvidia::inferenceserver::ModelVersionPolicy::All (C++ member)</a>
</li>
      <li><a href="protobuf_api/model_config.proto.html#_CPPv4N6nvidia15inferenceserver18ModelVersionPolicy6LatestE">nvidia::inferenceserver::ModelVersionPolicy::Latest (C++ member)</a>
</li>
      <li><a href="protobuf_api/model_config.proto.html#_CPPv4N6nvidia15inferenceserver18ModelVersionPolicy6Latest12num_versionsE">nvidia::inferenceserver::ModelVersionPolicy::Latest::num_versions (C++ member)</a>
</li>
      <li><a href="protobuf_api/model_config.proto.html#_CPPv4N6nvidia15inferenceserver18ModelVersionPolicy13policy_choiceE">nvidia::inferenceserver::ModelVersionPolicy::policy_choice (C++ member)</a>
</li>
      <li><a href="protobuf_api/model_config.proto.html#_CPPv4N6nvidia15inferenceserver18ModelVersionPolicy13policy_choice3allE">nvidia::inferenceserver::ModelVersionPolicy::policy_choice::all (C++ member)</a>
</li>
      <li><a href="protobuf_api/model_config.proto.html#_CPPv4N6nvidia15inferenceserver18ModelVersionPolicy13policy_choice6latestE">nvidia::inferenceserver::ModelVersionPolicy::policy_choice::latest (C++ member)</a>
</li>
      <li><a href="protobuf_api/model_config.proto.html#_CPPv4N6nvidia15inferenceserver18ModelVersionPolicy13policy_choice8specificE">nvidia::inferenceserver::ModelVersionPolicy::policy_choice::specific (C++ member)</a>
</li>
      <li><a href="protobuf_api/model_config.proto.html#_CPPv4N6nvidia15inferenceserver18ModelVersionPolicy8SpecificE">nvidia::inferenceserver::ModelVersionPolicy::Specific (C++ member)</a>
</li>
      <li><a href="protobuf_api/model_config.proto.html#_CPPv4N6nvidia15inferenceserver18ModelVersionPolicy8Specific8versionsE">nvidia::inferenceserver::ModelVersionPolicy::Specific::versions (C++ member)</a>
</li>
      <li><a href="protobuf_api/server_status.proto.html#_CPPv4N6nvidia15inferenceserver18ModelVersionStatusE">nvidia::inferenceserver::ModelVersionStatus (C++ member)</a>
</li>
      <li><a href="protobuf_api/server_status.proto.html#_CPPv4N6nvidia15inferenceserver18ModelVersionStatus11infer_statsE">nvidia::inferenceserver::ModelVersionStatus::infer_stats (C++ member)</a>
</li>
      <li><a href="protobuf_api/server_status.proto.html#_CPPv4N6nvidia15inferenceserver18ModelVersionStatus21model_execution_countE">nvidia::inferenceserver::ModelVersionStatus::model_execution_count (C++ member)</a>
</li>
      <li><a href="protobuf_api/server_status.proto.html#_CPPv4N6nvidia15inferenceserver18ModelVersionStatus21model_inference_countE">nvidia::inferenceserver::ModelVersionStatus::model_inference_count (C++ member)</a>
</li>
      <li><a href="protobuf_api/server_status.proto.html#_CPPv4N6nvidia15inferenceserver18ModelVersionStatus12ready_statueE">nvidia::inferenceserver::ModelVersionStatus::ready_statue (C++ member)</a>
</li>
      <li><a href="protobuf_api/grpc_service.proto.html#_CPPv4N6nvidia15inferenceserver14ProfileRequestE">nvidia::inferenceserver::ProfileRequest (C++ member)</a>
</li>
      <li><a href="protobuf_api/grpc_service.proto.html#_CPPv4N6nvidia15inferenceserver14ProfileRequest3cmdE">nvidia::inferenceserver::ProfileRequest::cmd (C++ member)</a>
</li>
      <li><a href="protobuf_api/server_status.proto.html#_CPPv4N6nvidia15inferenceserver19ProfileRequestStatsE">nvidia::inferenceserver::ProfileRequestStats (C++ member)</a>
</li>
      <li><a href="protobuf_api/server_status.proto.html#_CPPv4N6nvidia15inferenceserver19ProfileRequestStats7successE">nvidia::inferenceserver::ProfileRequestStats::success (C++ member)</a>
</li>
      <li><a href="protobuf_api/grpc_service.proto.html#_CPPv4N6nvidia15inferenceserver15ProfileResponseE">nvidia::inferenceserver::ProfileResponse (C++ member)</a>
</li>
      <li><a href="protobuf_api/grpc_service.proto.html#_CPPv4N6nvidia15inferenceserver15ProfileResponse14request_statusE">nvidia::inferenceserver::ProfileResponse::request_status (C++ member)</a>
</li>
      <li><a href="protobuf_api/request_status.proto.html#_CPPv4N6nvidia15inferenceserver13RequestStatusE">nvidia::inferenceserver::RequestStatus (C++ member)</a>
</li>
      <li><a href="protobuf_api/request_status.proto.html#_CPPv4N6nvidia15inferenceserver13RequestStatus4codeE">nvidia::inferenceserver::RequestStatus::code (C++ member)</a>
</li>
      <li><a href="protobuf_api/request_status.proto.html#_CPPv4N6nvidia15inferenceserver13RequestStatus3msgE">nvidia::inferenceserver::RequestStatus::msg (C++ member)</a>
</li>
      <li><a href="protobuf_api/request_status.proto.html#_CPPv4N6nvidia15inferenceserver13RequestStatus10request_idE">nvidia::inferenceserver::RequestStatus::request_id (C++ member)</a>
</li>
      <li><a href="protobuf_api/request_status.proto.html#_CPPv4N6nvidia15inferenceserver13RequestStatus9server_idE">nvidia::inferenceserver::RequestStatus::server_id (C++ member)</a>
</li>
      <li><a href="protobuf_api/request_status.proto.html#_CPPv4N6nvidia15inferenceserver17RequestStatusCodeE">nvidia::inferenceserver::RequestStatusCode (C++ enum)</a>
</li>
      <li><a href="protobuf_api/request_status.proto.html#_CPPv4N6nvidia15inferenceserver17RequestStatusCode17RequestStatusCode8INTERNALE">nvidia::inferenceserver::RequestStatusCode::RequestStatusCode::INTERNAL (C++ enumerator)</a>
</li>
      <li><a href="protobuf_api/request_status.proto.html#_CPPv4N6nvidia15inferenceserver17RequestStatusCode17RequestStatusCode7INVALIDE">nvidia::inferenceserver::RequestStatusCode::RequestStatusCode::INVALID (C++ enumerator)</a>
</li>
      <li><a href="protobuf_api/request_status.proto.html#_CPPv4N6nvidia15inferenceserver17RequestStatusCode17RequestStatusCode11INVALID_ARGE">nvidia::inferenceserver::RequestStatusCode::RequestStatusCode::INVALID_ARG (C++ enumerator)</a>
</li>
      <li><a href="protobuf_api/request_status.proto.html#_CPPv4N6nvidia15inferenceserver17RequestStatusCode17RequestStatusCode9NOT_FOUNDE">nvidia::inferenceserver::RequestStatusCode::RequestStatusCode::NOT_FOUND (C++ enumerator)</a>
</li>
      <li><a href="protobuf_api/request_status.proto.html#_CPPv4N6nvidia15inferenceserver17RequestStatusCode17RequestStatusCode7SUCCESSE">nvidia::inferenceserver::RequestStatusCode::RequestStatusCode::SUCCESS (C++ enumerator)</a>
</li>
      <li><a href="protobuf_api/request_status.proto.html#_CPPv4N6nvidia15inferenceserver17RequestStatusCode17RequestStatusCode11UNAVAILABLEE">nvidia::inferenceserver::RequestStatusCode::RequestStatusCode::UNAVAILABLE (C++ enumerator)</a>
</li>
      <li><a href="protobuf_api/request_status.proto.html#_CPPv4N6nvidia15inferenceserver17RequestStatusCode17RequestStatusCode7UNKNOWNE">nvidia::inferenceserver::RequestStatusCode::RequestStatusCode::UNKNOWN (C++ enumerator)</a>
</li>
      <li><a href="protobuf_api/request_status.proto.html#_CPPv4N6nvidia15inferenceserver17RequestStatusCode17RequestStatusCode11UNSUPPORTEDE">nvidia::inferenceserver::RequestStatusCode::RequestStatusCode::UNSUPPORTED (C++ enumerator)</a>
</li>
      <li><a href="protobuf_api/server_status.proto.html#_CPPv4N6nvidia15inferenceserver16ServerReadyStateE">nvidia::inferenceserver::ServerReadyState (C++ enum)</a>
</li>
      <li><a href="protobuf_api/server_status.proto.html#_CPPv4N6nvidia15inferenceserver16ServerReadyState16ServerReadyState14SERVER_EXITINGE">nvidia::inferenceserver::ServerReadyState::ServerReadyState::SERVER_EXITING (C++ enumerator)</a>
</li>
      <li><a href="protobuf_api/server_status.proto.html#_CPPv4N6nvidia15inferenceserver16ServerReadyState16ServerReadyState27SERVER_FAILED_TO_INITIALIZEE">nvidia::inferenceserver::ServerReadyState::ServerReadyState::SERVER_FAILED_TO_INITIALIZE (C++ enumerator)</a>
</li>
      <li><a href="protobuf_api/server_status.proto.html#_CPPv4N6nvidia15inferenceserver16ServerReadyState16ServerReadyState19SERVER_INITIALIZINGE">nvidia::inferenceserver::ServerReadyState::ServerReadyState::SERVER_INITIALIZING (C++ enumerator)</a>
</li>
      <li><a href="protobuf_api/server_status.proto.html#_CPPv4N6nvidia15inferenceserver16ServerReadyState16ServerReadyState14SERVER_INVALIDE">nvidia::inferenceserver::ServerReadyState::ServerReadyState::SERVER_INVALID (C++ enumerator)</a>
</li>
      <li><a href="protobuf_api/server_status.proto.html#_CPPv4N6nvidia15inferenceserver16ServerReadyState16ServerReadyState12SERVER_READYE">nvidia::inferenceserver::ServerReadyState::ServerReadyState::SERVER_READY (C++ enumerator)</a>
</li>
      <li><a href="protobuf_api/server_status.proto.html#_CPPv4N6nvidia15inferenceserver12ServerStatusE">nvidia::inferenceserver::ServerStatus (C++ member)</a>
</li>
      <li><a href="protobuf_api/server_status.proto.html#_CPPv4N6nvidia15inferenceserver12ServerStatus12health_statsE">nvidia::inferenceserver::ServerStatus::health_stats (C++ member)</a>
</li>
      <li><a href="protobuf_api/server_status.proto.html#_CPPv4N6nvidia15inferenceserver12ServerStatus2idE">nvidia::inferenceserver::ServerStatus::id (C++ member)</a>
</li>
      <li><a href="protobuf_api/server_status.proto.html#_CPPv4N6nvidia15inferenceserver12ServerStatus12model_statusE">nvidia::inferenceserver::ServerStatus::model_status (C++ member)</a>
</li>
      <li><a href="protobuf_api/server_status.proto.html#_CPPv4N6nvidia15inferenceserver12ServerStatus13profile_statsE">nvidia::inferenceserver::ServerStatus::profile_stats (C++ member)</a>
</li>
      <li><a href="protobuf_api/server_status.proto.html#_CPPv4N6nvidia15inferenceserver12ServerStatus11ready_stateE">nvidia::inferenceserver::ServerStatus::ready_state (C++ member)</a>
</li>
      <li><a href="protobuf_api/server_status.proto.html#_CPPv4N6nvidia15inferenceserver12ServerStatus12status_statsE">nvidia::inferenceserver::ServerStatus::status_stats (C++ member)</a>
</li>
      <li><a href="protobuf_api/server_status.proto.html#_CPPv4N6nvidia15inferenceserver12ServerStatus9uptime_nsE">nvidia::inferenceserver::ServerStatus::uptime_ns (C++ member)</a>
</li>
      <li><a href="protobuf_api/server_status.proto.html#_CPPv4N6nvidia15inferenceserver12ServerStatus7versionE">nvidia::inferenceserver::ServerStatus::version (C++ member)</a>
</li>
      <li><a href="protobuf_api/server_status.proto.html#_CPPv4N6nvidia15inferenceserver12StatDurationE">nvidia::inferenceserver::StatDuration (C++ member)</a>
</li>
      <li><a href="protobuf_api/server_status.proto.html#_CPPv4N6nvidia15inferenceserver12StatDuration5countE">nvidia::inferenceserver::StatDuration::count (C++ member)</a>
</li>
      <li><a href="protobuf_api/server_status.proto.html#_CPPv4N6nvidia15inferenceserver12StatDuration13total_time_nsE">nvidia::inferenceserver::StatDuration::total_time_ns (C++ member)</a>
</li>
      <li><a href="protobuf_api/grpc_service.proto.html#_CPPv4N6nvidia15inferenceserver13StatusRequestE">nvidia::inferenceserver::StatusRequest (C++ member)</a>
</li>
      <li><a href="protobuf_api/grpc_service.proto.html#_CPPv4N6nvidia15inferenceserver13StatusRequest10model_nameE">nvidia::inferenceserver::StatusRequest::model_name (C++ member)</a>
</li>
      <li><a href="protobuf_api/server_status.proto.html#_CPPv4N6nvidia15inferenceserver18StatusRequestStatsE">nvidia::inferenceserver::StatusRequestStats (C++ member)</a>
</li>
      <li><a href="protobuf_api/server_status.proto.html#_CPPv4N6nvidia15inferenceserver18StatusRequestStats7successE">nvidia::inferenceserver::StatusRequestStats::success (C++ member)</a>
</li>
      <li><a href="protobuf_api/grpc_service.proto.html#_CPPv4N6nvidia15inferenceserver14StatusResponseE">nvidia::inferenceserver::StatusResponse (C++ member)</a>
</li>
      <li><a href="protobuf_api/grpc_service.proto.html#_CPPv4N6nvidia15inferenceserver14StatusResponse14request_statusE">nvidia::inferenceserver::StatusResponse::request_status (C++ member)</a>
</li>
      <li><a href="protobuf_api/grpc_service.proto.html#_CPPv4N6nvidia15inferenceserver14StatusResponse13server_statusE">nvidia::inferenceserver::StatusResponse::server_status (C++ member)</a>
</li>
  </ul></td>
</tr></table>



           </div>
           
          </div>
          <footer>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2018, NVIDIA Corporation

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
    

  <style>
  a:link, a:visited {
    color: #76b900;
  }

  a:hover {
    color: #8c0;
  }

  .rst-content dl:not(.docutils) dt {
    background: rgba(118, 185, 0, 0.1);
    color: rgba(59,93,0,1);
    border-top: solid 3px rgba(59,93,0,1);
  }
  </style>
  

</body>
</html>