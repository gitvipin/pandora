<!--
# Copyright (c) 2018, NVIDIA CORPORATION. All rights reserved.
#
# Redistribution and use in source and binary forms, with or without
# modification, are permitted provided that the following conditions
# are met:
#  * Redistributions of source code must retain the above copyright
#    notice, this list of conditions and the following disclaimer.
#  * Redistributions in binary form must reproduce the above copyright
#    notice, this list of conditions and the following disclaimer in the
#    documentation and/or other materials provided with the distribution.
#  * Neither the name of NVIDIA CORPORATION nor the names of its
#    contributors may be used to endorse or promote products derived
#    from this software without specific prior written permission.
#
# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
# EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
# PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
# PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
# OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
# (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
# OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
-->


<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Inference Server API &mdash; NVIDIA TensorRT Inference Server 0.12.0dev
 documentation</title>
  

  
  
  
  
    <link rel="canonical" href="https://docs.nvidia.com/deeplearning/sdk/tensorrt-inference-server-guide/docs/index.htmlhttp_grpc_api.html"/>
  

  
  <script type="text/javascript" src="_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
        <script type="text/javascript" src="_static/jquery.js"></script>
        <script type="text/javascript" src="_static/underscore.js"></script>
        <script type="text/javascript" src="_static/doctools.js"></script>
        <script type="text/javascript" src="_static/language_data.js"></script>
        <script type="text/javascript" src="_static/collapsible-lists/js/CollapsibleLists.compressed.js"></script>
        <script type="text/javascript" src="_static/collapsible-lists/js/apply-collapsible-lists.js"></script>
    
    <script type="text/javascript" src="_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="_static/collapsible-lists/css/tree_view.css" type="text/css" />
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Metrics" href="metrics.html" />
    <link rel="prev" title="Model Configuration" href="model_configuration.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
           

          
            <a href="index.html" class="icon icon-home"> NVIDIA TensorRT Inference Server
          

          
          </a>

          
            
            
              <div class="version">
                0.12.0dev
-ad20b82<br/>
Version select: <select onChange="window.location.href = this.value" onFocus="this.selectedIndex = -1">
    <option value="https://docs.nvidia.com/deeplearning/sdk/tensorrt-inference-server-guide/docs/index.html">Current release</option>
    <option value="https://docs.nvidia.com/deeplearning/sdk/tensorrt-inference-server-master-branch-guide/docs/index.html">master (unstable)</option>
    <option value="https://docs.nvidia.com/deeplearning/sdk/inference-server-archived/index.html">Older releases</option>
</select>
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          

  <style>
    /* Sidebar header (and topbar for mobile) */
    .wy-side-nav-search, .wy-nav-top {
      background: #76b900;
    }

    .wy-side-nav-search a:link, .wy-nav-top a:link {
      color: #fff;
    }
    .wy-side-nav-search a:visited, .wy-nav-top a:visited {
      color: #fff;
    }
    .wy-side-nav-search a:hover, .wy-nav-top a:hover {
      color: #fff;
    }

    .wy-menu-vertical a:link, .wy-menu-vertical a:visited {
      color: #d9d9d9
    }

    .wy-menu-vertical a:active {
      background-color: #76b900
    }

    .wy-side-nav-search>div.version {
      color: rgba(0, 0, 0, 0.3)
    }
  </style>
  
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul>
<li class="toctree-l1"><a class="reference internal" href="index.html">Documentation home</a></li>
</ul>
<p class="caption"><span class="caption-text">User Guide</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="quickstart.html">Quickstart</a><ul>
<li class="toctree-l2"><a class="reference internal" href="quickstart.html#prerequisites">Prerequisites</a></li>
<li class="toctree-l2"><a class="reference internal" href="quickstart.html#using-a-prebuilt-docker-container">Using A Prebuilt Docker Container</a></li>
<li class="toctree-l2"><a class="reference internal" href="quickstart.html#building-from-source-code">Building From Source Code</a></li>
<li class="toctree-l2"><a class="reference internal" href="quickstart.html#run-tensorrt-inference-server">Run TensorRT Inference Server</a></li>
<li class="toctree-l2"><a class="reference internal" href="quickstart.html#verify-inference-server-is-running-correctly">Verify Inference Server Is Running Correctly</a></li>
<li class="toctree-l2"><a class="reference internal" href="quickstart.html#building-the-client-examples">Building The Client Examples</a></li>
<li class="toctree-l2"><a class="reference internal" href="quickstart.html#running-the-image-classification-example">Running The Image Classification Example</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="install.html">Installing the Server</a><ul>
<li class="toctree-l2"><a class="reference internal" href="install.html#installing-prebuilt-containers">Installing Prebuilt Containers</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="run.html">Running the Server</a><ul>
<li class="toctree-l2"><a class="reference internal" href="run.html#example-model-repository">Example Model Repository</a></li>
<li class="toctree-l2"><a class="reference internal" href="run.html#running-the-inference-server">Running The Inference Server</a></li>
<li class="toctree-l2"><a class="reference internal" href="run.html#running-the-inference-server-on-a-system-without-a-gpu">Running The Inference Server On A System Without A GPU</a></li>
<li class="toctree-l2"><a class="reference internal" href="run.html#checking-inference-server-status">Checking Inference Server Status</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="client.html">Client Libraries and Examples</a><ul>
<li class="toctree-l2"><a class="reference internal" href="client.html#building-the-client-libraries-and-examples">Building the Client Libraries and Examples</a></li>
<li class="toctree-l2"><a class="reference internal" href="client.html#image-classification-example-application">Image Classification Example Application</a></li>
<li class="toctree-l2"><a class="reference internal" href="client.html#performance-example-application">Performance Example Application</a></li>
<li class="toctree-l2"><a class="reference internal" href="client.html#client-api">Client API</a><ul>
<li class="toctree-l3"><a class="reference internal" href="client.html#string-datatype">String Datatype</a></li>
<li class="toctree-l3"><a class="reference internal" href="client.html#stream-inference">Stream Inference</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="model_repository.html">Model Repository</a><ul>
<li class="toctree-l2"><a class="reference internal" href="model_repository.html#modifying-the-model-repository">Modifying the Model Repository</a></li>
<li class="toctree-l2"><a class="reference internal" href="model_repository.html#model-versions">Model Versions</a></li>
<li class="toctree-l2"><a class="reference internal" href="model_repository.html#framework-model-definition">Framework Model Definition</a><ul>
<li class="toctree-l3"><a class="reference internal" href="model_repository.html#tensorrt-models">TensorRT Models</a></li>
<li class="toctree-l3"><a class="reference internal" href="model_repository.html#tensorflow-models">TensorFlow Models</a></li>
<li class="toctree-l3"><a class="reference internal" href="model_repository.html#caffe2-models">Caffe2 Models</a></li>
<li class="toctree-l3"><a class="reference internal" href="model_repository.html#tensorrt-tensorflow-models">TensorRT/TensorFlow Models</a></li>
<li class="toctree-l3"><a class="reference internal" href="model_repository.html#onnx-models">ONNX Models</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="model_repository.html#custom-backends">Custom Backends</a><ul>
<li class="toctree-l3"><a class="reference internal" href="model_repository.html#custom-backend-api">Custom Backend API</a></li>
<li class="toctree-l3"><a class="reference internal" href="model_repository.html#example-custom-backend">Example Custom Backend</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="model_configuration.html">Model Configuration</a><ul>
<li class="toctree-l2"><a class="reference internal" href="model_configuration.html#generated-model-configuration">Generated Model Configuration</a></li>
<li class="toctree-l2"><a class="reference internal" href="model_configuration.html#datatypes">Datatypes</a></li>
<li class="toctree-l2"><a class="reference internal" href="model_configuration.html#version-policy">Version Policy</a></li>
<li class="toctree-l2"><a class="reference internal" href="model_configuration.html#instance-groups">Instance Groups</a></li>
<li class="toctree-l2"><a class="reference internal" href="model_configuration.html#dynamic-batching">Dynamic Batching</a></li>
<li class="toctree-l2"><a class="reference internal" href="model_configuration.html#optimization-policy">Optimization Policy</a></li>
</ul>
</li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Inference Server API</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#health">Health</a></li>
<li class="toctree-l2"><a class="reference internal" href="#status">Status</a></li>
<li class="toctree-l2"><a class="reference internal" href="#inference">Inference</a></li>
<li class="toctree-l2"><a class="reference internal" href="#stream-inference">Stream Inference</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="metrics.html">Metrics</a></li>
</ul>
<p class="caption"><span class="caption-text">Developer Guide</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="architecture.html">Architecture</a><ul>
<li class="toctree-l2"><a class="reference internal" href="architecture.html#concurrent-model-execution">Concurrent Model Execution</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="contribute.html">Contributing</a><ul>
<li class="toctree-l2"><a class="reference internal" href="contribute.html#coding-convention">Coding Convention</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="build.html">Building</a><ul>
<li class="toctree-l2"><a class="reference internal" href="build.html#building-the-server">Building the Server</a><ul>
<li class="toctree-l3"><a class="reference internal" href="build.html#incremental-builds">Incremental Builds</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="build.html#building-the-client-libraries-and-examples">Building the Client Libraries and Examples</a></li>
<li class="toctree-l2"><a class="reference internal" href="build.html#building-the-documentation">Building the Documentation</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="test.html">Testing</a><ul>
<li class="toctree-l2"><a class="reference internal" href="test.html#generate-qa-model-repositories">Generate QA Model Repositories</a></li>
<li class="toctree-l2"><a class="reference internal" href="test.html#build-qa-container">Build QA Container</a></li>
<li class="toctree-l2"><a class="reference internal" href="test.html#run-qa-container">Run QA Container</a></li>
</ul>
</li>
</ul>
<p class="caption"><span class="caption-text">API Reference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="protobuf_api/protobuf_api_root.html">Protobuf API</a><ul>
<li class="toctree-l2"><a class="reference internal" href="protobuf_api/protobuf_api_root.html#http-grpc-api">HTTP/GRPC API</a></li>
<li class="toctree-l2"><a class="reference internal" href="protobuf_api/protobuf_api_root.html#model-configuration">Model Configuration</a></li>
<li class="toctree-l2"><a class="reference internal" href="protobuf_api/protobuf_api_root.html#status">Status</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="cpp_api/cpp_api_root.html">C++ API</a><ul>
<li class="toctree-l2"><a class="reference internal" href="cpp_api/cpp_api_root.html#class-hierarchy">Class Hierarchy</a></li>
<li class="toctree-l2"><a class="reference internal" href="cpp_api/cpp_api_root.html#file-hierarchy">File Hierarchy</a></li>
<li class="toctree-l2"><a class="reference internal" href="cpp_api/cpp_api_root.html#full-api">Full API</a><ul>
<li class="toctree-l3"><a class="reference internal" href="cpp_api/cpp_api_root.html#namespaces">Namespaces</a><ul>
<li class="toctree-l4"><a class="reference internal" href="cpp_api/namespace_nvidia.html">Namespace nvidia</a></li>
<li class="toctree-l4"><a class="reference internal" href="cpp_api/namespace_nvidia__inferenceserver.html">Namespace nvidia::inferenceserver</a></li>
<li class="toctree-l4"><a class="reference internal" href="cpp_api/namespace_nvidia__inferenceserver__client.html">Namespace nvidia::inferenceserver::client</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="cpp_api/cpp_api_root.html#classes-and-structs">Classes and Structs</a><ul>
<li class="toctree-l4"><a class="reference internal" href="cpp_api/structcustom__payload__struct.html">Struct custom_payload_struct</a></li>
<li class="toctree-l4"><a class="reference internal" href="cpp_api/structnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result_1_1ClassResult.html">Struct Result::ClassResult</a></li>
<li class="toctree-l4"><a class="reference internal" href="cpp_api/structnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Stat.html">Struct InferContext::Stat</a></li>
<li class="toctree-l4"><a class="reference internal" href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1Error.html">Class Error</a></li>
<li class="toctree-l4"><a class="reference internal" href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext.html">Class InferContext</a></li>
<li class="toctree-l4"><a class="reference internal" href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Input.html">Class InferContext::Input</a></li>
<li class="toctree-l4"><a class="reference internal" href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Options.html">Class InferContext::Options</a></li>
<li class="toctree-l4"><a class="reference internal" href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Output.html">Class InferContext::Output</a></li>
<li class="toctree-l4"><a class="reference internal" href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Request.html">Class InferContext::Request</a></li>
<li class="toctree-l4"><a class="reference internal" href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1RequestTimers.html">Class InferContext::RequestTimers</a></li>
<li class="toctree-l4"><a class="reference internal" href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result.html">Class InferContext::Result</a></li>
<li class="toctree-l4"><a class="reference internal" href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferGrpcContext.html">Class InferGrpcContext</a></li>
<li class="toctree-l4"><a class="reference internal" href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferGrpcStreamContext.html">Class InferGrpcStreamContext</a></li>
<li class="toctree-l4"><a class="reference internal" href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferHttpContext.html">Class InferHttpContext</a></li>
<li class="toctree-l4"><a class="reference internal" href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ProfileContext.html">Class ProfileContext</a></li>
<li class="toctree-l4"><a class="reference internal" href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ProfileGrpcContext.html">Class ProfileGrpcContext</a></li>
<li class="toctree-l4"><a class="reference internal" href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ProfileHttpContext.html">Class ProfileHttpContext</a></li>
<li class="toctree-l4"><a class="reference internal" href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ServerHealthContext.html">Class ServerHealthContext</a></li>
<li class="toctree-l4"><a class="reference internal" href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ServerHealthGrpcContext.html">Class ServerHealthGrpcContext</a></li>
<li class="toctree-l4"><a class="reference internal" href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ServerHealthHttpContext.html">Class ServerHealthHttpContext</a></li>
<li class="toctree-l4"><a class="reference internal" href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ServerStatusContext.html">Class ServerStatusContext</a></li>
<li class="toctree-l4"><a class="reference internal" href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ServerStatusGrpcContext.html">Class ServerStatusGrpcContext</a></li>
<li class="toctree-l4"><a class="reference internal" href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ServerStatusHttpContext.html">Class ServerStatusHttpContext</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="cpp_api/cpp_api_root.html#functions">Functions</a><ul>
<li class="toctree-l4"><a class="reference internal" href="cpp_api/function_custom_8h_1aedfdcb69f8eaab594173b197b10dc804.html">Function CustomErrorString</a></li>
<li class="toctree-l4"><a class="reference internal" href="cpp_api/function_custom_8h_1afc75c0a4e1a169562586c2d4ff09ba40.html">Function CustomExecute</a></li>
<li class="toctree-l4"><a class="reference internal" href="cpp_api/function_custom_8h_1a9cd799dfef77db82babc6ddd4a4be707.html">Function CustomFinalize</a></li>
<li class="toctree-l4"><a class="reference internal" href="cpp_api/function_custom_8h_1ad384f6af9bfb11b82b2b4e411f799b6f.html">Function CustomInitialize</a></li>
<li class="toctree-l4"><a class="reference internal" href="cpp_api/function_request_8h_1a19d352bb13848c14b5f03274b25ecee3.html">Function nvidia::inferenceserver::client::operator&lt;&lt;</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="cpp_api/cpp_api_root.html#defines">Defines</a><ul>
<li class="toctree-l4"><a class="reference internal" href="cpp_api/define_custom_8h_1aaf537da73bb55fda05fb4e39defe1949.html">Define CUSTOM_NO_GPU_DEVICE</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="cpp_api/cpp_api_root.html#typedefs">Typedefs</a><ul>
<li class="toctree-l4"><a class="reference internal" href="cpp_api/typedef_custom_8h_1aa6616d50105d3441ba277c6dbce66286.html">Typedef CustomErrorStringFn_t</a></li>
<li class="toctree-l4"><a class="reference internal" href="cpp_api/typedef_custom_8h_1ac0c3508cd00e9db56912ff40deecf9d2.html">Typedef CustomExecuteFn_t</a></li>
<li class="toctree-l4"><a class="reference internal" href="cpp_api/typedef_custom_8h_1aa32f51734df1f217cc7c2b07f31f8f3f.html">Typedef CustomFinalizeFn_t</a></li>
<li class="toctree-l4"><a class="reference internal" href="cpp_api/typedef_custom_8h_1aee3723f1a17ff4664691655e056a309c.html">Typedef CustomGetNextInputFn_t</a></li>
<li class="toctree-l4"><a class="reference internal" href="cpp_api/typedef_custom_8h_1a71cb419d6e7feee902115438973d02f8.html">Typedef CustomGetOutputFn_t</a></li>
<li class="toctree-l4"><a class="reference internal" href="cpp_api/typedef_custom_8h_1a11fea108abb0b28f86a96e6b7cd92a20.html">Typedef CustomInitializeFn_t</a></li>
<li class="toctree-l4"><a class="reference internal" href="cpp_api/typedef_custom_8h_1a99a7b9dcc756702784a869d9afb59df4.html">Typedef CustomPayload</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="cpp_api/cpp_api_root.html#directories">Directories</a><ul>
<li class="toctree-l4"><a class="reference internal" href="cpp_api/dir_src.html">Directory src</a></li>
<li class="toctree-l4"><a class="reference internal" href="cpp_api/dir_src_clients.html">Directory clients</a></li>
<li class="toctree-l4"><a class="reference internal" href="cpp_api/dir_src_clients_c++.html">Directory c++</a></li>
<li class="toctree-l4"><a class="reference internal" href="cpp_api/dir_src_servables.html">Directory servables</a></li>
<li class="toctree-l4"><a class="reference internal" href="cpp_api/dir_src_servables_custom.html">Directory custom</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="cpp_api/cpp_api_root.html#files">Files</a><ul>
<li class="toctree-l4"><a class="reference internal" href="cpp_api/file_src_servables_custom_custom.h.html">File custom.h</a></li>
<li class="toctree-l4"><a class="reference internal" href="cpp_api/file_src_clients_c++_request.h.html">File request.h</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="python_api.html">Python API</a><ul>
<li class="toctree-l2"><a class="reference internal" href="python_api.html#client">Client</a></li>
</ul>
</li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">NVIDIA TensorRT Inference Server</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html">Docs</a> &raquo;</li>
        
      <li>Inference Server API</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="_sources/http_grpc_api.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  
<style>
/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast,
.nboutput.nblast {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast + .nbinput {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<div class="section" id="inference-server-api">
<span id="section-inference-server-api"></span><h1>Inference Server API<a class="headerlink" href="#inference-server-api" title="Permalink to this headline">¶</a></h1>
<p>The TensorRT Inference Server exposes both HTTP and GRPC
endpoints. Three endpoints with identical functionality are exposed
for each protocol.</p>
<ul class="simple">
<li><a class="reference internal" href="#section-api-health"><span class="std std-ref">Health</span></a>: The server health API for determining
server liveness and readiness.</li>
<li><a class="reference internal" href="#section-api-status"><span class="std std-ref">Status</span></a>: The server status API for getting
information about the server and about the models being served.</li>
<li><a class="reference internal" href="#section-api-inference"><span class="std std-ref">Inference</span></a>: The inference API that accepts model
inputs, runs inference and returns the requested outputs.</li>
</ul>
<p>The inference server also exposes an endpoint based on GRPC streams that is
only available when using the GRPC protocol:</p>
<ul class="simple">
<li><a class="reference internal" href="#section-api-stream-inference"><span class="std std-ref">Stream Inference</span></a>: The stream inference API is the same
as the Inference API, except that once the connection is established,
the requests are sent in the same connection until it is closed.</li>
</ul>
<p>The HTTP endpoints can be used directly as described in this section,
but for most use-cases, the preferred way to access the inference
server is via the <a class="reference internal" href="client.html#section-client-libraries-and-examples"><span class="std std-ref">C++ and Python Client libraries</span></a>.</p>
<p>The GRPC endpoints can also be used via the <a class="reference internal" href="client.html#section-client-libraries-and-examples"><span class="std std-ref">C++ and Python Client
libraries</span></a> or a GRPC-generated
API can be used directly as shown in the grpc_image_client.py example.</p>
<div class="section" id="health">
<span id="section-api-health"></span><h2>Health<a class="headerlink" href="#health" title="Permalink to this headline">¶</a></h2>
<p>Performing an HTTP GET to /api/health/live returns a 200 status if the
server is able to receive and process requests. Any other status code
indicates that the server is still initializing or has failed in some
way that prevents it from processing requests.</p>
<p>Once the liveness endpoint indicates that the server is active,
performing an HTTP GET to /api/health/ready returns a 200 status if
the server is able to respond to inference requests for some or all
models (based on the inference server’s --strict-readiness option
explained below). Any other status code indicates that the server is
not ready to respond to some or all inference requests.</p>
<p>For GRPC the <a class="reference internal" href="protobuf_api/grpc_service.proto.html#_CPPv4N6nvidia15inferenceserver11GRPCServiceE" title="nvidia::inferenceserver::GRPCService"><code class="xref cpp cpp-var docutils literal notranslate"><span class="pre">GRPCService</span></code></a> uses the
<a class="reference internal" href="protobuf_api/grpc_service.proto.html#_CPPv4N6nvidia15inferenceserver13HealthRequestE" title="nvidia::inferenceserver::HealthRequest"><code class="xref cpp cpp-var docutils literal notranslate"><span class="pre">HealthRequest</span></code></a> and
<a class="reference internal" href="protobuf_api/grpc_service.proto.html#_CPPv4N6nvidia15inferenceserver14HealthResponseE" title="nvidia::inferenceserver::HealthResponse"><code class="xref cpp cpp-var docutils literal notranslate"><span class="pre">HealthResponse</span></code></a>
messages to implement the endpoint.</p>
<p>By default, the readiness endpoint will return success if the server
is responsive and all models loaded successfully. Thus, by default,
success indicates that an inference request for any model can be
handled by the server. For some use cases, you want the readiness
endpoint to return success even if all models are not available. In
this case, use the --strict-readiness=false option to cause the
readiness endpoint to report success as long as the server is
responsive (even if one or more models are not available).</p>
</div>
<div class="section" id="status">
<span id="section-api-status"></span><h2>Status<a class="headerlink" href="#status" title="Permalink to this headline">¶</a></h2>
<p>Performing an HTTP GET to /api/status returns status information about
the server and all the models being served. Performing an HTTP GET to
/api/status/&lt;model name&gt; returns information about the server and the
single model specified by &lt;model name&gt;. The server status is returned
in the HTTP response body in either text format (the default) or in
binary format if query parameter format=binary is specified (for
example, /api/status?format=binary). The success or failure of the
status request is indicated in the HTTP response code and the
<strong>NV-Status</strong> response header. The <strong>NV-Status</strong> response header
returns a text protobuf formatted <a class="reference internal" href="protobuf_api/request_status.proto.html#_CPPv4N6nvidia15inferenceserver13RequestStatusE" title="nvidia::inferenceserver::RequestStatus"><code class="xref cpp cpp-var docutils literal notranslate"><span class="pre">RequestStatus</span></code></a> message.</p>
<p>For GRPC the <a class="reference internal" href="protobuf_api/grpc_service.proto.html#_CPPv4N6nvidia15inferenceserver11GRPCServiceE" title="nvidia::inferenceserver::GRPCService"><code class="xref cpp cpp-var docutils literal notranslate"><span class="pre">GRPCService</span></code></a> uses the
<a class="reference internal" href="protobuf_api/grpc_service.proto.html#_CPPv4N6nvidia15inferenceserver13StatusRequestE" title="nvidia::inferenceserver::StatusRequest"><code class="xref cpp cpp-var docutils literal notranslate"><span class="pre">StatusRequest</span></code></a> and
<a class="reference internal" href="protobuf_api/grpc_service.proto.html#_CPPv4N6nvidia15inferenceserver14StatusResponseE" title="nvidia::inferenceserver::StatusResponse"><code class="xref cpp cpp-var docutils literal notranslate"><span class="pre">StatusResponse</span></code></a>
messages to implement the endpoint. The response includes a
<a class="reference internal" href="protobuf_api/request_status.proto.html#_CPPv4N6nvidia15inferenceserver13RequestStatusE" title="nvidia::inferenceserver::RequestStatus"><code class="xref cpp cpp-var docutils literal notranslate"><span class="pre">RequestStatus</span></code></a>
message indicating success or failure.</p>
<p>For either protocol the status itself is returned as a
<a class="reference internal" href="protobuf_api/server_status.proto.html#_CPPv4N6nvidia15inferenceserver12ServerStatusE" title="nvidia::inferenceserver::ServerStatus"><code class="xref cpp cpp-var docutils literal notranslate"><span class="pre">ServerStatus</span></code></a>
message.</p>
</div>
<div class="section" id="inference">
<span id="section-api-inference"></span><h2>Inference<a class="headerlink" href="#inference" title="Permalink to this headline">¶</a></h2>
<p>Performing an HTTP POST to /api/infer/&lt;model name&gt; performs inference
using the latest version of the model that is being made available by
the model’s <a class="reference internal" href="model_configuration.html#section-version-policy"><span class="std std-ref">version policy</span></a>. The latest
version is the numerically greatest version number. Performing an HTTP
POST to /api/infer/&lt;model name&gt;/&lt;model version&gt; performs inference
using a specific version of the model.</p>
<p>The request uses the <strong>NV-InferRequest</strong> header to communicate an
<a class="reference internal" href="protobuf_api/api.proto.html#_CPPv4N6nvidia15inferenceserver18InferRequestHeaderE" title="nvidia::inferenceserver::InferRequestHeader"><code class="xref cpp cpp-var docutils literal notranslate"><span class="pre">InferRequestHeader</span></code></a> message that describes
the input tensors and the requested output tensors. For example, for a
resnet50 model the following <strong>NV-InferRequest</strong> header indicates that
a batch-size 1 request is being made with a single input named
“input”, and that the result of the tensor named “output” should be
returned as the top-3 classification values:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>NV-InferRequest: batch_size: 1 input { name: &quot;input&quot; } output { name: &quot;output&quot; cls { count: 3 } }
</pre></div>
</div>
<p>The input tensor values are communicated in the body of the HTTP POST
request as raw binary in the order as the inputs are listed in the
request header.</p>
<p>The HTTP response includes an <strong>NV-InferResponse</strong> header that
communicates an <a class="reference internal" href="protobuf_api/api.proto.html#_CPPv4N6nvidia15inferenceserver19InferResponseHeaderE" title="nvidia::inferenceserver::InferResponseHeader"><code class="xref cpp cpp-var docutils literal notranslate"><span class="pre">InferResponseHeader</span></code></a> message that describes
the outputs. For example the above response could return the
following:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>NV-InferResponse: model_name: &quot;mymodel&quot; model_version: 1 batch_size: 1 output { name: &quot;output&quot; raw { dims: 4 dims: 4 batch_byte_size: 64 } }
</pre></div>
</div>
<p>This response shows that the output in a tensor with shape [ 4, 4 ]
and has a size of 64 bytes. The output tensor contents are returned in
the body of the HTTP response to the POST request. For outputs where
full result tensors were requested, the result values are communicated
in the body of the response in the order as the outputs are listed in
the <strong>NV-InferResponse</strong> header. After those, an
<a class="reference internal" href="protobuf_api/api.proto.html#_CPPv4N6nvidia15inferenceserver19InferResponseHeaderE" title="nvidia::inferenceserver::InferResponseHeader"><code class="xref cpp cpp-var docutils literal notranslate"><span class="pre">InferResponseHeader</span></code></a> message is appended to
the response body. The <a class="reference internal" href="protobuf_api/api.proto.html#_CPPv4N6nvidia15inferenceserver19InferResponseHeaderE" title="nvidia::inferenceserver::InferResponseHeader"><code class="xref cpp cpp-var docutils literal notranslate"><span class="pre">InferResponseHeader</span></code></a> message is returned in
either text format (the default) or in binary format if query
parameter format=binary is specified (for example,
/api/infer/foo?format=binary).</p>
<p>For example, assuming an inference request for a model that has ‘n’
outputs, the outputs specified in the <strong>NV-InferResponse</strong> header in
order are “output[0]”, …, “output[n-1]” the response body would
contain:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>&lt;raw binary tensor values for output[0] &gt;
...
&lt;raw binary tensor values for output[n-1] &gt;
&lt;text or binary encoded InferResponseHeader proto&gt;
</pre></div>
</div>
<p>The success or failure of the inference request is indicated in the
HTTP response code and the <strong>NV-Status</strong> response header. The
<strong>NV-Status</strong> response header returns a text protobuf formatted
<a class="reference internal" href="protobuf_api/request_status.proto.html#_CPPv4N6nvidia15inferenceserver13RequestStatusE" title="nvidia::inferenceserver::RequestStatus"><code class="xref cpp cpp-var docutils literal notranslate"><span class="pre">RequestStatus</span></code></a>
message.</p>
<p>For GRPC the <a class="reference internal" href="protobuf_api/grpc_service.proto.html#_CPPv4N6nvidia15inferenceserver11GRPCServiceE" title="nvidia::inferenceserver::GRPCService"><code class="xref cpp cpp-var docutils literal notranslate"><span class="pre">GRPCService</span></code></a> uses the
<a class="reference internal" href="protobuf_api/grpc_service.proto.html#_CPPv4N6nvidia15inferenceserver12InferRequestE" title="nvidia::inferenceserver::InferRequest"><code class="xref cpp cpp-var docutils literal notranslate"><span class="pre">InferRequest</span></code></a> and
<a class="reference internal" href="protobuf_api/grpc_service.proto.html#_CPPv4N6nvidia15inferenceserver13InferResponseE" title="nvidia::inferenceserver::InferResponse"><code class="xref cpp cpp-var docutils literal notranslate"><span class="pre">InferResponse</span></code></a>
messages to implement the endpoint. The response includes a
<a class="reference internal" href="protobuf_api/request_status.proto.html#_CPPv4N6nvidia15inferenceserver13RequestStatusE" title="nvidia::inferenceserver::RequestStatus"><code class="xref cpp cpp-var docutils literal notranslate"><span class="pre">RequestStatus</span></code></a>
message indicating success or failure, <a class="reference internal" href="protobuf_api/api.proto.html#_CPPv4N6nvidia15inferenceserver19InferResponseHeaderE" title="nvidia::inferenceserver::InferResponseHeader"><code class="xref cpp cpp-var docutils literal notranslate"><span class="pre">InferResponseHeader</span></code></a> message giving
response meta-data, and the raw output tensors.</p>
</div>
<div class="section" id="stream-inference">
<span id="section-api-stream-inference"></span><h2>Stream Inference<a class="headerlink" href="#stream-inference" title="Permalink to this headline">¶</a></h2>
<p>For GRPC the <a class="reference internal" href="protobuf_api/grpc_service.proto.html#_CPPv4N6nvidia15inferenceserver11GRPCServiceE" title="nvidia::inferenceserver::GRPCService"><code class="xref cpp cpp-var docutils literal notranslate"><span class="pre">GRPCService</span></code></a> uses the
<a class="reference internal" href="protobuf_api/grpc_service.proto.html#_CPPv4N6nvidia15inferenceserver12InferRequestE" title="nvidia::inferenceserver::InferRequest"><code class="xref cpp cpp-var docutils literal notranslate"><span class="pre">InferRequest</span></code></a> and
<a class="reference internal" href="protobuf_api/grpc_service.proto.html#_CPPv4N6nvidia15inferenceserver13InferResponseE" title="nvidia::inferenceserver::InferResponse"><code class="xref cpp cpp-var docutils literal notranslate"><span class="pre">InferResponse</span></code></a>
messages to implement the endpoint. The response includes a
<a class="reference internal" href="protobuf_api/request_status.proto.html#_CPPv4N6nvidia15inferenceserver13RequestStatusE" title="nvidia::inferenceserver::RequestStatus"><code class="xref cpp cpp-var docutils literal notranslate"><span class="pre">RequestStatus</span></code></a>
message indicating success or failure, <a class="reference internal" href="protobuf_api/api.proto.html#_CPPv4N6nvidia15inferenceserver19InferResponseHeaderE" title="nvidia::inferenceserver::InferResponseHeader"><code class="xref cpp cpp-var docutils literal notranslate"><span class="pre">InferResponseHeader</span></code></a> message giving
response meta-data, and the raw output tensors.</p>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="metrics.html" class="btn btn-neutral float-right" title="Metrics" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="model_configuration.html" class="btn btn-neutral float-left" title="Model Configuration" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2018, NVIDIA Corporation

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
    

  <style>
  a:link, a:visited {
    color: #76b900;
  }

  a:hover {
    color: #8c0;
  }

  .rst-content dl:not(.docutils) dt {
    background: rgba(118, 185, 0, 0.1);
    color: rgba(59,93,0,1);
    border-top: solid 3px rgba(59,93,0,1);
  }
  </style>
  

</body>
</html>