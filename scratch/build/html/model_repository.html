<!--
# Copyright (c) 2018, NVIDIA CORPORATION. All rights reserved.
#
# Redistribution and use in source and binary forms, with or without
# modification, are permitted provided that the following conditions
# are met:
#  * Redistributions of source code must retain the above copyright
#    notice, this list of conditions and the following disclaimer.
#  * Redistributions in binary form must reproduce the above copyright
#    notice, this list of conditions and the following disclaimer in the
#    documentation and/or other materials provided with the distribution.
#  * Neither the name of NVIDIA CORPORATION nor the names of its
#    contributors may be used to endorse or promote products derived
#    from this software without specific prior written permission.
#
# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
# EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
# PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
# PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
# OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
# (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
# OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
-->


<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Model Repository &mdash; NVIDIA TensorRT Inference Server 0.12.0dev
 documentation</title>
  

  
  
  
  
    <link rel="canonical" href="https://docs.nvidia.com/deeplearning/sdk/tensorrt-inference-server-guide/docs/index.htmlmodel_repository.html"/>
  

  
  <script type="text/javascript" src="_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
        <script type="text/javascript" src="_static/jquery.js"></script>
        <script type="text/javascript" src="_static/underscore.js"></script>
        <script type="text/javascript" src="_static/doctools.js"></script>
        <script type="text/javascript" src="_static/language_data.js"></script>
        <script type="text/javascript" src="_static/collapsible-lists/js/CollapsibleLists.compressed.js"></script>
        <script type="text/javascript" src="_static/collapsible-lists/js/apply-collapsible-lists.js"></script>
    
    <script type="text/javascript" src="_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="_static/collapsible-lists/css/tree_view.css" type="text/css" />
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Model Configuration" href="model_configuration.html" />
    <link rel="prev" title="Client Libraries and Examples" href="client.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
           

          
            <a href="index.html" class="icon icon-home"> NVIDIA TensorRT Inference Server
          

          
          </a>

          
            
            
              <div class="version">
                0.12.0dev
-ad20b82<br/>
Version select: <select onChange="window.location.href = this.value" onFocus="this.selectedIndex = -1">
    <option value="https://docs.nvidia.com/deeplearning/sdk/tensorrt-inference-server-guide/docs/index.html">Current release</option>
    <option value="https://docs.nvidia.com/deeplearning/sdk/tensorrt-inference-server-master-branch-guide/docs/index.html">master (unstable)</option>
    <option value="https://docs.nvidia.com/deeplearning/sdk/inference-server-archived/index.html">Older releases</option>
</select>
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          

  <style>
    /* Sidebar header (and topbar for mobile) */
    .wy-side-nav-search, .wy-nav-top {
      background: #76b900;
    }

    .wy-side-nav-search a:link, .wy-nav-top a:link {
      color: #fff;
    }
    .wy-side-nav-search a:visited, .wy-nav-top a:visited {
      color: #fff;
    }
    .wy-side-nav-search a:hover, .wy-nav-top a:hover {
      color: #fff;
    }

    .wy-menu-vertical a:link, .wy-menu-vertical a:visited {
      color: #d9d9d9
    }

    .wy-menu-vertical a:active {
      background-color: #76b900
    }

    .wy-side-nav-search>div.version {
      color: rgba(0, 0, 0, 0.3)
    }
  </style>
  
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul>
<li class="toctree-l1"><a class="reference internal" href="index.html">Documentation home</a></li>
</ul>
<p class="caption"><span class="caption-text">User Guide</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="quickstart.html">Quickstart</a><ul>
<li class="toctree-l2"><a class="reference internal" href="quickstart.html#prerequisites">Prerequisites</a></li>
<li class="toctree-l2"><a class="reference internal" href="quickstart.html#using-a-prebuilt-docker-container">Using A Prebuilt Docker Container</a></li>
<li class="toctree-l2"><a class="reference internal" href="quickstart.html#building-from-source-code">Building From Source Code</a></li>
<li class="toctree-l2"><a class="reference internal" href="quickstart.html#run-tensorrt-inference-server">Run TensorRT Inference Server</a></li>
<li class="toctree-l2"><a class="reference internal" href="quickstart.html#verify-inference-server-is-running-correctly">Verify Inference Server Is Running Correctly</a></li>
<li class="toctree-l2"><a class="reference internal" href="quickstart.html#building-the-client-examples">Building The Client Examples</a></li>
<li class="toctree-l2"><a class="reference internal" href="quickstart.html#running-the-image-classification-example">Running The Image Classification Example</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="install.html">Installing the Server</a><ul>
<li class="toctree-l2"><a class="reference internal" href="install.html#installing-prebuilt-containers">Installing Prebuilt Containers</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="run.html">Running the Server</a><ul>
<li class="toctree-l2"><a class="reference internal" href="run.html#example-model-repository">Example Model Repository</a></li>
<li class="toctree-l2"><a class="reference internal" href="run.html#running-the-inference-server">Running The Inference Server</a></li>
<li class="toctree-l2"><a class="reference internal" href="run.html#running-the-inference-server-on-a-system-without-a-gpu">Running The Inference Server On A System Without A GPU</a></li>
<li class="toctree-l2"><a class="reference internal" href="run.html#checking-inference-server-status">Checking Inference Server Status</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="client.html">Client Libraries and Examples</a><ul>
<li class="toctree-l2"><a class="reference internal" href="client.html#building-the-client-libraries-and-examples">Building the Client Libraries and Examples</a></li>
<li class="toctree-l2"><a class="reference internal" href="client.html#image-classification-example-application">Image Classification Example Application</a></li>
<li class="toctree-l2"><a class="reference internal" href="client.html#performance-example-application">Performance Example Application</a></li>
<li class="toctree-l2"><a class="reference internal" href="client.html#client-api">Client API</a><ul>
<li class="toctree-l3"><a class="reference internal" href="client.html#string-datatype">String Datatype</a></li>
<li class="toctree-l3"><a class="reference internal" href="client.html#stream-inference">Stream Inference</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Model Repository</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#modifying-the-model-repository">Modifying the Model Repository</a></li>
<li class="toctree-l2"><a class="reference internal" href="#model-versions">Model Versions</a></li>
<li class="toctree-l2"><a class="reference internal" href="#framework-model-definition">Framework Model Definition</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#tensorrt-models">TensorRT Models</a></li>
<li class="toctree-l3"><a class="reference internal" href="#tensorflow-models">TensorFlow Models</a></li>
<li class="toctree-l3"><a class="reference internal" href="#caffe2-models">Caffe2 Models</a></li>
<li class="toctree-l3"><a class="reference internal" href="#tensorrt-tensorflow-models">TensorRT/TensorFlow Models</a></li>
<li class="toctree-l3"><a class="reference internal" href="#onnx-models">ONNX Models</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#custom-backends">Custom Backends</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#custom-backend-api">Custom Backend API</a></li>
<li class="toctree-l3"><a class="reference internal" href="#example-custom-backend">Example Custom Backend</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="model_configuration.html">Model Configuration</a><ul>
<li class="toctree-l2"><a class="reference internal" href="model_configuration.html#generated-model-configuration">Generated Model Configuration</a></li>
<li class="toctree-l2"><a class="reference internal" href="model_configuration.html#datatypes">Datatypes</a></li>
<li class="toctree-l2"><a class="reference internal" href="model_configuration.html#version-policy">Version Policy</a></li>
<li class="toctree-l2"><a class="reference internal" href="model_configuration.html#instance-groups">Instance Groups</a></li>
<li class="toctree-l2"><a class="reference internal" href="model_configuration.html#dynamic-batching">Dynamic Batching</a></li>
<li class="toctree-l2"><a class="reference internal" href="model_configuration.html#optimization-policy">Optimization Policy</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="http_grpc_api.html">Inference Server API</a><ul>
<li class="toctree-l2"><a class="reference internal" href="http_grpc_api.html#health">Health</a></li>
<li class="toctree-l2"><a class="reference internal" href="http_grpc_api.html#status">Status</a></li>
<li class="toctree-l2"><a class="reference internal" href="http_grpc_api.html#inference">Inference</a></li>
<li class="toctree-l2"><a class="reference internal" href="http_grpc_api.html#stream-inference">Stream Inference</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="metrics.html">Metrics</a></li>
</ul>
<p class="caption"><span class="caption-text">Developer Guide</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="architecture.html">Architecture</a><ul>
<li class="toctree-l2"><a class="reference internal" href="architecture.html#concurrent-model-execution">Concurrent Model Execution</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="contribute.html">Contributing</a><ul>
<li class="toctree-l2"><a class="reference internal" href="contribute.html#coding-convention">Coding Convention</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="build.html">Building</a><ul>
<li class="toctree-l2"><a class="reference internal" href="build.html#building-the-server">Building the Server</a><ul>
<li class="toctree-l3"><a class="reference internal" href="build.html#incremental-builds">Incremental Builds</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="build.html#building-the-client-libraries-and-examples">Building the Client Libraries and Examples</a></li>
<li class="toctree-l2"><a class="reference internal" href="build.html#building-the-documentation">Building the Documentation</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="test.html">Testing</a><ul>
<li class="toctree-l2"><a class="reference internal" href="test.html#generate-qa-model-repositories">Generate QA Model Repositories</a></li>
<li class="toctree-l2"><a class="reference internal" href="test.html#build-qa-container">Build QA Container</a></li>
<li class="toctree-l2"><a class="reference internal" href="test.html#run-qa-container">Run QA Container</a></li>
</ul>
</li>
</ul>
<p class="caption"><span class="caption-text">API Reference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="protobuf_api/protobuf_api_root.html">Protobuf API</a><ul>
<li class="toctree-l2"><a class="reference internal" href="protobuf_api/protobuf_api_root.html#http-grpc-api">HTTP/GRPC API</a></li>
<li class="toctree-l2"><a class="reference internal" href="protobuf_api/protobuf_api_root.html#model-configuration">Model Configuration</a></li>
<li class="toctree-l2"><a class="reference internal" href="protobuf_api/protobuf_api_root.html#status">Status</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="cpp_api/cpp_api_root.html">C++ API</a><ul>
<li class="toctree-l2"><a class="reference internal" href="cpp_api/cpp_api_root.html#class-hierarchy">Class Hierarchy</a></li>
<li class="toctree-l2"><a class="reference internal" href="cpp_api/cpp_api_root.html#file-hierarchy">File Hierarchy</a></li>
<li class="toctree-l2"><a class="reference internal" href="cpp_api/cpp_api_root.html#full-api">Full API</a><ul>
<li class="toctree-l3"><a class="reference internal" href="cpp_api/cpp_api_root.html#namespaces">Namespaces</a><ul>
<li class="toctree-l4"><a class="reference internal" href="cpp_api/namespace_nvidia.html">Namespace nvidia</a></li>
<li class="toctree-l4"><a class="reference internal" href="cpp_api/namespace_nvidia__inferenceserver.html">Namespace nvidia::inferenceserver</a></li>
<li class="toctree-l4"><a class="reference internal" href="cpp_api/namespace_nvidia__inferenceserver__client.html">Namespace nvidia::inferenceserver::client</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="cpp_api/cpp_api_root.html#classes-and-structs">Classes and Structs</a><ul>
<li class="toctree-l4"><a class="reference internal" href="cpp_api/structcustom__payload__struct.html">Struct custom_payload_struct</a></li>
<li class="toctree-l4"><a class="reference internal" href="cpp_api/structnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result_1_1ClassResult.html">Struct Result::ClassResult</a></li>
<li class="toctree-l4"><a class="reference internal" href="cpp_api/structnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Stat.html">Struct InferContext::Stat</a></li>
<li class="toctree-l4"><a class="reference internal" href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1Error.html">Class Error</a></li>
<li class="toctree-l4"><a class="reference internal" href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext.html">Class InferContext</a></li>
<li class="toctree-l4"><a class="reference internal" href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Input.html">Class InferContext::Input</a></li>
<li class="toctree-l4"><a class="reference internal" href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Options.html">Class InferContext::Options</a></li>
<li class="toctree-l4"><a class="reference internal" href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Output.html">Class InferContext::Output</a></li>
<li class="toctree-l4"><a class="reference internal" href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Request.html">Class InferContext::Request</a></li>
<li class="toctree-l4"><a class="reference internal" href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1RequestTimers.html">Class InferContext::RequestTimers</a></li>
<li class="toctree-l4"><a class="reference internal" href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result.html">Class InferContext::Result</a></li>
<li class="toctree-l4"><a class="reference internal" href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferGrpcContext.html">Class InferGrpcContext</a></li>
<li class="toctree-l4"><a class="reference internal" href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferGrpcStreamContext.html">Class InferGrpcStreamContext</a></li>
<li class="toctree-l4"><a class="reference internal" href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferHttpContext.html">Class InferHttpContext</a></li>
<li class="toctree-l4"><a class="reference internal" href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ProfileContext.html">Class ProfileContext</a></li>
<li class="toctree-l4"><a class="reference internal" href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ProfileGrpcContext.html">Class ProfileGrpcContext</a></li>
<li class="toctree-l4"><a class="reference internal" href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ProfileHttpContext.html">Class ProfileHttpContext</a></li>
<li class="toctree-l4"><a class="reference internal" href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ServerHealthContext.html">Class ServerHealthContext</a></li>
<li class="toctree-l4"><a class="reference internal" href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ServerHealthGrpcContext.html">Class ServerHealthGrpcContext</a></li>
<li class="toctree-l4"><a class="reference internal" href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ServerHealthHttpContext.html">Class ServerHealthHttpContext</a></li>
<li class="toctree-l4"><a class="reference internal" href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ServerStatusContext.html">Class ServerStatusContext</a></li>
<li class="toctree-l4"><a class="reference internal" href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ServerStatusGrpcContext.html">Class ServerStatusGrpcContext</a></li>
<li class="toctree-l4"><a class="reference internal" href="cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ServerStatusHttpContext.html">Class ServerStatusHttpContext</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="cpp_api/cpp_api_root.html#functions">Functions</a><ul>
<li class="toctree-l4"><a class="reference internal" href="cpp_api/function_custom_8h_1aedfdcb69f8eaab594173b197b10dc804.html">Function CustomErrorString</a></li>
<li class="toctree-l4"><a class="reference internal" href="cpp_api/function_custom_8h_1afc75c0a4e1a169562586c2d4ff09ba40.html">Function CustomExecute</a></li>
<li class="toctree-l4"><a class="reference internal" href="cpp_api/function_custom_8h_1a9cd799dfef77db82babc6ddd4a4be707.html">Function CustomFinalize</a></li>
<li class="toctree-l4"><a class="reference internal" href="cpp_api/function_custom_8h_1ad384f6af9bfb11b82b2b4e411f799b6f.html">Function CustomInitialize</a></li>
<li class="toctree-l4"><a class="reference internal" href="cpp_api/function_request_8h_1a19d352bb13848c14b5f03274b25ecee3.html">Function nvidia::inferenceserver::client::operator&lt;&lt;</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="cpp_api/cpp_api_root.html#defines">Defines</a><ul>
<li class="toctree-l4"><a class="reference internal" href="cpp_api/define_custom_8h_1aaf537da73bb55fda05fb4e39defe1949.html">Define CUSTOM_NO_GPU_DEVICE</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="cpp_api/cpp_api_root.html#typedefs">Typedefs</a><ul>
<li class="toctree-l4"><a class="reference internal" href="cpp_api/typedef_custom_8h_1aa6616d50105d3441ba277c6dbce66286.html">Typedef CustomErrorStringFn_t</a></li>
<li class="toctree-l4"><a class="reference internal" href="cpp_api/typedef_custom_8h_1ac0c3508cd00e9db56912ff40deecf9d2.html">Typedef CustomExecuteFn_t</a></li>
<li class="toctree-l4"><a class="reference internal" href="cpp_api/typedef_custom_8h_1aa32f51734df1f217cc7c2b07f31f8f3f.html">Typedef CustomFinalizeFn_t</a></li>
<li class="toctree-l4"><a class="reference internal" href="cpp_api/typedef_custom_8h_1aee3723f1a17ff4664691655e056a309c.html">Typedef CustomGetNextInputFn_t</a></li>
<li class="toctree-l4"><a class="reference internal" href="cpp_api/typedef_custom_8h_1a71cb419d6e7feee902115438973d02f8.html">Typedef CustomGetOutputFn_t</a></li>
<li class="toctree-l4"><a class="reference internal" href="cpp_api/typedef_custom_8h_1a11fea108abb0b28f86a96e6b7cd92a20.html">Typedef CustomInitializeFn_t</a></li>
<li class="toctree-l4"><a class="reference internal" href="cpp_api/typedef_custom_8h_1a99a7b9dcc756702784a869d9afb59df4.html">Typedef CustomPayload</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="cpp_api/cpp_api_root.html#directories">Directories</a><ul>
<li class="toctree-l4"><a class="reference internal" href="cpp_api/dir_src.html">Directory src</a></li>
<li class="toctree-l4"><a class="reference internal" href="cpp_api/dir_src_clients.html">Directory clients</a></li>
<li class="toctree-l4"><a class="reference internal" href="cpp_api/dir_src_clients_c++.html">Directory c++</a></li>
<li class="toctree-l4"><a class="reference internal" href="cpp_api/dir_src_servables.html">Directory servables</a></li>
<li class="toctree-l4"><a class="reference internal" href="cpp_api/dir_src_servables_custom.html">Directory custom</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="cpp_api/cpp_api_root.html#files">Files</a><ul>
<li class="toctree-l4"><a class="reference internal" href="cpp_api/file_src_servables_custom_custom.h.html">File custom.h</a></li>
<li class="toctree-l4"><a class="reference internal" href="cpp_api/file_src_clients_c++_request.h.html">File request.h</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="python_api.html">Python API</a><ul>
<li class="toctree-l2"><a class="reference internal" href="python_api.html#client">Client</a></li>
</ul>
</li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">NVIDIA TensorRT Inference Server</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html">Docs</a> &raquo;</li>
        
      <li>Model Repository</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="_sources/model_repository.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  
<style>
/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast,
.nboutput.nblast {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast + .nbinput {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<div class="section" id="model-repository">
<span id="section-model-repository"></span><h1>Model Repository<a class="headerlink" href="#model-repository" title="Permalink to this headline">¶</a></h1>
<p>The TensorRT Inference Server accesses models from a locally
accessible file path or from Google Cloud Storage. This path is
specified when the server is started using the --model-store option.</p>
<p>For a locally accessible file-system the absolute path must be
specified, for example, --model-store=/path/to/model/repository. For
a model repository residing in Google Cloud Storage, the path must be
prefixed with gs://, for example,
--model-store=gs://bucket/path/to/model/repository.</p>
<p><a class="reference internal" href="run.html#section-example-model-repository"><span class="std std-ref">Example Model Repository</span></a> describes how to create an
example repository with a couple if image classification models.</p>
<p>An example of a typical model repository layout is shown below:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>&lt;model-repository-path&gt;/
  model_0/
    config.pbtxt
    output0_labels.txt
    1/
      model.plan
    2/
      model.plan
  model_1/
    config.pbtxt
    output0_labels.txt
    output1_labels.txt
    0/
      model.graphdef
    7/
      model.graphdef
</pre></div>
</div>
<p>Any number of models may be specified and the inference server will
attempt to load all models into the CPU and GPU when the server
starts. The <a class="reference internal" href="http_grpc_api.html#section-api-status"><span class="std std-ref">Status API</span></a> can be used to
determine if any models failed to load successfully. The server’s
console log will also show the reason for any failures during startup.</p>
<p>The name of the model directory (model_0 and model_1 in the above
example) must match the name of the model specified in the
<a class="reference internal" href="model_configuration.html#section-model-configuration"><span class="std std-ref">model configuration file</span></a>,
config.pbtxt. The model name is used in the <a class="reference internal" href="client.html#section-client-api"><span class="std std-ref">client API</span></a> and <a class="reference internal" href="http_grpc_api.html#section-inference-server-api"><span class="std std-ref">server API</span></a> to identify the model. Each model
directory must have at least one numeric subdirectory. Each of these
subdirectories holds a version of the model with the version number
corresponding to the directory name.</p>
<p>For more information about how the model versions are handled by the
server see <a class="reference internal" href="#section-model-versions"><span class="std std-ref">Model Versions</span></a>.  Within each version
subdirectory there are one or more model definition files that specify
the actual model. The model definition can be either a
<a class="reference internal" href="#section-framework-model-definition"><span class="std std-ref">framework-specific model file</span></a> or a shared library implementing
a <a class="reference internal" href="#section-custom-backends"><span class="std std-ref">custom backend</span></a>.</p>
<p>The *_labels.txt files are optional and are used to provide labels for
outputs that represent classifications. The label file must be
specified in the <a class="reference internal" href="protobuf_api/model_config.proto.html#_CPPv4N6nvidia15inferenceserver11ModelOutput14label_filenameE" title="nvidia::inferenceserver::ModelOutput::label_filename"><code class="xref cpp cpp-var docutils literal notranslate"><span class="pre">label_filename</span></code></a> property of
the output it corresponds to in the <a class="reference internal" href="model_configuration.html#section-model-configuration"><span class="std std-ref">model configuration</span></a>.</p>
<div class="section" id="modifying-the-model-repository">
<span id="section-modifying-the-model-repository"></span><h2>Modifying the Model Repository<a class="headerlink" href="#modifying-the-model-repository" title="Permalink to this headline">¶</a></h2>
<p>By default, changes to the model repository will be detected and the
server will attempt to add, remove, and reload models as necessary
based on those changes. Changes to the model repository may not be
detected immediately because the server polls the repository
periodically. You can control the polling interval with the
--repository-poll-secs options. The console log or the <a class="reference internal" href="http_grpc_api.html#section-api-status"><span class="std std-ref">Status
API</span></a> can be used to determine when model
repository changes have taken effect. You can disable the server from
responding to repository changes by using the
--allow-poll-model-repository=false option.</p>
<p>The TensorRT Inference Server responds to the following changes:</p>
<ul class="simple">
<li>Versions may be added and removed from models by adding and removing
the corresponding version subdirectory. The inference server will
allow in-flight requests to complete even if they are using a
removed version of the model. New requests for a removed model
version will fail. Depending on the model’s <a class="reference internal" href="model_configuration.html#section-version-policy"><span class="std std-ref">version policy</span></a>, changes to the available versions may
change which model version is served by default.</li>
<li>Existing models can be removed from the repository by removing the
corresponding model directory.  The inference server will allow
in-flight requests to any version of the removed model to
complete. New requests for a removed model will fail.</li>
<li>New models can be added to the repository by adding a new model
directory.</li>
<li>The <a class="reference internal" href="model_configuration.html#section-model-configuration"><span class="std std-ref">model configuration</span></a>
(config.pbtxt) can be changed and the server will unload and reload
the model to pick up the new model configuration.</li>
<li>Labels files providing labels for outputs that represent
classifications can be added, removed, or modified and the inference
server will unload and reload the model to pick up the new
labels. If a label file is added or removed the corresponding edit
to the <a class="reference internal" href="protobuf_api/model_config.proto.html#_CPPv4N6nvidia15inferenceserver11ModelOutput14label_filenameE" title="nvidia::inferenceserver::ModelOutput::label_filename"><code class="xref cpp cpp-var docutils literal notranslate"><span class="pre">label_filename</span></code></a> property of
the output it corresponds to in the <a class="reference internal" href="model_configuration.html#section-model-configuration"><span class="std std-ref">model configuration</span></a> must be performed at the same time.</li>
</ul>
</div>
<div class="section" id="model-versions">
<span id="section-model-versions"></span><h2>Model Versions<a class="headerlink" href="#model-versions" title="Permalink to this headline">¶</a></h2>
<p>Each model can have one or more versions available in the model
repository. Each version is stored in its own, numerically named,
subdirectory where the name of the subdirectory corresponds to the
version number of the model. Each model specifies a <a class="reference internal" href="model_configuration.html#section-version-policy"><span class="std std-ref">version
policy</span></a> that controls which of the versions
in the model repository are made available by the server at any given
time.</p>
</div>
<div class="section" id="framework-model-definition">
<span id="section-framework-model-definition"></span><h2>Framework Model Definition<a class="headerlink" href="#framework-model-definition" title="Permalink to this headline">¶</a></h2>
<p>Each model version subdirectory must contain at least one model
definition. By default, the name of this file or directory must be:</p>
<ul class="simple">
<li><strong>model.plan</strong> for TensorRT models</li>
<li><strong>model.graphdef</strong> for TensorFlow GraphDef models</li>
<li><strong>model.savedmodel</strong> for TensorFlow SavedModel models</li>
<li><strong>model.netdef</strong> and <strong>init_model.netdef</strong> for Caffe2 Netdef models</li>
</ul>
<p>This default name can be overridden using the <em>default_model_filename</em>
property in the <a class="reference internal" href="model_configuration.html#section-model-configuration"><span class="std std-ref">model configuration</span></a>.</p>
<p>Optionally, a model can provide multiple model definition files, each
targeted at a GPU with a different <a class="reference external" href="https://developer.nvidia.com/cuda-gpus">Compute Capability</a>. Most commonly, this
feature is needed for TensorRT and TensorFlow/TensorRT integrated
models where the model definition is valid for only a single compute
capability. See the <em>cc_model_filenames</em> property in the <a class="reference internal" href="model_configuration.html#section-model-configuration"><span class="std std-ref">model
configuration</span></a> for description of how to
specify different model definitions for different compute
capabilities.</p>
<div class="section" id="tensorrt-models">
<span id="section-tensorrt-models"></span><h3>TensorRT Models<a class="headerlink" href="#tensorrt-models" title="Permalink to this headline">¶</a></h3>
<p>A TensorRT model definition is called a <em>Plan</em>. A TensorRT Plan is a
single file that by default must be named model.plan. A TensorRT Plan
is specific to CUDA Compute Capability and so it is typically
necessary to use the <a class="reference internal" href="model_configuration.html#section-model-configuration"><span class="std std-ref">model configuration’s</span></a> <em>cc_model_filenames</em> property as
described above.</p>
<p>A minimal model repository for a single TensorRT model would look
like:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>models/
  &lt;model-name&gt;/
    config.pbtxt
    1/
      model.plan
</pre></div>
</div>
<p>As described in <a class="reference internal" href="model_configuration.html#section-generated-model-configuration"><span class="std std-ref">Generated Model Configuration</span></a> the
config.pbtxt is optional for some models. In cases where it is not
required the minimal model repository would look like:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>models/
  &lt;model-name&gt;/
    1/
      model.plan
</pre></div>
</div>
</div>
<div class="section" id="tensorflow-models">
<span id="section-tensorflow-models"></span><h3>TensorFlow Models<a class="headerlink" href="#tensorflow-models" title="Permalink to this headline">¶</a></h3>
<p>TensorFlow saves trained models in one of two ways: <em>GraphDef</em> or
<em>SavedModel</em>. The inference server supports both formats. Once you
have a trained model in TensorFlow, you can save it as a GraphDef
directly or convert it to a GraphDef by using a script like
<a class="reference external" href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/tools/freeze_graph.py">freeze_graph.py</a>,
or save it as a SavedModel using a <a class="reference external" href="https://www.tensorflow.org/serving/serving_basic">SavedModelBuilder</a> or
<a class="reference external" href="https://www.tensorflow.org/api_docs/python/tf/saved_model/simple_save">tf.saved_model.simple_save</a>.</p>
<p>A TensorFlow GraphDef is a single file that by default must be named
model.graphdef. A minimal model repository for a single TensorFlow
GraphDef model would look like:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>models/
  &lt;model-name&gt;/
    config.pbtxt
    1/
      model.graphdef
</pre></div>
</div>
<p>A TensorFlow SavedModel is a directory containing multiple files. By
default the directory must be named model.savedmodel. A minimal model
repository for a single TensorFlow SavedModel model would look like:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>models/
  &lt;model-name&gt;/
    config.pbtxt
    1/
      model.savedmodel/
         &lt;saved-model files&gt;
</pre></div>
</div>
<p>As described in <a class="reference internal" href="model_configuration.html#section-generated-model-configuration"><span class="std std-ref">Generated Model Configuration</span></a> the
config.pbtxt is optional for some models. In cases where it is not
required the minimal model repository would look like:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>models/
  &lt;model-name&gt;/
    1/
      model.savedmodel/
         &lt;saved-model files&gt;
</pre></div>
</div>
</div>
<div class="section" id="caffe2-models">
<h3>Caffe2 Models<a class="headerlink" href="#caffe2-models" title="Permalink to this headline">¶</a></h3>
<p>A Caffe2 model definition is called a <em>NetDef</em>. A Caffe2 NetDef is a
single file that by default must be named model.netdef. A minimal
model repository for a single NetDef model would look like:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>models/
  &lt;model-name&gt;/
    config.pbtxt
    1/
      model.netdef
</pre></div>
</div>
</div>
<div class="section" id="tensorrt-tensorflow-models">
<h3>TensorRT/TensorFlow Models<a class="headerlink" href="#tensorrt-tensorflow-models" title="Permalink to this headline">¶</a></h3>
<p>TensorFlow 1.7 and later integrates TensorRT to enable TensorFlow
models to benefit from the inference optimizations provided by
TensorRT. The inference server supports models that have been
optimized with TensorRT and can serve those models just like any other
TensorFlow model. The inference server’s TensorRT version (available
in the Release Notes) must match the TensorRT version that was used
when the model was created.</p>
<p>A TensorRT/TensorFlow integrated model is specific to CUDA Compute
Capability and so it is typically necessary to use the <a class="reference internal" href="model_configuration.html#section-model-configuration"><span class="std std-ref">model
configuration’s</span></a> <em>cc_model_filenames</em>
property as described above.</p>
</div>
<div class="section" id="onnx-models">
<h3>ONNX Models<a class="headerlink" href="#onnx-models" title="Permalink to this headline">¶</a></h3>
<p>The TensorRT Inference Server cannot directly perform inferencing
using <a class="reference external" href="http://onnx.ai/">ONNX</a> models. An ONNX model must be
converted to either a TensorRT Plan or a Caffe2 NetDef. To convert
your ONNX model to a TensorRT Plan use either the <a class="reference external" href="https://docs.nvidia.com/deeplearning/sdk/tensorrt-developer-guide/index.html#api">ONNX Parser</a>
included in TensorRT or the <a class="reference external" href="https://github.com/onnx/onnx-tensorrt">open-source TensorRT backend for ONNX</a>. Another option is to
convert your ONNX model to Caffe2 NetDef <a class="reference external" href="https://github.com/pytorch/pytorch/tree/master/caffe2/python/onnx">as described here</a>.</p>
</div>
</div>
<div class="section" id="custom-backends">
<span id="section-custom-backends"></span><h2>Custom Backends<a class="headerlink" href="#custom-backends" title="Permalink to this headline">¶</a></h2>
<p>A model using a custom backend is represented in the model repository
in the same way as models using a deep-learning framework backend.
Each model version subdirectory must contain at least one shared
library that implements the custom model backend. By default, the name
of this shared library must be <strong>libcustom.so</strong> but the default name
can be overridden using the <em>default_model_filename</em> property in the
<a class="reference internal" href="model_configuration.html#section-model-configuration"><span class="std std-ref">model configuration</span></a>.</p>
<p>Optionally, a model can provide multiple shared libraries, each
targeted at a GPU with a different <a class="reference external" href="https://developer.nvidia.com/cuda-gpus">Compute Capability</a>. See the
<em>cc_model_filenames</em> property in the <a class="reference internal" href="model_configuration.html#section-model-configuration"><span class="std std-ref">model configuration</span></a> for description of how to specify
different shared libraries for different compute capabilities.</p>
<div class="section" id="custom-backend-api">
<h3>Custom Backend API<a class="headerlink" href="#custom-backend-api" title="Permalink to this headline">¶</a></h3>
<p>A custom backend must implement the C interface defined in <a class="reference external" href="https://github.com/NVIDIA/tensorrt-inference-server/blob/master/src/servables/custom/custom.h">custom.h</a>. The
interface is also documented in the <a class="reference external" href="file:///home/david/dev/github/NVIDIA/tensorrt-inference-server/docs/build/html/cpp_api/file_src_servables_custom_custom.h.html">API Reference</a>.</p>
</div>
<div class="section" id="example-custom-backend">
<h3>Example Custom Backend<a class="headerlink" href="#example-custom-backend" title="Permalink to this headline">¶</a></h3>
<p>An example of a custom backend can be found in the <a class="reference external" href="https://github.com/NVIDIA/tensorrt-inference-server/blob/master/src/custom/addsub/addsub.cc">addsub backend</a>. You
can see the custom backend being used as part of CI testing in
<a class="reference external" href="https://github.com/NVIDIA/tensorrt-inference-server/tree/master/qa/L0_infer">L0_infer</a>.</p>
</div>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="model_configuration.html" class="btn btn-neutral float-right" title="Model Configuration" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="client.html" class="btn btn-neutral float-left" title="Client Libraries and Examples" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2018, NVIDIA Corporation

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
    

  <style>
  a:link, a:visited {
    color: #76b900;
  }

  a:hover {
    color: #8c0;
  }

  .rst-content dl:not(.docutils) dt {
    background: rgba(118, 185, 0, 0.1);
    color: rgba(59,93,0,1);
    border-top: solid 3px rgba(59,93,0,1);
  }
  </style>
  

</body>
</html>