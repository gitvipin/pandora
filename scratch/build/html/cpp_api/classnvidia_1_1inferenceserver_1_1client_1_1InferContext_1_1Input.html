<!--
# Copyright (c) 2018, NVIDIA CORPORATION. All rights reserved.
#
# Redistribution and use in source and binary forms, with or without
# modification, are permitted provided that the following conditions
# are met:
#  * Redistributions of source code must retain the above copyright
#    notice, this list of conditions and the following disclaimer.
#  * Redistributions in binary form must reproduce the above copyright
#    notice, this list of conditions and the following disclaimer in the
#    documentation and/or other materials provided with the distribution.
#  * Neither the name of NVIDIA CORPORATION nor the names of its
#    contributors may be used to endorse or promote products derived
#    from this software without specific prior written permission.
#
# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
# EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
# PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
# PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
# OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
# (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
# OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
-->


<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Class InferContext::Input &mdash; NVIDIA TensorRT Inference Server 0.12.0dev
 documentation</title>
  

  
  
  
  
    <link rel="canonical" href="https://docs.nvidia.com/deeplearning/sdk/tensorrt-inference-server-guide/docs/index.htmlcpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Input.html"/>
  

  
  <script type="text/javascript" src="../_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script type="text/javascript" src="../_static/jquery.js"></script>
        <script type="text/javascript" src="../_static/underscore.js"></script>
        <script type="text/javascript" src="../_static/doctools.js"></script>
        <script type="text/javascript" src="../_static/language_data.js"></script>
        <script type="text/javascript" src="../_static/collapsible-lists/js/CollapsibleLists.compressed.js"></script>
        <script type="text/javascript" src="../_static/collapsible-lists/js/apply-collapsible-lists.js"></script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/collapsible-lists/css/tree_view.css" type="text/css" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Class InferContext::Options" href="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Options.html" />
    <link rel="prev" title="Class InferContext" href="classnvidia_1_1inferenceserver_1_1client_1_1InferContext.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
           

          
            <a href="../index.html" class="icon icon-home"> NVIDIA TensorRT Inference Server
          

          
          </a>

          
            
            
              <div class="version">
                0.12.0dev
-ad20b82<br/>
Version select: <select onChange="window.location.href = this.value" onFocus="this.selectedIndex = -1">
    <option value="https://docs.nvidia.com/deeplearning/sdk/tensorrt-inference-server-guide/docs/index.html">Current release</option>
    <option value="https://docs.nvidia.com/deeplearning/sdk/tensorrt-inference-server-master-branch-guide/docs/index.html">master (unstable)</option>
    <option value="https://docs.nvidia.com/deeplearning/sdk/inference-server-archived/index.html">Older releases</option>
</select>
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          

  <style>
    /* Sidebar header (and topbar for mobile) */
    .wy-side-nav-search, .wy-nav-top {
      background: #76b900;
    }

    .wy-side-nav-search a:link, .wy-nav-top a:link {
      color: #fff;
    }
    .wy-side-nav-search a:visited, .wy-nav-top a:visited {
      color: #fff;
    }
    .wy-side-nav-search a:hover, .wy-nav-top a:hover {
      color: #fff;
    }

    .wy-menu-vertical a:link, .wy-menu-vertical a:visited {
      color: #d9d9d9
    }

    .wy-menu-vertical a:active {
      background-color: #76b900
    }

    .wy-side-nav-search>div.version {
      color: rgba(0, 0, 0, 0.3)
    }
  </style>
  
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul>
<li class="toctree-l1"><a class="reference internal" href="../index.html">Documentation home</a></li>
</ul>
<p class="caption"><span class="caption-text">User Guide</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../quickstart.html">Quickstart</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../quickstart.html#prerequisites">Prerequisites</a></li>
<li class="toctree-l2"><a class="reference internal" href="../quickstart.html#using-a-prebuilt-docker-container">Using A Prebuilt Docker Container</a></li>
<li class="toctree-l2"><a class="reference internal" href="../quickstart.html#building-from-source-code">Building From Source Code</a></li>
<li class="toctree-l2"><a class="reference internal" href="../quickstart.html#run-tensorrt-inference-server">Run TensorRT Inference Server</a></li>
<li class="toctree-l2"><a class="reference internal" href="../quickstart.html#verify-inference-server-is-running-correctly">Verify Inference Server Is Running Correctly</a></li>
<li class="toctree-l2"><a class="reference internal" href="../quickstart.html#building-the-client-examples">Building The Client Examples</a></li>
<li class="toctree-l2"><a class="reference internal" href="../quickstart.html#running-the-image-classification-example">Running The Image Classification Example</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../install.html">Installing the Server</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../install.html#installing-prebuilt-containers">Installing Prebuilt Containers</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../run.html">Running the Server</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../run.html#example-model-repository">Example Model Repository</a></li>
<li class="toctree-l2"><a class="reference internal" href="../run.html#running-the-inference-server">Running The Inference Server</a></li>
<li class="toctree-l2"><a class="reference internal" href="../run.html#running-the-inference-server-on-a-system-without-a-gpu">Running The Inference Server On A System Without A GPU</a></li>
<li class="toctree-l2"><a class="reference internal" href="../run.html#checking-inference-server-status">Checking Inference Server Status</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../client.html">Client Libraries and Examples</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../client.html#building-the-client-libraries-and-examples">Building the Client Libraries and Examples</a></li>
<li class="toctree-l2"><a class="reference internal" href="../client.html#image-classification-example-application">Image Classification Example Application</a></li>
<li class="toctree-l2"><a class="reference internal" href="../client.html#performance-example-application">Performance Example Application</a></li>
<li class="toctree-l2"><a class="reference internal" href="../client.html#client-api">Client API</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../client.html#string-datatype">String Datatype</a></li>
<li class="toctree-l3"><a class="reference internal" href="../client.html#stream-inference">Stream Inference</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../model_repository.html">Model Repository</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../model_repository.html#modifying-the-model-repository">Modifying the Model Repository</a></li>
<li class="toctree-l2"><a class="reference internal" href="../model_repository.html#model-versions">Model Versions</a></li>
<li class="toctree-l2"><a class="reference internal" href="../model_repository.html#framework-model-definition">Framework Model Definition</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../model_repository.html#tensorrt-models">TensorRT Models</a></li>
<li class="toctree-l3"><a class="reference internal" href="../model_repository.html#tensorflow-models">TensorFlow Models</a></li>
<li class="toctree-l3"><a class="reference internal" href="../model_repository.html#caffe2-models">Caffe2 Models</a></li>
<li class="toctree-l3"><a class="reference internal" href="../model_repository.html#tensorrt-tensorflow-models">TensorRT/TensorFlow Models</a></li>
<li class="toctree-l3"><a class="reference internal" href="../model_repository.html#onnx-models">ONNX Models</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../model_repository.html#custom-backends">Custom Backends</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../model_repository.html#custom-backend-api">Custom Backend API</a></li>
<li class="toctree-l3"><a class="reference internal" href="../model_repository.html#example-custom-backend">Example Custom Backend</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../model_configuration.html">Model Configuration</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../model_configuration.html#generated-model-configuration">Generated Model Configuration</a></li>
<li class="toctree-l2"><a class="reference internal" href="../model_configuration.html#datatypes">Datatypes</a></li>
<li class="toctree-l2"><a class="reference internal" href="../model_configuration.html#version-policy">Version Policy</a></li>
<li class="toctree-l2"><a class="reference internal" href="../model_configuration.html#instance-groups">Instance Groups</a></li>
<li class="toctree-l2"><a class="reference internal" href="../model_configuration.html#dynamic-batching">Dynamic Batching</a></li>
<li class="toctree-l2"><a class="reference internal" href="../model_configuration.html#optimization-policy">Optimization Policy</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../http_grpc_api.html">Inference Server API</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../http_grpc_api.html#health">Health</a></li>
<li class="toctree-l2"><a class="reference internal" href="../http_grpc_api.html#status">Status</a></li>
<li class="toctree-l2"><a class="reference internal" href="../http_grpc_api.html#inference">Inference</a></li>
<li class="toctree-l2"><a class="reference internal" href="../http_grpc_api.html#stream-inference">Stream Inference</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../metrics.html">Metrics</a></li>
</ul>
<p class="caption"><span class="caption-text">Developer Guide</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../architecture.html">Architecture</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../architecture.html#concurrent-model-execution">Concurrent Model Execution</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../contribute.html">Contributing</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../contribute.html#coding-convention">Coding Convention</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../build.html">Building</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../build.html#building-the-server">Building the Server</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../build.html#incremental-builds">Incremental Builds</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../build.html#building-the-client-libraries-and-examples">Building the Client Libraries and Examples</a></li>
<li class="toctree-l2"><a class="reference internal" href="../build.html#building-the-documentation">Building the Documentation</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../test.html">Testing</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../test.html#generate-qa-model-repositories">Generate QA Model Repositories</a></li>
<li class="toctree-l2"><a class="reference internal" href="../test.html#build-qa-container">Build QA Container</a></li>
<li class="toctree-l2"><a class="reference internal" href="../test.html#run-qa-container">Run QA Container</a></li>
</ul>
</li>
</ul>
<p class="caption"><span class="caption-text">API Reference</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../protobuf_api/protobuf_api_root.html">Protobuf API</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../protobuf_api/protobuf_api_root.html#http-grpc-api">HTTP/GRPC API</a></li>
<li class="toctree-l2"><a class="reference internal" href="../protobuf_api/protobuf_api_root.html#model-configuration">Model Configuration</a></li>
<li class="toctree-l2"><a class="reference internal" href="../protobuf_api/protobuf_api_root.html#status">Status</a></li>
</ul>
</li>
<li class="toctree-l1 current"><a class="reference internal" href="cpp_api_root.html">C++ API</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="cpp_api_root.html#class-hierarchy">Class Hierarchy</a></li>
<li class="toctree-l2"><a class="reference internal" href="cpp_api_root.html#file-hierarchy">File Hierarchy</a></li>
<li class="toctree-l2 current"><a class="reference internal" href="cpp_api_root.html#full-api">Full API</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="cpp_api_root.html#namespaces">Namespaces</a><ul>
<li class="toctree-l4"><a class="reference internal" href="namespace_nvidia.html">Namespace nvidia</a></li>
<li class="toctree-l4"><a class="reference internal" href="namespace_nvidia__inferenceserver.html">Namespace nvidia::inferenceserver</a></li>
<li class="toctree-l4"><a class="reference internal" href="namespace_nvidia__inferenceserver__client.html">Namespace nvidia::inferenceserver::client</a></li>
</ul>
</li>
<li class="toctree-l3 current"><a class="reference internal" href="cpp_api_root.html#classes-and-structs">Classes and Structs</a><ul class="current">
<li class="toctree-l4"><a class="reference internal" href="structcustom__payload__struct.html">Struct custom_payload_struct</a></li>
<li class="toctree-l4"><a class="reference internal" href="structnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result_1_1ClassResult.html">Struct Result::ClassResult</a></li>
<li class="toctree-l4"><a class="reference internal" href="structnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Stat.html">Struct InferContext::Stat</a></li>
<li class="toctree-l4"><a class="reference internal" href="classnvidia_1_1inferenceserver_1_1client_1_1Error.html">Class Error</a></li>
<li class="toctree-l4"><a class="reference internal" href="classnvidia_1_1inferenceserver_1_1client_1_1InferContext.html">Class InferContext</a></li>
<li class="toctree-l4 current"><a class="current reference internal" href="#">Class InferContext::Input</a></li>
<li class="toctree-l4"><a class="reference internal" href="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Options.html">Class InferContext::Options</a></li>
<li class="toctree-l4"><a class="reference internal" href="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Output.html">Class InferContext::Output</a></li>
<li class="toctree-l4"><a class="reference internal" href="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Request.html">Class InferContext::Request</a></li>
<li class="toctree-l4"><a class="reference internal" href="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1RequestTimers.html">Class InferContext::RequestTimers</a></li>
<li class="toctree-l4"><a class="reference internal" href="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result.html">Class InferContext::Result</a></li>
<li class="toctree-l4"><a class="reference internal" href="classnvidia_1_1inferenceserver_1_1client_1_1InferGrpcContext.html">Class InferGrpcContext</a></li>
<li class="toctree-l4"><a class="reference internal" href="classnvidia_1_1inferenceserver_1_1client_1_1InferGrpcStreamContext.html">Class InferGrpcStreamContext</a></li>
<li class="toctree-l4"><a class="reference internal" href="classnvidia_1_1inferenceserver_1_1client_1_1InferHttpContext.html">Class InferHttpContext</a></li>
<li class="toctree-l4"><a class="reference internal" href="classnvidia_1_1inferenceserver_1_1client_1_1ProfileContext.html">Class ProfileContext</a></li>
<li class="toctree-l4"><a class="reference internal" href="classnvidia_1_1inferenceserver_1_1client_1_1ProfileGrpcContext.html">Class ProfileGrpcContext</a></li>
<li class="toctree-l4"><a class="reference internal" href="classnvidia_1_1inferenceserver_1_1client_1_1ProfileHttpContext.html">Class ProfileHttpContext</a></li>
<li class="toctree-l4"><a class="reference internal" href="classnvidia_1_1inferenceserver_1_1client_1_1ServerHealthContext.html">Class ServerHealthContext</a></li>
<li class="toctree-l4"><a class="reference internal" href="classnvidia_1_1inferenceserver_1_1client_1_1ServerHealthGrpcContext.html">Class ServerHealthGrpcContext</a></li>
<li class="toctree-l4"><a class="reference internal" href="classnvidia_1_1inferenceserver_1_1client_1_1ServerHealthHttpContext.html">Class ServerHealthHttpContext</a></li>
<li class="toctree-l4"><a class="reference internal" href="classnvidia_1_1inferenceserver_1_1client_1_1ServerStatusContext.html">Class ServerStatusContext</a></li>
<li class="toctree-l4"><a class="reference internal" href="classnvidia_1_1inferenceserver_1_1client_1_1ServerStatusGrpcContext.html">Class ServerStatusGrpcContext</a></li>
<li class="toctree-l4"><a class="reference internal" href="classnvidia_1_1inferenceserver_1_1client_1_1ServerStatusHttpContext.html">Class ServerStatusHttpContext</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="cpp_api_root.html#functions">Functions</a><ul>
<li class="toctree-l4"><a class="reference internal" href="function_custom_8h_1aedfdcb69f8eaab594173b197b10dc804.html">Function CustomErrorString</a></li>
<li class="toctree-l4"><a class="reference internal" href="function_custom_8h_1afc75c0a4e1a169562586c2d4ff09ba40.html">Function CustomExecute</a></li>
<li class="toctree-l4"><a class="reference internal" href="function_custom_8h_1a9cd799dfef77db82babc6ddd4a4be707.html">Function CustomFinalize</a></li>
<li class="toctree-l4"><a class="reference internal" href="function_custom_8h_1ad384f6af9bfb11b82b2b4e411f799b6f.html">Function CustomInitialize</a></li>
<li class="toctree-l4"><a class="reference internal" href="function_request_8h_1a19d352bb13848c14b5f03274b25ecee3.html">Function nvidia::inferenceserver::client::operator&lt;&lt;</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="cpp_api_root.html#defines">Defines</a><ul>
<li class="toctree-l4"><a class="reference internal" href="define_custom_8h_1aaf537da73bb55fda05fb4e39defe1949.html">Define CUSTOM_NO_GPU_DEVICE</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="cpp_api_root.html#typedefs">Typedefs</a><ul>
<li class="toctree-l4"><a class="reference internal" href="typedef_custom_8h_1aa6616d50105d3441ba277c6dbce66286.html">Typedef CustomErrorStringFn_t</a></li>
<li class="toctree-l4"><a class="reference internal" href="typedef_custom_8h_1ac0c3508cd00e9db56912ff40deecf9d2.html">Typedef CustomExecuteFn_t</a></li>
<li class="toctree-l4"><a class="reference internal" href="typedef_custom_8h_1aa32f51734df1f217cc7c2b07f31f8f3f.html">Typedef CustomFinalizeFn_t</a></li>
<li class="toctree-l4"><a class="reference internal" href="typedef_custom_8h_1aee3723f1a17ff4664691655e056a309c.html">Typedef CustomGetNextInputFn_t</a></li>
<li class="toctree-l4"><a class="reference internal" href="typedef_custom_8h_1a71cb419d6e7feee902115438973d02f8.html">Typedef CustomGetOutputFn_t</a></li>
<li class="toctree-l4"><a class="reference internal" href="typedef_custom_8h_1a11fea108abb0b28f86a96e6b7cd92a20.html">Typedef CustomInitializeFn_t</a></li>
<li class="toctree-l4"><a class="reference internal" href="typedef_custom_8h_1a99a7b9dcc756702784a869d9afb59df4.html">Typedef CustomPayload</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="cpp_api_root.html#directories">Directories</a><ul>
<li class="toctree-l4"><a class="reference internal" href="dir_src.html">Directory src</a></li>
<li class="toctree-l4"><a class="reference internal" href="dir_src_clients.html">Directory clients</a></li>
<li class="toctree-l4"><a class="reference internal" href="dir_src_clients_c++.html">Directory c++</a></li>
<li class="toctree-l4"><a class="reference internal" href="dir_src_servables.html">Directory servables</a></li>
<li class="toctree-l4"><a class="reference internal" href="dir_src_servables_custom.html">Directory custom</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="cpp_api_root.html#files">Files</a><ul>
<li class="toctree-l4"><a class="reference internal" href="file_src_servables_custom_custom.h.html">File custom.h</a></li>
<li class="toctree-l4"><a class="reference internal" href="file_src_clients_c++_request.h.html">File request.h</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../python_api.html">Python API</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../python_api.html#client">Client</a></li>
</ul>
</li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">NVIDIA TensorRT Inference Server</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html">Docs</a> &raquo;</li>
        
          <li><a href="cpp_api_root.html">C++ API</a> &raquo;</li>
        
      <li>Class InferContext::Input</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../_sources/cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Input.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  
<style>
/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast,
.nboutput.nblast {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast + .nbinput {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<div class="section" id="class-infercontext-input">
<span id="exhale-class-classnvidia-1-1inferenceserver-1-1client-1-1infercontext-1-1input"></span><h1>Class InferContext::Input<a class="headerlink" href="#class-infercontext-input" title="Permalink to this headline">¶</a></h1>
<ul class="simple">
<li>Defined in <a class="reference internal" href="file_src_clients_c++_request.h.html#file-src-clients-c-request-h"><span class="std std-ref">File request.h</span></a></li>
</ul>
<div class="section" id="nested-relationships">
<h2>Nested Relationships<a class="headerlink" href="#nested-relationships" title="Permalink to this headline">¶</a></h2>
<p>This class is a nested type of <a class="reference internal" href="classnvidia_1_1inferenceserver_1_1client_1_1InferContext.html#exhale-class-classnvidia-1-1inferenceserver-1-1client-1-1infercontext"><span class="std std-ref">Class InferContext</span></a>.</p>
</div>
<div class="section" id="class-documentation">
<h2>Class Documentation<a class="headerlink" href="#class-documentation" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="_CPPv4N6nvidia15inferenceserver6client12InferContext5InputE">
<span class="target" id="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Input"></span><em class="property">class </em><code class="descname">Input</code><br /></dt>
<dd><p>An input to the model. </p>
<div class="breathe-sectiondef docutils container">
<p class="breathe-sectiondef-title rubric">Public Functions</p>
<dl class="function">
<dt id="_CPPv4N6nvidia15inferenceserver6client12InferContext5InputD0Ev">
<span class="target" id="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Input_1aeb32510fa7e86063d3341302ff5cc322"></span><em class="property">virtual</em> <code class="descname">~Input</code><span class="sig-paren">(</span><span class="sig-paren">)</span><br /></dt>
<dd><p>Destroy the input. </p>
</dd></dl>

<dl class="function">
<dt id="_CPPv4NK6nvidia15inferenceserver6client12InferContext5Input4NameEv">
<span class="target" id="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Input_1a66b20a9167b8eeb9ddde10430b37fd01"></span><em class="property">virtual</em> <em class="property">const</em> std::string &amp;<code class="descname">Name</code><span class="sig-paren">(</span><span class="sig-paren">)</span> <em class="property">const</em> = 0<br /></dt>
<dd><p><dl class="docutils">
<dt><strong>Return</strong></dt>
<dd>The name of the input. </dd>
</dl>
</p>
</dd></dl>

<dl class="function">
<dt id="_CPPv4NK6nvidia15inferenceserver6client12InferContext5Input8ByteSizeEv">
<span class="target" id="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Input_1a34951a4f1c906bf3c147ee84549d8667"></span><em class="property">virtual</em> int64_t <code class="descname">ByteSize</code><span class="sig-paren">(</span><span class="sig-paren">)</span> <em class="property">const</em> = 0<br /></dt>
<dd><p><dl class="docutils">
<dt><strong>Return</strong></dt>
<dd>The size in bytes of this input. This is the size for one instance of the input, not the entire size of a batched input. When the byte-size is not known, for example for non-fixed-sized types like TYPE_STRING or for inputs with variable-size dimensions, this will return -1. </dd>
</dl>
</p>
</dd></dl>

<dl class="function">
<dt id="_CPPv4NK6nvidia15inferenceserver6client12InferContext5Input13TotalByteSizeEv">
<span class="target" id="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Input_1a07c715257ed5ed2feab0982d9c6fd941"></span><em class="property">virtual</em> size_t <code class="descname">TotalByteSize</code><span class="sig-paren">(</span><span class="sig-paren">)</span> <em class="property">const</em> = 0<br /></dt>
<dd><p><dl class="docutils">
<dt><strong>Return</strong></dt>
<dd>The size in bytes of entire batch of this input. For fixed-sized types this is just <a class="reference internal" href="#classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Input_1a34951a4f1c906bf3c147ee84549d8667"><span class="std std-ref">ByteSize()</span></a> * batch-size, but for non-fixed-sized types like TYPE_STRING it is the only way to get the entire input size. </dd>
</dl>
</p>
</dd></dl>

<dl class="function">
<dt id="_CPPv4NK6nvidia15inferenceserver6client12InferContext5Input5DTypeEv">
<span class="target" id="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Input_1ab90380c6a46ea973a08f86c5a1bb3ea8"></span><em class="property">virtual</em> <a class="reference internal" href="../protobuf_api/model_config.proto.html#_CPPv4N6nvidia15inferenceserver8DataTypeE" title="nvidia::inferenceserver::DataType">DataType</a> <code class="descname">DType</code><span class="sig-paren">(</span><span class="sig-paren">)</span> <em class="property">const</em> = 0<br /></dt>
<dd><p><dl class="docutils">
<dt><strong>Return</strong></dt>
<dd>The data-type of the input. </dd>
</dl>
</p>
</dd></dl>

<dl class="function">
<dt id="_CPPv4NK6nvidia15inferenceserver6client12InferContext5Input6FormatEv">
<span class="target" id="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Input_1a9ab3e04fbb0ad181fd74bf7675249fef"></span><em class="property">virtual</em> <a class="reference internal" href="../protobuf_api/model_config.proto.html#_CPPv4N6nvidia15inferenceserver10ModelInputE" title="nvidia::inferenceserver::ModelInput">ModelInput</a>::<a class="reference internal" href="../protobuf_api/model_config.proto.html#_CPPv4N6nvidia15inferenceserver10ModelInput6FormatE" title="nvidia::inferenceserver::ModelInput::Format">Format</a> <code class="descname">Format</code><span class="sig-paren">(</span><span class="sig-paren">)</span> <em class="property">const</em> = 0<br /></dt>
<dd><p><dl class="docutils">
<dt><strong>Return</strong></dt>
<dd>The format of the input. </dd>
</dl>
</p>
</dd></dl>

<dl class="function">
<dt id="_CPPv4NK6nvidia15inferenceserver6client12InferContext5Input4DimsEv">
<span class="target" id="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Input_1a500ee9536afa9d6286cd23ee1308ebda"></span><em class="property">virtual</em> <em class="property">const</em> DimsList &amp;<code class="descname">Dims</code><span class="sig-paren">(</span><span class="sig-paren">)</span> <em class="property">const</em> = 0<br /></dt>
<dd><p><dl class="docutils">
<dt><strong>Return</strong></dt>
<dd>The dimensions/shape of the input specified in the model configuration. Variable-size dimensions are reported as -1. </dd>
</dl>
</p>
</dd></dl>

<dl class="function">
<dt id="_CPPv4N6nvidia15inferenceserver6client12InferContext5Input5ResetEv">
<span class="target" id="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Input_1a3dee261f7bed6cb3840db299c6b1d246"></span><em class="property">virtual</em> <a class="reference internal" href="classnvidia_1_1inferenceserver_1_1client_1_1Error.html#_CPPv4N6nvidia15inferenceserver6client5ErrorE" title="nvidia::inferenceserver::client::Error">Error</a> <code class="descname">Reset</code><span class="sig-paren">(</span><span class="sig-paren">)</span> = 0<br /></dt>
<dd><p>Prepare this input to receive new tensor values. </p>
<p>Forget any existing values that were set by previous calls to <a class="reference internal" href="#classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Input_1a414fe9afbb60a48f162bac5439beee4d"><span class="std std-ref">SetRaw()</span></a>. <dl class="docutils">
<dt><strong>Return</strong></dt>
<dd><a class="reference internal" href="classnvidia_1_1inferenceserver_1_1client_1_1Error.html#classnvidia_1_1inferenceserver_1_1client_1_1Error"><span class="std std-ref">Error</span></a> object indicating success or failure. </dd>
</dl>
</p>
</dd></dl>

<dl class="function">
<dt id="_CPPv4NK6nvidia15inferenceserver6client12InferContext5Input5ShapeEv">
<span class="target" id="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Input_1aedd9cacfe4e87008521ed63c1eb8e035"></span><em class="property">virtual</em> <em class="property">const</em> std::vector&lt;int64_t&gt; &amp;<code class="descname">Shape</code><span class="sig-paren">(</span><span class="sig-paren">)</span> <em class="property">const</em> = 0<br /></dt>
<dd><p>Get the shape for this input that was most recently set by SetShape. </p>
<p><dl class="docutils">
<dt><strong>Return</strong></dt>
<dd>The shape, or empty vector if SetShape has not been called. </dd>
</dl>
</p>
</dd></dl>

<dl class="function">
<dt id="_CPPv4N6nvidia15inferenceserver6client12InferContext5Input8SetShapeERKNSt6vectorI7int64_tEE">
<span class="target" id="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Input_1a0e2c4e3813ebfda6ecef7e23a7f00de5"></span><em class="property">virtual</em> <a class="reference internal" href="classnvidia_1_1inferenceserver_1_1client_1_1Error.html#_CPPv4N6nvidia15inferenceserver6client5ErrorE" title="nvidia::inferenceserver::client::Error">Error</a> <code class="descname">SetShape</code><span class="sig-paren">(</span><em class="property">const</em> std::vector&lt;int64_t&gt; &amp;<em>dims</em><span class="sig-paren">)</span> = 0<br /></dt>
<dd><p>Set the shape for this input. </p>
<p>The shape must be set for inputs that have variable-size dimensions and is optional for other inputs. The shape must be set before calling SetRaw or SetFromString. <dl class="docutils">
<dt><strong>Return</strong></dt>
<dd><a class="reference internal" href="classnvidia_1_1inferenceserver_1_1client_1_1Error.html#classnvidia_1_1inferenceserver_1_1client_1_1Error"><span class="std std-ref">Error</span></a> object indicating success or failure. </dd>
<dt><strong>Parameters</strong></dt>
<dd><ul class="breatheparameterlist first last simple">
<li><code class="docutils literal notranslate"><span class="pre">dims</span></code>: The dimensions of the shape. </li>
</ul>
</dd>
</dl>
</p>
</dd></dl>

<dl class="function">
<dt id="_CPPv4N6nvidia15inferenceserver6client12InferContext5Input6SetRawEPK7uint8_t6size_t">
<span class="target" id="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Input_1a414fe9afbb60a48f162bac5439beee4d"></span><em class="property">virtual</em> <a class="reference internal" href="classnvidia_1_1inferenceserver_1_1client_1_1Error.html#_CPPv4N6nvidia15inferenceserver6client5ErrorE" title="nvidia::inferenceserver::client::Error">Error</a> <code class="descname">SetRaw</code><span class="sig-paren">(</span><em class="property">const</em> uint8_t *<em>input</em>, size_t <em>input_byte_size</em><span class="sig-paren">)</span> = 0<br /></dt>
<dd><p>Set tensor values for this input from a byte array. </p>
<p>The array is not copied and so it must not be modified or destroyed until this input is no longer needed (that is until the <a class="reference internal" href="classnvidia_1_1inferenceserver_1_1client_1_1InferContext.html#classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1a4cac8002817c9259851305b14c258f36"><span class="std std-ref">Run()</span></a> call(s) that use the input have completed). For batched inputs this function must be called batch-size times to provide all tensor values for a batch of this input. <dl class="docutils">
<dt><strong>Return</strong></dt>
<dd><a class="reference internal" href="classnvidia_1_1inferenceserver_1_1client_1_1Error.html#classnvidia_1_1inferenceserver_1_1client_1_1Error"><span class="std std-ref">Error</span></a> object indicating success or failure. </dd>
<dt><strong>Parameters</strong></dt>
<dd><ul class="breatheparameterlist first last simple">
<li><code class="docutils literal notranslate"><span class="pre">input</span></code>: The pointer to the array holding the tensor value. </li>
<li><code class="docutils literal notranslate"><span class="pre">input_byte_size</span></code>: The size of the array in bytes, must match the size expected by the input. </li>
</ul>
</dd>
</dl>
</p>
</dd></dl>

<dl class="function">
<dt id="_CPPv4N6nvidia15inferenceserver6client12InferContext5Input6SetRawERKNSt6vectorI7uint8_tEE">
<span class="target" id="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Input_1a094ec9eece1e10d877db9af69867090e"></span><em class="property">virtual</em> <a class="reference internal" href="classnvidia_1_1inferenceserver_1_1client_1_1Error.html#_CPPv4N6nvidia15inferenceserver6client5ErrorE" title="nvidia::inferenceserver::client::Error">Error</a> <code class="descname">SetRaw</code><span class="sig-paren">(</span><em class="property">const</em> std::vector&lt;uint8_t&gt; &amp;<em>input</em><span class="sig-paren">)</span> = 0<br /></dt>
<dd><p>Set tensor values for this input from a byte vector. </p>
<p>The vector is not copied and so it must not be modified or destroyed until this input is no longer needed (that is until the <a class="reference internal" href="classnvidia_1_1inferenceserver_1_1client_1_1InferContext.html#classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1a4cac8002817c9259851305b14c258f36"><span class="std std-ref">Run()</span></a> call(s) that use the input have completed). For batched inputs this function must be called batch-size times to provide all tensor values for a batch of this input. <dl class="docutils">
<dt><strong>Return</strong></dt>
<dd><a class="reference internal" href="classnvidia_1_1inferenceserver_1_1client_1_1Error.html#classnvidia_1_1inferenceserver_1_1client_1_1Error"><span class="std std-ref">Error</span></a> object indicating success or failure. </dd>
<dt><strong>Parameters</strong></dt>
<dd><ul class="breatheparameterlist first last simple">
<li><code class="docutils literal notranslate"><span class="pre">input</span></code>: The vector holding tensor values. </li>
</ul>
</dd>
</dl>
</p>
</dd></dl>

<dl class="function">
<dt id="_CPPv4N6nvidia15inferenceserver6client12InferContext5Input13SetFromStringERKNSt6vectorINSt6stringEEE">
<span class="target" id="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Input_1a73495802b757c43b67edd334fec341d7"></span><em class="property">virtual</em> <a class="reference internal" href="classnvidia_1_1inferenceserver_1_1client_1_1Error.html#_CPPv4N6nvidia15inferenceserver6client5ErrorE" title="nvidia::inferenceserver::client::Error">Error</a> <code class="descname">SetFromString</code><span class="sig-paren">(</span><em class="property">const</em> std::vector&lt;std::string&gt; &amp;<em>input</em><span class="sig-paren">)</span> = 0<br /></dt>
<dd><p>Set tensor values for this input from a vector or strings. </p>
<p>This method can only be used for tensors with STRING data-type. The strings are assigned in row-major order to the elements of the tensor. The strings are copied and so the ‘input’ does not need to be preserved as with <a class="reference internal" href="#classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Input_1a414fe9afbb60a48f162bac5439beee4d"><span class="std std-ref">SetRaw()</span></a>. For batched inputs this function must be called batch-size times to provide all tensor values for a batch of this input. <dl class="docutils">
<dt><strong>Return</strong></dt>
<dd><a class="reference internal" href="classnvidia_1_1inferenceserver_1_1client_1_1Error.html#classnvidia_1_1inferenceserver_1_1client_1_1Error"><span class="std std-ref">Error</span></a> object indicating success or failure. </dd>
<dt><strong>Parameters</strong></dt>
<dd><ul class="breatheparameterlist first last simple">
<li><code class="docutils literal notranslate"><span class="pre">input</span></code>: The vector holding tensor string values. </li>
</ul>
</dd>
</dl>
</p>
</dd></dl>

</div>
</dd></dl>

</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Options.html" class="btn btn-neutral float-right" title="Class InferContext::Options" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="classnvidia_1_1inferenceserver_1_1client_1_1InferContext.html" class="btn btn-neutral float-left" title="Class InferContext" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2018, NVIDIA Corporation

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
    

  <style>
  a:link, a:visited {
    color: #76b900;
  }

  a:hover {
    color: #8c0;
  }

  .rst-content dl:not(.docutils) dt {
    background: rgba(118, 185, 0, 0.1);
    color: rgba(59,93,0,1);
    border-top: solid 3px rgba(59,93,0,1);
  }
  </style>
  

</body>
</html>