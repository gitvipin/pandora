%% Generated by Sphinx.
\def\sphinxdocclass{report}
\documentclass[letterpaper,10pt,english]{sphinxmanual}
\ifdefined\pdfpxdimen
   \let\sphinxpxdimen\pdfpxdimen\else\newdimen\sphinxpxdimen
\fi \sphinxpxdimen=.75bp\relax

\PassOptionsToPackage{warn}{textcomp}
\usepackage[utf8]{inputenc}
\ifdefined\DeclareUnicodeCharacter
% support both utf8 and utf8x syntaxes
\edef\sphinxdqmaybe{\ifdefined\DeclareUnicodeCharacterAsOptional\string"\fi}
  \DeclareUnicodeCharacter{\sphinxdqmaybe00A0}{\nobreakspace}
  \DeclareUnicodeCharacter{\sphinxdqmaybe2500}{\sphinxunichar{2500}}
  \DeclareUnicodeCharacter{\sphinxdqmaybe2502}{\sphinxunichar{2502}}
  \DeclareUnicodeCharacter{\sphinxdqmaybe2514}{\sphinxunichar{2514}}
  \DeclareUnicodeCharacter{\sphinxdqmaybe251C}{\sphinxunichar{251C}}
  \DeclareUnicodeCharacter{\sphinxdqmaybe2572}{\textbackslash}
\fi
\usepackage{cmap}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amssymb,amstext}
\usepackage{babel}
\usepackage{times}
\usepackage[Bjarne]{fncychap}
\usepackage{sphinx}

\fvset{fontsize=\small}
\usepackage{geometry}

% Include hyperref last.
\usepackage{hyperref}
% Fix anchor placement for figures with captions.
\usepackage{hypcap}% it must be loaded after hyperref.
% Set up styles of URL: it should be placed after hyperref.
\urlstyle{same}

\addto\captionsenglish{\renewcommand{\figurename}{Fig.\@ }}
\makeatletter
\def\fnum@figure{\figurename\thefigure{}}
\makeatother
\addto\captionsenglish{\renewcommand{\tablename}{Table }}
\makeatletter
\def\fnum@table{\tablename\thetable{}}
\makeatother
\addto\captionsenglish{\renewcommand{\literalblockname}{Listing}}

\addto\captionsenglish{\renewcommand{\literalblockcontinuedname}{continued from previous page}}
\addto\captionsenglish{\renewcommand{\literalblockcontinuesname}{continues on next page}}
\addto\captionsenglish{\renewcommand{\sphinxnonalphabeticalgroupname}{Non-alphabetical}}
\addto\captionsenglish{\renewcommand{\sphinxsymbolsname}{Symbols}}
\addto\captionsenglish{\renewcommand{\sphinxnumbersname}{Numbers}}

\addto\extrasenglish{\def\pageautorefname{page}}




% Jupyter Notebook code cell colors
\definecolor{nbsphinxin}{HTML}{307FC1}
\definecolor{nbsphinxout}{HTML}{BF5B3D}
\definecolor{nbsphinx-code-bg}{HTML}{F5F5F5}
\definecolor{nbsphinx-code-border}{HTML}{E0E0E0}
\definecolor{nbsphinx-stderr}{HTML}{FFDDDD}
% ANSI colors for output streams and traceback highlighting
\definecolor{ansi-black}{HTML}{3E424D}
\definecolor{ansi-black-intense}{HTML}{282C36}
\definecolor{ansi-red}{HTML}{E75C58}
\definecolor{ansi-red-intense}{HTML}{B22B31}
\definecolor{ansi-green}{HTML}{00A250}
\definecolor{ansi-green-intense}{HTML}{007427}
\definecolor{ansi-yellow}{HTML}{DDB62B}
\definecolor{ansi-yellow-intense}{HTML}{B27D12}
\definecolor{ansi-blue}{HTML}{208FFB}
\definecolor{ansi-blue-intense}{HTML}{0065CA}
\definecolor{ansi-magenta}{HTML}{D160C4}
\definecolor{ansi-magenta-intense}{HTML}{A03196}
\definecolor{ansi-cyan}{HTML}{60C6C8}
\definecolor{ansi-cyan-intense}{HTML}{258F8F}
\definecolor{ansi-white}{HTML}{C5C1B4}
\definecolor{ansi-white-intense}{HTML}{A1A6B2}
\definecolor{ansi-default-inverse-fg}{HTML}{FFFFFF}
\definecolor{ansi-default-inverse-bg}{HTML}{000000}

% Define an environment for non-plain-text code cell outputs (e.g. images)
\makeatletter
\newenvironment{nbsphinxfancyoutput}{%
    % Avoid fatal error with framed.sty if graphics too long to fit on one page
    \let\sphinxincludegraphics\nbsphinxincludegraphics
    \nbsphinx@image@maxheight\textheight
    \advance\nbsphinx@image@maxheight -2\fboxsep   % default \fboxsep 3pt
    \advance\nbsphinx@image@maxheight -2\fboxrule  % default \fboxrule 0.4pt
    \advance\nbsphinx@image@maxheight -\baselineskip
\def\nbsphinxfcolorbox{\spx@fcolorbox{nbsphinx-code-border}{white}}%
\def\FrameCommand{\nbsphinxfcolorbox\nbsphinxfancyaddprompt\@empty}%
\def\FirstFrameCommand{\nbsphinxfcolorbox\nbsphinxfancyaddprompt\sphinxVerbatim@Continues}%
\def\MidFrameCommand{\nbsphinxfcolorbox\sphinxVerbatim@Continued\sphinxVerbatim@Continues}%
\def\LastFrameCommand{\nbsphinxfcolorbox\sphinxVerbatim@Continued\@empty}%
\MakeFramed{\advance\hsize-\width\@totalleftmargin\z@\linewidth\hsize\@setminipage}%
}{\par\unskip\@minipagefalse\endMakeFramed}
\makeatother
\newbox\nbsphinxpromptbox
\def\nbsphinxfancyaddprompt{\ifvoid\nbsphinxpromptbox\else
    \kern\fboxrule\kern\fboxsep
    \copy\nbsphinxpromptbox
    \kern-\ht\nbsphinxpromptbox\kern-\dp\nbsphinxpromptbox
    \kern-\fboxsep\kern-\fboxrule\nointerlineskip
    \fi}
\newlength\nbsphinxcodecellspacing
\setlength{\nbsphinxcodecellspacing}{0pt}

% Define support macros for attaching opening and closing lines to notebooks
\newsavebox\nbsphinxbox
\makeatletter
\newcommand{\nbsphinxstartnotebook}[1]{%
    \par
    % measure needed space
    \setbox\nbsphinxbox\vtop{{#1\par}}
    % reserve some space at bottom of page, else start new page
    \needspace{\dimexpr2.5\baselineskip+\ht\nbsphinxbox+\dp\nbsphinxbox}
    % mimick vertical spacing from \section command
      \addpenalty\@secpenalty
      \@tempskipa 3.5ex \@plus 1ex \@minus .2ex\relax
      \addvspace\@tempskipa
      {\Large\@tempskipa\baselineskip
             \advance\@tempskipa-\prevdepth
             \advance\@tempskipa-\ht\nbsphinxbox
             \ifdim\@tempskipa>\z@
               \vskip \@tempskipa
             \fi}
    \unvbox\nbsphinxbox
    % if notebook starts with a \section, prevent it from adding extra space
    \@nobreaktrue\everypar{\@nobreakfalse\everypar{}}%
    % compensate the parskip which will get inserted by next paragraph
    \nobreak\vskip-\parskip
    % do not break here
    \nobreak
}% end of \nbsphinxstartnotebook

\newcommand{\nbsphinxstopnotebook}[1]{%
    \par
    % measure needed space
    \setbox\nbsphinxbox\vbox{{#1\par}}
    \nobreak % it updates page totals
    \dimen@\pagegoal
    \advance\dimen@-\pagetotal \advance\dimen@-\pagedepth
    \advance\dimen@-\ht\nbsphinxbox \advance\dimen@-\dp\nbsphinxbox
    \ifdim\dimen@<\z@
      % little space left
      \unvbox\nbsphinxbox
      \kern-.8\baselineskip
      \nobreak\vskip\z@\@plus1fil
      \penalty100
      \vskip\z@\@plus-1fil
      \kern.8\baselineskip
    \else
      \unvbox\nbsphinxbox
    \fi
}% end of \nbsphinxstopnotebook

% Ensure height of an included graphics fits in nbsphinxfancyoutput frame
\newdimen\nbsphinx@image@maxheight % set in nbsphinxfancyoutput environment
\newcommand*{\nbsphinxincludegraphics}[2][]{%
    \gdef\spx@includegraphics@options{#1}%
    \setbox\spx@image@box\hbox{\includegraphics[#1,draft]{#2}}%
    \in@false
    \ifdim \wd\spx@image@box>\linewidth
      \g@addto@macro\spx@includegraphics@options{,width=\linewidth}%
      \in@true
    \fi
    % no rotation, no need to worry about depth
    \ifdim \ht\spx@image@box>\nbsphinx@image@maxheight
      \g@addto@macro\spx@includegraphics@options{,height=\nbsphinx@image@maxheight}%
      \in@true
    \fi
    \ifin@
      \g@addto@macro\spx@includegraphics@options{,keepaspectratio}%
    \fi
    \setbox\spx@image@box\box\voidb@x % clear memory
    \expandafter\includegraphics\expandafter[\spx@includegraphics@options]{#2}%
}% end of "\MakeFrame"-safe variant of \sphinxincludegraphics
\makeatother



\title{NVIDIA TensorRT Inference Server Documentation}
\date{Feb 15, 2019}
\release{0.12.0dev
}
\author{NVIDIA Corporation}
\newcommand{\sphinxlogo}{\vbox{}}
\renewcommand{\releasename}{Release}
\makeindex
\begin{document}

\pagestyle{empty}
\sphinxmaketitle
\pagestyle{plain}
\sphinxtableofcontents
\pagestyle{normal}
\phantomsection\label{\detokenize{index::doc}}


\begin{sphinxadmonition}{warning}{Warning:}
You are currently viewing unstable developer preview
of the documentation. To see the documentation for the latest
stable release click \sphinxhref{https://docs.nvidia.com/deeplearning/sdk/tensorrt-inference-server-guide/docs/index.html}{here}.
\end{sphinxadmonition}

The NVIDIA TensorRT Inference Server provides a cloud inferencing
solution optimized for NVIDIA GPUs. The server provides an inference
service via an HTTP or gRPC endpoint, allowing remote clients to
request inferencing for any model being managed by the server. The
inference server provides the following features:
\begin{itemize}
\item {} 
\sphinxhref{https://docs.nvidia.com/deeplearning/sdk/tensorrt-inference-server-master-branch-guide/docs/model\_repository.html\#framework-model-definition}{Multiple framework support}. The
server can manage any number and mix of models (limited by system
disk and memory resources). Supports TensorRT, TensorFlow GraphDef,
TensorFlow SavedModel and Caffe2 NetDef model formats. Also supports
TensorFlow-TensorRT integrated models.

\item {} 
\sphinxhref{https://docs.nvidia.com/deeplearning/sdk/tensorrt-inference-server-master-branch-guide/docs/model\_repository.html\#custom-backends}{Custom backend support}. The inference server
allows individual models to be implemented with custom backends
instead of by a deep-learning framework. With a custom backend a
model can implement any logic desired, while still benefiting from
the GPU support, concurrent execution, dynamic batching and other
features provided by the server.

\item {} 
The inference server \sphinxhref{https://docs.nvidia.com/deeplearning/sdk/tensorrt-inference-server-master-branch-guide/docs/model\_repository.html\#modifying-the-model-repository}{monitors the model repository}
for any change and dynamically reloads the model(s) when necessary,
without requiring a server restart. Models and model versions can be
added and removed, and model configurations can be modified while
the server is running.

\item {} 
Multi-GPU support. The server can distribute inferencing across all
system GPUs.

\item {} 
\sphinxhref{https://docs.nvidia.com/deeplearning/sdk/tensorrt-inference-server-master-branch-guide/docs/model\_configuration.html?highlight=batching\#instance-groups}{Concurrent model execution support}. Multiple
models (or multiple instances of the same model) can run
simultaneously on the same GPU.

\item {} 
Batching support. For models that support batching, the server can
accept requests for a batch of inputs and respond with the
corresponding batch of outputs. The inference server also supports
\sphinxhref{https://docs.nvidia.com/deeplearning/sdk/tensorrt-inference-server-master-branch-guide/docs/model\_configuration.html?highlight=batching\#dynamic-batching}{dynamic batching}
where individual inference requests are dynamically combined
together to improve inference throughput. Dynamic batching is
transparent to the client requesting inference.

\item {} 
\sphinxhref{https://docs.nvidia.com/deeplearning/sdk/tensorrt-inference-server-master-branch-guide/docs/model\_repository.html\#}{Model repositories}
may reside on a locally accessible file system (e.g. NFS) or in
Google Cloud Storage.

\item {} 
Readiness and liveness \sphinxhref{https://docs.nvidia.com/deeplearning/sdk/tensorrt-inference-server-master-branch-guide/docs/http\_grpc\_api.html\#health}{health endpoints}
suitable for any orchestration or deployment framework, such as
Kubernetes.

\item {} 
\sphinxhref{https://docs.nvidia.com/deeplearning/sdk/tensorrt-inference-server-master-branch-guide/docs/metrics.html}{Metrics}
indicating GPU utiliization, server throughput, and server latency.

\end{itemize}


\chapter{Quickstart}
\label{\detokenize{quickstart:quickstart}}\label{\detokenize{quickstart::doc}}
The TensorRT Inference Server is available in two ways:
\begin{itemize}
\item {} 
As a pre-built Docker container container available from the \sphinxhref{https://ngc.nvidia.com}{NVIDIA
GPU Cloud (NGC)}. For more information,
see {\hyperref[\detokenize{quickstart:section-using-a-prebuilt-docker-container}]{\sphinxcrossref{\DUrole{std,std-ref}{Using A Prebuilt Docker Container}}}}.

\item {} 
As buildable source code located in GitHub. Building the inference
server yourself requires the usage of Docker and the TensorFlow and
PyTorch containers available from \sphinxhref{https://ngc.nvidia.com}{NGC}.
For more information, see {\hyperref[\detokenize{quickstart:section-building-from-source-code}]{\sphinxcrossref{\DUrole{std,std-ref}{Building From Source Code}}}}.

\end{itemize}


\section{Prerequisites}
\label{\detokenize{quickstart:prerequisites}}\label{\detokenize{quickstart:section-prerequisites}}
Regardless of which method you choose (starting with a pre-built
container from NGC or building from source), you must perform the
following prerequisite steps:
\begin{itemize}
\item {} 
Ensure you have access and are logged into NGC.  For step-by-step
instructions, see the \sphinxhref{http://docs.nvidia.com/ngc/ngc-getting-started-guide/index.html}{NGC Getting Started Guide}.

\item {} 
Install Docker and nvidia-docker.  For DGX users, see \sphinxhref{http://docs.nvidia.com/deeplearning/dgx/preparing-containers/index.html}{Preparing to
use NVIDIA Containers}.
For users other than DGX, see the \sphinxhref{https://github.com/NVIDIA/nvidia-docker}{nvidia-docker installation
documentation}.

\item {} 
Clone the TensorRT Inference Server GitHub repo. Even if you choose
to get the pre-built inference server from NGC, you need the GitHub
repo for the example model repository and to build the example
applications. Go to
\sphinxurl{https://github.com/NVIDIA/tensorrt-inference-server}, select the
r\textless{}xx.yy\textgreater{} release branch that you are using, and then select the
clone or download drop down button.

\item {} 
Create a model repository containing one or more models that you
want the inference server to serve. An example model repository is
included in the docs/examples/model\_repository directory of the
GitHub repo. Before using the repository, you must fetch any missing
model definition files from their public model zoos via the provided
docs/examples/fetch\_models.sh script:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZdl{} cd docs/examples
\PYGZdl{} ./fetch\PYGZus{}models.sh
\end{sphinxVerbatim}

\end{itemize}


\section{Using A Prebuilt Docker Container}
\label{\detokenize{quickstart:using-a-prebuilt-docker-container}}\label{\detokenize{quickstart:section-using-a-prebuilt-docker-container}}
Make sure you log into NGC as described in
{\hyperref[\detokenize{quickstart:section-prerequisites}]{\sphinxcrossref{\DUrole{std,std-ref}{Prerequisites}}}} before attempting the steps in this
section.  Use docker pull to get the TensorRT Inference Server
container from NGC:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZdl{} docker pull nvcr.io/nvidia/tensorrtserver:\PYGZlt{}xx.yy\PYGZgt{}\PYGZhy{}py3
\end{sphinxVerbatim}

Where \textless{}xx.yy\textgreater{} is the version of the inference server that you want to
pull. Once you have the container follow these steps to run the server
and the example client applications.
\begin{enumerate}
\def\theenumi{\arabic{enumi}}
\def\labelenumi{\theenumi .}
\makeatletter\def\p@enumii{\p@enumi \theenumi .}\makeatother
\item {} 
{\hyperref[\detokenize{quickstart:section-run-tensorrt-inference-server}]{\sphinxcrossref{\DUrole{std,std-ref}{Run the inference server}}}}.

\item {} 
{\hyperref[\detokenize{quickstart:section-verify-inference-server-status}]{\sphinxcrossref{\DUrole{std,std-ref}{Verify that the server is running correct}}}}.

\item {} 
{\hyperref[\detokenize{quickstart:section-building-the-client-examples}]{\sphinxcrossref{\DUrole{std,std-ref}{Build the example client applications}}}}.

\item {} 
{\hyperref[\detokenize{quickstart:section-running-the-image-classification-example}]{\sphinxcrossref{\DUrole{std,std-ref}{Run the image classification example}}}}.

\end{enumerate}


\section{Building From Source Code}
\label{\detokenize{quickstart:building-from-source-code}}\label{\detokenize{quickstart:section-building-from-source-code}}
Make sure you complete the steps in {\hyperref[\detokenize{quickstart:section-prerequisites}]{\sphinxcrossref{\DUrole{std,std-ref}{Prerequisites}}}}
before attempting to build the inference server. To build the
inference server from source, change to the root directory of the
GitHub repo and use docker to build:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZdl{} docker build \PYGZhy{}\PYGZhy{}pull \PYGZhy{}t tensorrtserver
\end{sphinxVerbatim}

After the build completes follow these steps to run the server and the
example client applications.
\begin{enumerate}
\def\theenumi{\arabic{enumi}}
\def\labelenumi{\theenumi .}
\makeatletter\def\p@enumii{\p@enumi \theenumi .}\makeatother
\item {} 
{\hyperref[\detokenize{quickstart:section-run-tensorrt-inference-server}]{\sphinxcrossref{\DUrole{std,std-ref}{Run the inference server}}}}.

\item {} 
{\hyperref[\detokenize{quickstart:section-verify-inference-server-status}]{\sphinxcrossref{\DUrole{std,std-ref}{Verify that the server is running correct}}}}.

\item {} 
{\hyperref[\detokenize{quickstart:section-building-the-client-examples}]{\sphinxcrossref{\DUrole{std,std-ref}{Build the example client applications}}}}.

\item {} 
{\hyperref[\detokenize{quickstart:section-running-the-image-classification-example}]{\sphinxcrossref{\DUrole{std,std-ref}{Run the image classification example}}}}.

\end{enumerate}


\section{Run TensorRT Inference Server}
\label{\detokenize{quickstart:run-tensorrt-inference-server}}\label{\detokenize{quickstart:section-run-tensorrt-inference-server}}
Assuming the example model repository is available in
/full/path/to/example/model/repository, use the following command to
run the inference server container:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZdl{} nvidia\PYGZhy{}docker run \PYGZhy{}\PYGZhy{}rm \PYGZhy{}\PYGZhy{}shm\PYGZhy{}size=1g \PYGZhy{}\PYGZhy{}ulimit memlock=\PYGZhy{}1 \PYGZhy{}\PYGZhy{}ulimit stack=67108864 \PYGZhy{}p8000:8000 \PYGZhy{}p8001:8001 \PYGZhy{}p8002:8002 \PYGZhy{}v/full/path/to/example/model/repository:/models \PYGZlt{}docker image\PYGZgt{} trtserver \PYGZhy{}\PYGZhy{}model\PYGZhy{}store=/models
\end{sphinxVerbatim}

Where \textless{}docker image\textgreater{} is \sphinxstyleemphasis{nvcr.io/nvidia/tensorrtserver:\textless{}xx.yy\textgreater{}-py3} if
you pulled the inference server container from NGC, or is
\sphinxstyleemphasis{tensorrtserver} if you built the inference server from source.

For more information, see {\hyperref[\detokenize{run:section-running-the-inference-server}]{\sphinxcrossref{\DUrole{std,std-ref}{Running The Inference Server}}}}.


\section{Verify Inference Server Is Running Correctly}
\label{\detokenize{quickstart:verify-inference-server-is-running-correctly}}\label{\detokenize{quickstart:section-verify-inference-server-status}}
Use the server’s \sphinxstyleemphasis{Status} endpoint to verify that the server and the
models are ready for inference.  From the host system use curl to
access the HTTP endpoint to request the server status. For example:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZdl{} curl localhost:8000/api/status
id: \PYGZdq{}inference:0\PYGZdq{}
version: \PYGZdq{}0.6.0\PYGZdq{}
uptime\PYGZus{}ns: 23322988571
model\PYGZus{}status \PYGZob{}
  key: \PYGZdq{}resnet50\PYGZus{}netdef\PYGZdq{}
  value \PYGZob{}
    config \PYGZob{}
      name: \PYGZdq{}resnet50\PYGZus{}netdef\PYGZdq{}
      platform: \PYGZdq{}caffe2\PYGZus{}netdef\PYGZdq{}
    \PYGZcb{}
    ...
    version\PYGZus{}status \PYGZob{}
      key: 1
      value \PYGZob{}
        ready\PYGZus{}state: MODEL\PYGZus{}READY
      \PYGZcb{}
    \PYGZcb{}
  \PYGZcb{}
\PYGZcb{}
ready\PYGZus{}state: SERVER\PYGZus{}READY
\end{sphinxVerbatim}

The ready\_state field should return SERVER\_READY to indicate that the
inference server is online, that models are properly loaded, and that
the server is ready to receive inference requests.

For more information, see
{\hyperref[\detokenize{run:section-checking-inference-server-status}]{\sphinxcrossref{\DUrole{std,std-ref}{Checking Inference Server Status}}}}.


\section{Building The Client Examples}
\label{\detokenize{quickstart:building-the-client-examples}}\label{\detokenize{quickstart:section-building-the-client-examples}}
To build the C++ client library, C++ and Python examples, and a Python
wheel file for the Python client library, change to the root directory
of the GitHub repo and use docker to build:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZdl{} docker build \PYGZhy{}t tensorrtserver\PYGZus{}clients \PYGZhy{}\PYGZhy{}target trtserver\PYGZus{}build \PYGZhy{}\PYGZhy{}build\PYGZhy{}arg \PYGZdq{}BUILD\PYGZus{}CLIENTS\PYGZus{}ONLY=1\PYGZdq{} .
\end{sphinxVerbatim}

After the build completes, the tensorrtserver\_clients Docker image
will contain the built client libraries and examples. Run the client
image so that the client examples can access the inference server
running in its own container:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZdl{} docker run \PYGZhy{}it \PYGZhy{}\PYGZhy{}rm \PYGZhy{}\PYGZhy{}net=host tensorrtserver\PYGZus{}clients
\end{sphinxVerbatim}

For more information, see
{\hyperref[\detokenize{client:section-building-the-client-libraries-and-examples}]{\sphinxcrossref{\DUrole{std,std-ref}{Building the Client Libraries and Examples}}}}.


\section{Running The Image Classification Example}
\label{\detokenize{quickstart:running-the-image-classification-example}}\label{\detokenize{quickstart:section-running-the-image-classification-example}}
From within the tensorrtserver\_clients image, run the example
image-client application to perform image classification using the
example resnet50\_netdef from the example model repository.

To send a request for the resnet50\_netdef (Caffe2) model from the
example model repository for an image from the qa/images directory:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZdl{} /opt/tensorrtserver/bin/image\PYGZus{}client \PYGZhy{}m resnet50\PYGZus{}netdef \PYGZhy{}s INCEPTION qa/images/mug.jpg
Request 0, batch size 1
Image \PYGZsq{}../qa/images/mug.jpg\PYGZsq{}:
    504 (COFFEE MUG) = 0.723991
\end{sphinxVerbatim}

The Python version of the application accepts the same command-line
arguments:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZdl{} python3 /workspace/src/clients/python/image\PYGZus{}client.py \PYGZhy{}m resnet50\PYGZus{}netdef \PYGZhy{}s INCEPTION qa/images/mug.jpg
Request 0, batch size 1
Image \PYGZsq{}../qa/images/mug.jpg\PYGZsq{}:
    504 (COFFEE MUG) = 0.778078556061
\end{sphinxVerbatim}

For more information, see {\hyperref[\detokenize{client:section-image-classification-example}]{\sphinxcrossref{\DUrole{std,std-ref}{Image Classification Example Application}}}}.


\chapter{Installing the Server}
\label{\detokenize{install:installing-the-server}}\label{\detokenize{install::doc}}
The TensorRT Inference Server is available as a pre-built Docker
container or you can {\hyperref[\detokenize{build:section-building-the-server}]{\sphinxcrossref{\DUrole{std,std-ref}{build it from source}}}}.


\section{Installing Prebuilt Containers}
\label{\detokenize{install:installing-prebuilt-containers}}\label{\detokenize{install:section-installing-prebuilt-containers}}
The inference server is provided as a pre-built container on the
\sphinxhref{https://ngc.nvidia.com}{NVIDIA GPU Cloud (NGC)}.  Before pulling the
container you must have access and be logged into the NGC container
registry as explained in the \sphinxhref{http://docs.nvidia.com/ngc/ngc-getting-started-guide/index.html}{NGC Getting Started Guide}.

Before you can pull a container from the NGC container registry, you
must have Docker and nvidia-docker installed. For DGX users, this is
explained in \sphinxhref{http://docs.nvidia.com/deeplearning/dgx/preparing-containers/index.html}{Preparing to use NVIDIA Containers Getting Started Guide}.
For users other than DGX, follow the \sphinxhref{https://github.com/NVIDIA/nvidia-docker}{nvidia-docker installation
documentation} to install
the most recent version of CUDA, Docker, and nvidia-docker.

After performing the above setup, you can pull the TensorRT Inference
Server container using the following command:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
docker pull nvcr.io/nvidia/tensorrtserver:19.01\PYGZhy{}py3
\end{sphinxVerbatim}

Replace \sphinxstyleemphasis{19.01} with the version of inference server that you want to
pull.


\chapter{Running the Server}
\label{\detokenize{run:running-the-server}}\label{\detokenize{run::doc}}
For best performance the TensorRT Inference Server should be run on a
system that contains Docker, nvidia-docker, CUDA and one or more
supported GPUs, as explained in
{\hyperref[\detokenize{run:section-running-the-inference-server}]{\sphinxcrossref{\DUrole{std,std-ref}{Running The Inference Server}}}}. The inference server can
also be run on non-CUDA, non-GPU systems as described in
{\hyperref[\detokenize{run:section-running-the-inference-server-without-gpu}]{\sphinxcrossref{\DUrole{std,std-ref}{Running The Inference Server On A System Without A GPU}}}}.


\section{Example Model Repository}
\label{\detokenize{run:example-model-repository}}\label{\detokenize{run:section-example-model-repository}}
Before running the TensorRT Inference Server, you must first set up a
model repository containing the models that the server will make
available for inferencing.

An example model repository containing a Caffe2 ResNet50, a TensorFlow
Inception model, and a simple TensorFlow GraphDef model (used by the
{\hyperref[\detokenize{client:section-client-api}]{\sphinxcrossref{\DUrole{std,std-ref}{simple\_client example}}}}) are provided in the
\sphinxhref{https://github.com/NVIDIA/tensorrt-inference-server/tree/master/docs/examples/model\_repository}{docs/examples/model\_repository}
directory. Before using the example model repository you must fetch
any missing model definition files from their public model zoos:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZdl{} cd docs/examples
\PYGZdl{} ./fetch\PYGZus{}models.sh
\end{sphinxVerbatim}


\section{Running The Inference Server}
\label{\detokenize{run:running-the-inference-server}}\label{\detokenize{run:section-running-the-inference-server}}
Before running the inference server, you must first set up a model
repository containing the models that the server will make available
for inferencing. Section {\hyperref[\detokenize{model_repository:section-model-repository}]{\sphinxcrossref{\DUrole{std,std-ref}{Model Repository}}}} describes how
to create your own model repository. You can also use
{\hyperref[\detokenize{run:section-example-model-repository}]{\sphinxcrossref{\DUrole{std,std-ref}{Example Model Repository}}}} to set up an example model
repository.

Assuming the sample model repository is available in
/path/to/model/repository, the following command runs the container
you pulled from NGC or built locally:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZdl{} nvidia\PYGZhy{}docker run \PYGZhy{}\PYGZhy{}rm \PYGZhy{}\PYGZhy{}shm\PYGZhy{}size=1g \PYGZhy{}\PYGZhy{}ulimit memlock=\PYGZhy{}1 \PYGZhy{}\PYGZhy{}ulimit stack=67108864 \PYGZhy{}p8000:8000 \PYGZhy{}p8001:8001 \PYGZhy{}p8002:8002 \PYGZhy{}v/path/to/model/repository:/models \PYGZlt{}tensorrtserver image name\PYGZgt{} trtserver \PYGZhy{}\PYGZhy{}model\PYGZhy{}store=/models
\end{sphinxVerbatim}

Where \sphinxstyleemphasis{\textless{}tensorrtserver image name\textgreater{}} will be something like
\sphinxstylestrong{nvcr.io/nvidia/tensorrtserver:19.01-py3} if you {\hyperref[\detokenize{install:section-installing-prebuilt-containers}]{\sphinxcrossref{\DUrole{std,std-ref}{pulled the
container from the NGC registry}}}}, or \sphinxstylestrong{tensorrtserver} if
you {\hyperref[\detokenize{build:section-building-the-server}]{\sphinxcrossref{\DUrole{std,std-ref}{built it from source}}}}.

The nvidia-docker -v option maps /path/to/model/repository on the host
into the container at /models, and the --model-store option to the
server is used to point to /models as the model repository.

The -p flags expose the container ports where the inference server
listens for HTTP requests (port 8000), listens for GRPC requests (port
8001), and reports Prometheus metrics (port 8002).

The --shm-size and --ulimit flags are recommended to improve the
server’s performance. For --shm-size the minimum recommended size is
1g but larger sizes may be necessary depending on the number and size
of models being served.

For more information on the Prometheus metrics provided by the
inference server see {\hyperref[\detokenize{metrics:section-metrics}]{\sphinxcrossref{\DUrole{std,std-ref}{Metrics}}}}.


\section{Running The Inference Server On A System Without A GPU}
\label{\detokenize{run:running-the-inference-server-on-a-system-without-a-gpu}}\label{\detokenize{run:section-running-the-inference-server-without-gpu}}
On a system without GPUs, the inference server should be run using
docker instead of nvidia-docker, but is otherwise identical to what is
described in {\hyperref[\detokenize{run:section-running-the-inference-server}]{\sphinxcrossref{\DUrole{std,std-ref}{Running The Inference Server}}}}:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZdl{} docker run \PYGZhy{}\PYGZhy{}rm \PYGZhy{}\PYGZhy{}shm\PYGZhy{}size=1g \PYGZhy{}\PYGZhy{}ulimit memlock=\PYGZhy{}1 \PYGZhy{}\PYGZhy{}ulimit stack=67108864 \PYGZhy{}p8000:8000 \PYGZhy{}p8001:8001 \PYGZhy{}p8002:8002 \PYGZhy{}v/path/to/model/repository:/models \PYGZlt{}tensorrtserver image name\PYGZgt{} trtserver \PYGZhy{}\PYGZhy{}model\PYGZhy{}store=/models
\end{sphinxVerbatim}

Because a GPU is not available, the inference server will be unable to
load any model configuration that requires a GPU or that specified a
GPU instance by an {\hyperref[\detokenize{model_configuration:section-instance-groups}]{\sphinxcrossref{\DUrole{std,std-ref}{instance-group}}}}
configuration.


\section{Checking Inference Server Status}
\label{\detokenize{run:checking-inference-server-status}}\label{\detokenize{run:section-checking-inference-server-status}}
The simplest way to verify that the inference server is running
correctly is to use the Status API to query the server’s status. From
the host system use \sphinxstyleemphasis{curl} to access the HTTP endpoint to request
server status. The response is protobuf text showing the status for
the server and for each model being served, for example:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZdl{} curl localhost:8000/api/status
id: \PYGZdq{}inference:0\PYGZdq{}
version: \PYGZdq{}0.6.0\PYGZdq{}
uptime\PYGZus{}ns: 23322988571
model\PYGZus{}status \PYGZob{}
  key: \PYGZdq{}resnet50\PYGZus{}netdef\PYGZdq{}
  value \PYGZob{}
    config \PYGZob{}
      name: \PYGZdq{}resnet50\PYGZus{}netdef\PYGZdq{}
      platform: \PYGZdq{}caffe2\PYGZus{}netdef\PYGZdq{}
    \PYGZcb{}
    ...
    version\PYGZus{}status \PYGZob{}
      key: 1
      value \PYGZob{}
        ready\PYGZus{}state: MODEL\PYGZus{}READY
      \PYGZcb{}
    \PYGZcb{}
  \PYGZcb{}
\PYGZcb{}
ready\PYGZus{}state: SERVER\PYGZus{}READY
\end{sphinxVerbatim}

This status shows configuration information as well as indicating that
version 1 of the resnet50\_netdef model is MODEL\_READY. This means that
the server is ready to accept inferencing requests for version 1 of
that model. A model version ready\_state will show up as
MODEL\_UNAVAILABLE if the model failed to load for some reason.


\chapter{Client Libraries and Examples}
\label{\detokenize{client:client-libraries-and-examples}}\label{\detokenize{client:section-client-libraries-and-examples}}\label{\detokenize{client::doc}}
The inference server \sphinxstyleemphasis{client libraries} make it easy to communicate
with the TensorRT Inference Server from you C++ or Python
application. Using these libraries you can send either HTTP or GRPC
requests to server to check status or health and to make inference
requests.

A couple of example applications show how to use the client libraries
to perform image classification and to test performance:
\begin{itemize}
\item {} 
C++ and Python versions of \sphinxstyleemphasis{image\_client}, an example application
that uses the C++ or Python client library to execute image
classification models on the TensorRT Inference Server.

\item {} 
Python version of \sphinxstyleemphasis{grpc\_image\_client}, an example application that
is functionally equivalent to \sphinxstyleemphasis{image\_client} but that uses GRPC
generated client code to communicate with the inference server
(instead of the client library).

\item {} 
C++ version of \sphinxstyleemphasis{perf\_client}, an example application that issues a
large number of concurrent requests to the inference server to
measure latency and throughput for a given model. You can use this
to experiment with different model configuration settings for your
models.

\end{itemize}


\section{Building the Client Libraries and Examples}
\label{\detokenize{client:building-the-client-libraries-and-examples}}\label{\detokenize{client:section-building-the-client-libraries-and-examples}}
The provided Dockerfile can be used to build just the client libraries
and examples. Issue the following command to build the C++ client
library, C++ and Python examples, and a Python wheel file for the
Python client library:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZdl{} docker build \PYGZhy{}t tensorrtserver\PYGZus{}clients \PYGZhy{}\PYGZhy{}target trtserver\PYGZus{}build \PYGZhy{}\PYGZhy{}build\PYGZhy{}arg \PYGZdq{}BUILD\PYGZus{}CLIENTS\PYGZus{}ONLY=1\PYGZdq{} .
\end{sphinxVerbatim}

You can optionally add \sphinxstyleemphasis{--build-arg “PYVER=\textless{}ver\textgreater{}”} to set the Python
version that you want the Python client library built for. Supported
values for \sphinxstyleemphasis{\textless{}ver\textgreater{}} are 2.6 and 3.5, with 3.5 being the default.

After the build completes the tensorrtserver\_clients docker image will
contain the built client libraries and examples. The easiest way to
try the examples described in the following sections is to run the
client image with --net=host so that the client examples can access
the inference server running in its own container (see
{\hyperref[\detokenize{run:section-running-the-inference-server}]{\sphinxcrossref{\DUrole{std,std-ref}{Running The Inference Server}}}} for more information about
running the inference server):

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZdl{} docker run \PYGZhy{}it \PYGZhy{}\PYGZhy{}rm \PYGZhy{}\PYGZhy{}net=host tensorrtserver\PYGZus{}clients
\end{sphinxVerbatim}

In the client image you can find the example executables in
/opt/tensorrtserver/bin, and the Python wheel in
/opt/tensorrtserver/pip.

If your host sytem is Ubuntu-16.04, an alternative to running the
examples within the tensorrtserver\_clients container is to instead
copy the libraries and examples from the docker image to the host
system:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZdl{} docker run \PYGZhy{}it \PYGZhy{}\PYGZhy{}rm \PYGZhy{}v/tmp:/tmp/host tensorrtserver\PYGZus{}clients
\PYGZsh{} cp /opt/tensorrtserver/bin/image\PYGZus{}client /tmp/host/.
\PYGZsh{} cp /opt/tensorrtserver/bin/perf\PYGZus{}client /tmp/host/.
\PYGZsh{} cp /opt/tensorrtserver/bin/simple\PYGZus{}client /tmp/host/.
\PYGZsh{} cp /opt/tensorrtserver/pip/tensorrtserver\PYGZhy{}*.whl /tmp/host/.
\PYGZsh{} cp /opt/tensorrtserver/lib/librequest.* /tmp/host/.
\end{sphinxVerbatim}

You can now access the files from /tmp on the host system. To run the
C++ examples you must install some dependencies on your host system:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZdl{} apt\PYGZhy{}get install curl libcurl3\PYGZhy{}dev libopencv\PYGZhy{}dev libopencv\PYGZhy{}core\PYGZhy{}dev
\end{sphinxVerbatim}

To run the Python examples you will need to additionally install the
client whl file and some other dependencies:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZdl{} apt\PYGZhy{}get install python3 python3\PYGZhy{}pip
\PYGZdl{} pip3 install \PYGZhy{}\PYGZhy{}user \PYGZhy{}\PYGZhy{}upgrade tensorrtserver\PYGZhy{}*.whl numpy pillow
\end{sphinxVerbatim}


\section{Image Classification Example Application}
\label{\detokenize{client:image-classification-example-application}}\label{\detokenize{client:section-image-classification-example}}
The image classification example that uses the C++ client API is
available at \sphinxhref{https://github.com/NVIDIA/tensorrt-inference-server/blob/master/src/clients/c\%2B\%2B/image\_client.cc}{src/clients/c++/image\_client.cc}. The
Python version of the image classification client is available at
\sphinxhref{https://github.com/NVIDIA/tensorrt-inference-server/blob/master/src/clients/python/image\_client.py}{src/clients/python/image\_client.py}.

To use image\_client (or image\_client.py) you must first have a
running inference server that is serving one or more image
classification models. The image\_client application requires that the
model have a single image input and produce a single classification
output. If you don’t have a model repository with image classification
models see {\hyperref[\detokenize{run:section-example-model-repository}]{\sphinxcrossref{\DUrole{std,std-ref}{Example Model Repository}}}} for instructions on
how to create one.

Follow the instructions in {\hyperref[\detokenize{run:section-running-the-inference-server}]{\sphinxcrossref{\DUrole{std,std-ref}{Running The Inference Server}}}}
to launch the server using the model repository. Once the server is
running you can use the image\_client application to send inference
requests to the server. You can specify a single image or a directory
holding images. Here we send a request for the resnet50\_netdef model
from the {\hyperref[\detokenize{run:section-example-model-repository}]{\sphinxcrossref{\DUrole{std,std-ref}{example model repository}}}} for an image from the \sphinxhref{https://github.com/NVIDIA/tensorrt-inference-server/tree/master/qa/images}{qa/images}
directory:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZdl{} /opt/tensorrtserver/bin/image\PYGZus{}client \PYGZhy{}m resnet50\PYGZus{}netdef \PYGZhy{}s INCEPTION qa/images/mug.jpg
Request 0, batch size 1
Image \PYGZsq{}../qa/images/mug.jpg\PYGZsq{}:
    504 (COFFEE MUG) = 0.723991
\end{sphinxVerbatim}

The Python version of the application accepts the same command-line
arguments:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZdl{} python3 /workspace/src/clients/python/image\PYGZus{}client.py \PYGZhy{}m resnet50\PYGZus{}netdef \PYGZhy{}s INCEPTION qa/images/mug.jpg
Request 0, batch size 1
Image \PYGZsq{}../qa/images/mug.jpg\PYGZsq{}:
    504 (COFFEE MUG) = 0.778078556061
\end{sphinxVerbatim}

The image\_client and image\_client.py applications use the inference
server client library to talk to the server. By default image\_client
instructs the client library to use HTTP protocol to talk to the
server, but you can use GRPC protocol by providing the -i flag. You
must also use the -u flag to point at the GRPC endpoint on the
inference server:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZdl{} /opt/tensorrtserver/bin/image\PYGZus{}client \PYGZhy{}i grpc \PYGZhy{}u localhost:8001 \PYGZhy{}m resnet50\PYGZus{}netdef \PYGZhy{}s INCEPTION qa/images/mug.jpg
Request 0, batch size 1
Image \PYGZsq{}../qa/images/mug.jpg\PYGZsq{}:
    504 (COFFEE MUG) = 0.723991
\end{sphinxVerbatim}

By default the client prints the most probable classification for the
image. Use the -c flag to see more classifications:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZdl{} /opt/tensorrtserver/bin/image\PYGZus{}client \PYGZhy{}m resnet50\PYGZus{}netdef \PYGZhy{}s INCEPTION \PYGZhy{}c 3 qa/images/mug.jpg
Request 0, batch size 1
Image \PYGZsq{}../qa/images/mug.jpg\PYGZsq{}:
    504 (COFFEE MUG) = 0.723991
    968 (CUP) = 0.270953
    967 (ESPRESSO) = 0.00115996
\end{sphinxVerbatim}

The -b flag allows you to send a batch of images for inferencing.
The image\_client application will form the batch from the image or
images that you specified. If the batch is bigger than the number of
images then image\_client will just repeat the images to fill the
batch:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZdl{} /opt/tensorrtserver/bin/image\PYGZus{}client \PYGZhy{}m resnet50\PYGZus{}netdef \PYGZhy{}s INCEPTION \PYGZhy{}c 3 \PYGZhy{}b 2 qa/images/mug.jpg
Request 0, batch size 2
Image \PYGZsq{}../qa/images/mug.jpg\PYGZsq{}:
    504 (COFFEE MUG) = 0.778078556061
    968 (CUP) = 0.213262036443
    967 (ESPRESSO) = 0.00293014757335
Image \PYGZsq{}../qa/images/mug.jpg\PYGZsq{}:
    504 (COFFEE MUG) = 0.778078556061
    968 (CUP) = 0.213262036443
    967 (ESPRESSO) = 0.00293014757335
\end{sphinxVerbatim}

Provide a directory instead of a single image to perform inferencing
on all images in the directory:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZdl{} /opt/tensorrtserver/bin/image\PYGZus{}client \PYGZhy{}m resnet50\PYGZus{}netdef \PYGZhy{}s INCEPTION \PYGZhy{}c 3 \PYGZhy{}b 2 qa/images
Request 0, batch size 2
Image \PYGZsq{}../qa/images/car.jpg\PYGZsq{}:
    817 (SPORTS CAR) = 0.836187
    511 (CONVERTIBLE) = 0.0708251
    751 (RACER) = 0.0597549
Image \PYGZsq{}../qa/images/mug.jpg\PYGZsq{}:
    504 (COFFEE MUG) = 0.723991
    968 (CUP) = 0.270953
    967 (ESPRESSO) = 0.00115996
Request 1, batch size 2
Image \PYGZsq{}../qa/images/vulture.jpeg\PYGZsq{}:
    23 (VULTURE) = 0.992326
    8 (HEN) = 0.00231854
    84 (PEACOCK) = 0.00201471
Image \PYGZsq{}../qa/images/car.jpg\PYGZsq{}:
    817 (SPORTS CAR) = 0.836187
    511 (CONVERTIBLE) = 0.0708251
    751 (RACER) = 0.0597549
\end{sphinxVerbatim}

The grpc\_image\_client.py application at available at
\sphinxhref{https://github.com/NVIDIA/tensorrt-inference-server/blob/master/src/clients/python/grpc\_image\_client.py}{src/clients/python/grpc\_image\_client.py}
behaves the same as the image\_client except that instead of using the
inference server client library it uses the GRPC generated client
library to communicate with the server.


\section{Performance Example Application}
\label{\detokenize{client:performance-example-application}}
The perf\_client example application located at
\sphinxhref{https://github.com/NVIDIA/tensorrt-inference-server/blob/master/src/clients/c\%2B\%2B/perf\_client.cc}{src/clients/c++/perf\_client.cc}
uses the C++ client API to send concurrent requests to the server to
measure latency and inferences per second under varying client loads.

To use perf\_client you must first have a running inference server
that is serving one or more models. The perf\_client application works
with any type of model by sending random data for all input tensors
and by reading and ignoring all output tensors. If you don’t have a
model repository see {\hyperref[\detokenize{run:section-example-model-repository}]{\sphinxcrossref{\DUrole{std,std-ref}{Example Model Repository}}}} for
instructions on how to create one.

Follow the instructions in {\hyperref[\detokenize{run:section-running-the-inference-server}]{\sphinxcrossref{\DUrole{std,std-ref}{Running The Inference Server}}}}
to launch the inference server using the model repository.

The perf\_client application has two major modes. In the first mode
you specify how many concurrent clients you want to simulate and
perf\_client finds a stable latency and inferences/second for that
level of concurrency. Use the -t flag to control concurrency and -v
to see verbose output. The following example simulates four clients
continuously sending requests to the inference server:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZdl{} /opt/tensorrtserver/bin/perf\PYGZus{}client \PYGZhy{}m resnet50\PYGZus{}netdef \PYGZhy{}p3000 \PYGZhy{}t4 \PYGZhy{}v
*** Measurement Settings ***
  Batch size: 1
  Measurement window: 3000 msec

Request concurrency: 4
  Pass [1] throughput: 207 infer/sec. Avg latency: 19268 usec (std 910 usec)
  Pass [2] throughput: 206 infer/sec. Avg latency: 19362 usec (std 941 usec)
  Pass [3] throughput: 208 infer/sec. Avg latency: 19252 usec (std 841 usec)
  Client:
    Request count: 624
    Throughput: 208 infer/sec
    Avg latency: 19252 usec (standard deviation 841 usec)
    Avg HTTP time: 19224 usec (send 714 usec + response wait 18486 usec + receive 24 usec)
  Server:
    Request count: 749
    Avg request latency: 17886 usec (overhead 55 usec + queue 26 usec + compute 17805 usec)
\end{sphinxVerbatim}

In the second mode perf\_client will generate an inferences/second
vs. latency curve by increasing concurrency until a specific latency
limit or concurrency limit is reached. This mode is enabled by using
the -d option and -l to specify the latency limit and optionally the
-c to specify a maximum concurrency limit:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZdl{} /opt/tensorrtserver/bin/perf\PYGZus{}client \PYGZhy{}m resnet50\PYGZus{}netdef \PYGZhy{}p3000 \PYGZhy{}d \PYGZhy{}l50 \PYGZhy{}c 3
*** Measurement Settings ***
  Batch size: 1
  Measurement window: 3000 msec
  Latency limit: 50 msec
  Concurrency limit: 3 concurrent requests

Request concurrency: 1
  Client:
    Request count: 327
    Throughput: 109 infer/sec
    Avg latency: 9191 usec (standard deviation 822 usec)
    Avg HTTP time: 9188 usec (send/recv 1007 usec + response wait 8181 usec)
  Server:
    Request count: 391
    Avg request latency: 7661 usec (overhead 90 usec + queue 68 usec + compute 7503 usec)

Request concurrency: 2
  Client:
    Request count: 521
    Throughput: 173 infer/sec
    Avg latency: 11523 usec (standard deviation 616 usec)
    Avg HTTP time: 11448 usec (send/recv 711 usec + response wait 10737 usec)
  Server:
    Request count: 629
    Avg request latency: 10018 usec (overhead 70 usec + queue 41 usec + compute 9907 usec)

Request concurrency: 3
  Client:
    Request count: 580
    Throughput: 193 infer/sec
    Avg latency: 15518 usec (standard deviation 635 usec)
    Avg HTTP time: 15487 usec (send/recv 779 usec + response wait 14708 usec)
  Server:
    Request count: 697
    Avg request latency: 14083 usec (overhead 59 usec + queue 30 usec + compute 13994 usec)

Inferences/Second vs. Client Average Batch Latency
Concurrency: 1, 109 infer/sec, latency 9191 usec
Concurrency: 2, 173 infer/sec, latency 11523 usec
Concurrency: 3, 193 infer/sec, latency 15518 usec
\end{sphinxVerbatim}

Use the -f flag to generate a file containing CSV output of the
results:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZdl{} /opt/tensorrtserver/bin/perf\PYGZus{}client \PYGZhy{}m resnet50\PYGZus{}netdef \PYGZhy{}p3000 \PYGZhy{}d \PYGZhy{}l50 \PYGZhy{}c 3 \PYGZhy{}f perf.csv
\end{sphinxVerbatim}

You can then import the CSV file into a spreadsheet to help visualize
the latency vs inferences/second tradeoff as well as see some
components of the latency. Follow these steps:
\begin{itemize}
\item {} 
Open \sphinxhref{https://docs.google.com/spreadsheets/d/1zszgmbSNHHXy0DVEU\_4lrL4Md-6dUKwy\_mLVmcseUrE}{this spreadsheet}

\item {} 
Make a copy from the File menu “Make a copy…”

\item {} 
Open the copy

\item {} 
Select the A2 cell

\item {} 
From the File menu select “Import…”

\item {} 
Select “Upload” and upload the file

\item {} 
Select “Replace data at selected cell” and then select the “Import data” button

\end{itemize}


\section{Client API}
\label{\detokenize{client:client-api}}\label{\detokenize{client:section-client-api}}
The C++ client API exposes a class-based interface for querying server
and model status and for performing inference. The commented interface
is available at \sphinxhref{https://github.com/NVIDIA/tensorrt-inference-server/blob/master/src/clients/c\%2B\%2B/request.h}{src/clients/c++/request.h}
and in the API Reference.

The Python client API provides similar capabilities as the C++
API. The commented interface is available at
\sphinxhref{https://github.com/NVIDIA/tensorrt-inference-server/blob/master/src/clients/python/\_\_init\_\_.py}{src/clients/python/\_\_init\_\_.py}
and in the API Reference.

A simple C++ example application at \sphinxhref{https://github.com/NVIDIA/tensorrt-inference-server/blob/master/src/clients/c\%2B\%2B/simple\_client.cc}{src/clients/c++/simple\_client.cc}
and a Python version at \sphinxhref{https://github.com/NVIDIA/tensorrt-inference-server/blob/master/src/clients/python/simple\_client.py}{src/clients/python/simple\_client.py}
demonstrate basic client API usage.

To run the the C++ version of the simple example, first build as
described in {\hyperref[\detokenize{client:section-building-the-client-libraries-and-examples}]{\sphinxcrossref{\DUrole{std,std-ref}{Building the Client Libraries and Examples}}}}
and then:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZdl{} /opt/tensorrtserver/bin/simple\PYGZus{}client
0 + 1 = 1
0 \PYGZhy{} 1 = \PYGZhy{}1
1 + 1 = 2
1 \PYGZhy{} 1 = 0
2 + 1 = 3
2 \PYGZhy{} 1 = 1
3 + 1 = 4
3 \PYGZhy{} 1 = 2
4 + 1 = 5
4 \PYGZhy{} 1 = 3
5 + 1 = 6
5 \PYGZhy{} 1 = 4
6 + 1 = 7
6 \PYGZhy{} 1 = 5
7 + 1 = 8
7 \PYGZhy{} 1 = 6
8 + 1 = 9
8 \PYGZhy{} 1 = 7
9 + 1 = 10
9 \PYGZhy{} 1 = 8
10 + 1 = 11
10 \PYGZhy{} 1 = 9
11 + 1 = 12
11 \PYGZhy{} 1 = 10
12 + 1 = 13
12 \PYGZhy{} 1 = 11
13 + 1 = 14
13 \PYGZhy{} 1 = 12
14 + 1 = 15
14 \PYGZhy{} 1 = 13
15 + 1 = 16
15 \PYGZhy{} 1 = 14
\end{sphinxVerbatim}

To run the the Python version of the simple example, first build as
described in {\hyperref[\detokenize{client:section-building-the-client-libraries-and-examples}]{\sphinxcrossref{\DUrole{std,std-ref}{Building the Client Libraries and Examples}}}}
and install the tensorrtserver whl, then:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZdl{} python3 /workspace/src/clients/python/simple\PYGZus{}client.py
\end{sphinxVerbatim}


\subsection{String Datatype}
\label{\detokenize{client:string-datatype}}
Some frameworks support tensors where each element in the tensor is a
string (see {\hyperref[\detokenize{model_configuration:section-datatypes}]{\sphinxcrossref{\DUrole{std,std-ref}{Datatypes}}}} for information on supported
datatypes). For the most part, the Client API is identical for string
and non-string tensors. One exception is that in the C++ API a string
input tensor must be initialized with SetFromString() instead of
SetRaw().

String tensors are demonstrated in the C++ example application at
\sphinxhref{https://github.com/NVIDIA/tensorrt-inference-server/blob/master/src/clients/c\%2B\%2B/simple\_string\_client.cc}{src/clients/c++/simple\_string\_client.cc}
and a Python version at \sphinxhref{https://github.com/NVIDIA/tensorrt-inference-server/blob/master/src/clients/python/simple\_string\_client.py}{src/clients/python/simple\_string\_client.py}.


\subsection{Stream Inference}
\label{\detokenize{client:stream-inference}}
Some applications may prefer to send requests in one connection rather than
establishing connections for individual requests. For instance, in the case
where multiple instances of TensorRT Inference Server are created with the
purpose of load balancing, requests sent in different connections may be routed
to different server instances. This scenario will not fit the need if the
requests are correlated, where they are expected to be processed by the same
model instance, like inferencing with sequence models. By using stream
inference, the requests will be sent to the same server instance once the
connection is established, and then they will be processed by the same model
instance if proper \sphinxcode{\sphinxupquote{correlation\_id}} is set.

Stream inference and use of correlation ID are demonstrated in the C++ example
application at
\sphinxhref{https://github.com/NVIDIA/tensorrt-inference-server/blob/master/src/clients/c\%2B\%2B/simple\_sequence\_client.cc}{src/clients/c++/simple\_sequence\_client.cc}
and a Python version at \sphinxhref{https://github.com/NVIDIA/tensorrt-inference-server/blob/master/src/clients/python/simple\_sequence\_client.py}{src/clients/python/simple\_sequence\_client.py}.


\chapter{Model Repository}
\label{\detokenize{model_repository:model-repository}}\label{\detokenize{model_repository:section-model-repository}}\label{\detokenize{model_repository::doc}}
The TensorRT Inference Server accesses models from a locally
accessible file path or from Google Cloud Storage. This path is
specified when the server is started using the --model-store option.

For a locally accessible file-system the absolute path must be
specified, for example, --model-store=/path/to/model/repository. For
a model repository residing in Google Cloud Storage, the path must be
prefixed with gs://, for example,
--model-store=gs://bucket/path/to/model/repository.

{\hyperref[\detokenize{run:section-example-model-repository}]{\sphinxcrossref{\DUrole{std,std-ref}{Example Model Repository}}}} describes how to create an
example repository with a couple if image classification models.

An example of a typical model repository layout is shown below:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}model\PYGZhy{}repository\PYGZhy{}path\PYGZgt{}/
  model\PYGZus{}0/
    config.pbtxt
    output0\PYGZus{}labels.txt
    1/
      model.plan
    2/
      model.plan
  model\PYGZus{}1/
    config.pbtxt
    output0\PYGZus{}labels.txt
    output1\PYGZus{}labels.txt
    0/
      model.graphdef
    7/
      model.graphdef
\end{sphinxVerbatim}

Any number of models may be specified and the inference server will
attempt to load all models into the CPU and GPU when the server
starts. The {\hyperref[\detokenize{http_grpc_api:section-api-status}]{\sphinxcrossref{\DUrole{std,std-ref}{Status API}}}} can be used to
determine if any models failed to load successfully. The server’s
console log will also show the reason for any failures during startup.

The name of the model directory (model\_0 and model\_1 in the above
example) must match the name of the model specified in the
{\hyperref[\detokenize{model_configuration:section-model-configuration}]{\sphinxcrossref{\DUrole{std,std-ref}{model configuration file}}}},
config.pbtxt. The model name is used in the {\hyperref[\detokenize{client:section-client-api}]{\sphinxcrossref{\DUrole{std,std-ref}{client API}}}} and {\hyperref[\detokenize{http_grpc_api:section-inference-server-api}]{\sphinxcrossref{\DUrole{std,std-ref}{server API}}}} to identify the model. Each model
directory must have at least one numeric subdirectory. Each of these
subdirectories holds a version of the model with the version number
corresponding to the directory name.

For more information about how the model versions are handled by the
server see {\hyperref[\detokenize{model_repository:section-model-versions}]{\sphinxcrossref{\DUrole{std,std-ref}{Model Versions}}}}.  Within each version
subdirectory there are one or more model definition files that specify
the actual model. The model definition can be either a
{\hyperref[\detokenize{model_repository:section-framework-model-definition}]{\sphinxcrossref{\DUrole{std,std-ref}{framework-specific model file}}}} or a shared library implementing
a {\hyperref[\detokenize{model_repository:section-custom-backends}]{\sphinxcrossref{\DUrole{std,std-ref}{custom backend}}}}.

The *\_labels.txt files are optional and are used to provide labels for
outputs that represent classifications. The label file must be
specified in the \sphinxcode{\sphinxupquote{label\_filename}} property of
the output it corresponds to in the {\hyperref[\detokenize{model_configuration:section-model-configuration}]{\sphinxcrossref{\DUrole{std,std-ref}{model configuration}}}}.


\section{Modifying the Model Repository}
\label{\detokenize{model_repository:modifying-the-model-repository}}\label{\detokenize{model_repository:section-modifying-the-model-repository}}
By default, changes to the model repository will be detected and the
server will attempt to add, remove, and reload models as necessary
based on those changes. Changes to the model repository may not be
detected immediately because the server polls the repository
periodically. You can control the polling interval with the
--repository-poll-secs options. The console log or the {\hyperref[\detokenize{http_grpc_api:section-api-status}]{\sphinxcrossref{\DUrole{std,std-ref}{Status
API}}}} can be used to determine when model
repository changes have taken effect. You can disable the server from
responding to repository changes by using the
--allow-poll-model-repository=false option.

The TensorRT Inference Server responds to the following changes:
\begin{itemize}
\item {} 
Versions may be added and removed from models by adding and removing
the corresponding version subdirectory. The inference server will
allow in-flight requests to complete even if they are using a
removed version of the model. New requests for a removed model
version will fail. Depending on the model’s {\hyperref[\detokenize{model_configuration:section-version-policy}]{\sphinxcrossref{\DUrole{std,std-ref}{version policy}}}}, changes to the available versions may
change which model version is served by default.

\item {} 
Existing models can be removed from the repository by removing the
corresponding model directory.  The inference server will allow
in-flight requests to any version of the removed model to
complete. New requests for a removed model will fail.

\item {} 
New models can be added to the repository by adding a new model
directory.

\item {} 
The {\hyperref[\detokenize{model_configuration:section-model-configuration}]{\sphinxcrossref{\DUrole{std,std-ref}{model configuration}}}}
(config.pbtxt) can be changed and the server will unload and reload
the model to pick up the new model configuration.

\item {} 
Labels files providing labels for outputs that represent
classifications can be added, removed, or modified and the inference
server will unload and reload the model to pick up the new
labels. If a label file is added or removed the corresponding edit
to the \sphinxcode{\sphinxupquote{label\_filename}} property of
the output it corresponds to in the {\hyperref[\detokenize{model_configuration:section-model-configuration}]{\sphinxcrossref{\DUrole{std,std-ref}{model configuration}}}} must be performed at the same time.

\end{itemize}


\section{Model Versions}
\label{\detokenize{model_repository:model-versions}}\label{\detokenize{model_repository:section-model-versions}}
Each model can have one or more versions available in the model
repository. Each version is stored in its own, numerically named,
subdirectory where the name of the subdirectory corresponds to the
version number of the model. Each model specifies a {\hyperref[\detokenize{model_configuration:section-version-policy}]{\sphinxcrossref{\DUrole{std,std-ref}{version
policy}}}} that controls which of the versions
in the model repository are made available by the server at any given
time.


\section{Framework Model Definition}
\label{\detokenize{model_repository:framework-model-definition}}\label{\detokenize{model_repository:section-framework-model-definition}}
Each model version subdirectory must contain at least one model
definition. By default, the name of this file or directory must be:
\begin{itemize}
\item {} 
\sphinxstylestrong{model.plan} for TensorRT models

\item {} 
\sphinxstylestrong{model.graphdef} for TensorFlow GraphDef models

\item {} 
\sphinxstylestrong{model.savedmodel} for TensorFlow SavedModel models

\item {} 
\sphinxstylestrong{model.netdef} and \sphinxstylestrong{init\_model.netdef} for Caffe2 Netdef models

\end{itemize}

This default name can be overridden using the \sphinxstyleemphasis{default\_model\_filename}
property in the {\hyperref[\detokenize{model_configuration:section-model-configuration}]{\sphinxcrossref{\DUrole{std,std-ref}{model configuration}}}}.

Optionally, a model can provide multiple model definition files, each
targeted at a GPU with a different \sphinxhref{https://developer.nvidia.com/cuda-gpus}{Compute Capability}. Most commonly, this
feature is needed for TensorRT and TensorFlow/TensorRT integrated
models where the model definition is valid for only a single compute
capability. See the \sphinxstyleemphasis{cc\_model\_filenames} property in the {\hyperref[\detokenize{model_configuration:section-model-configuration}]{\sphinxcrossref{\DUrole{std,std-ref}{model
configuration}}}} for description of how to
specify different model definitions for different compute
capabilities.


\subsection{TensorRT Models}
\label{\detokenize{model_repository:tensorrt-models}}\label{\detokenize{model_repository:section-tensorrt-models}}
A TensorRT model definition is called a \sphinxstyleemphasis{Plan}. A TensorRT Plan is a
single file that by default must be named model.plan. A TensorRT Plan
is specific to CUDA Compute Capability and so it is typically
necessary to use the {\hyperref[\detokenize{model_configuration:section-model-configuration}]{\sphinxcrossref{\DUrole{std,std-ref}{model configuration’s}}}} \sphinxstyleemphasis{cc\_model\_filenames} property as
described above.

A minimal model repository for a single TensorRT model would look
like:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
models/
  \PYGZlt{}model\PYGZhy{}name\PYGZgt{}/
    config.pbtxt
    1/
      model.plan
\end{sphinxVerbatim}

As described in {\hyperref[\detokenize{model_configuration:section-generated-model-configuration}]{\sphinxcrossref{\DUrole{std,std-ref}{Generated Model Configuration}}}} the
config.pbtxt is optional for some models. In cases where it is not
required the minimal model repository would look like:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
models/
  \PYGZlt{}model\PYGZhy{}name\PYGZgt{}/
    1/
      model.plan
\end{sphinxVerbatim}


\subsection{TensorFlow Models}
\label{\detokenize{model_repository:tensorflow-models}}\label{\detokenize{model_repository:section-tensorflow-models}}
TensorFlow saves trained models in one of two ways: \sphinxstyleemphasis{GraphDef} or
\sphinxstyleemphasis{SavedModel}. The inference server supports both formats. Once you
have a trained model in TensorFlow, you can save it as a GraphDef
directly or convert it to a GraphDef by using a script like
\sphinxhref{https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/tools/freeze\_graph.py}{freeze\_graph.py},
or save it as a SavedModel using a \sphinxhref{https://www.tensorflow.org/serving/serving\_basic}{SavedModelBuilder} or
\sphinxhref{https://www.tensorflow.org/api\_docs/python/tf/saved\_model/simple\_save}{tf.saved\_model.simple\_save}.

A TensorFlow GraphDef is a single file that by default must be named
model.graphdef. A minimal model repository for a single TensorFlow
GraphDef model would look like:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
models/
  \PYGZlt{}model\PYGZhy{}name\PYGZgt{}/
    config.pbtxt
    1/
      model.graphdef
\end{sphinxVerbatim}

A TensorFlow SavedModel is a directory containing multiple files. By
default the directory must be named model.savedmodel. A minimal model
repository for a single TensorFlow SavedModel model would look like:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
models/
  \PYGZlt{}model\PYGZhy{}name\PYGZgt{}/
    config.pbtxt
    1/
      model.savedmodel/
         \PYGZlt{}saved\PYGZhy{}model files\PYGZgt{}
\end{sphinxVerbatim}

As described in {\hyperref[\detokenize{model_configuration:section-generated-model-configuration}]{\sphinxcrossref{\DUrole{std,std-ref}{Generated Model Configuration}}}} the
config.pbtxt is optional for some models. In cases where it is not
required the minimal model repository would look like:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
models/
  \PYGZlt{}model\PYGZhy{}name\PYGZgt{}/
    1/
      model.savedmodel/
         \PYGZlt{}saved\PYGZhy{}model files\PYGZgt{}
\end{sphinxVerbatim}


\subsection{Caffe2 Models}
\label{\detokenize{model_repository:caffe2-models}}
A Caffe2 model definition is called a \sphinxstyleemphasis{NetDef}. A Caffe2 NetDef is a
single file that by default must be named model.netdef. A minimal
model repository for a single NetDef model would look like:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
models/
  \PYGZlt{}model\PYGZhy{}name\PYGZgt{}/
    config.pbtxt
    1/
      model.netdef
\end{sphinxVerbatim}


\subsection{TensorRT/TensorFlow Models}
\label{\detokenize{model_repository:tensorrt-tensorflow-models}}
TensorFlow 1.7 and later integrates TensorRT to enable TensorFlow
models to benefit from the inference optimizations provided by
TensorRT. The inference server supports models that have been
optimized with TensorRT and can serve those models just like any other
TensorFlow model. The inference server’s TensorRT version (available
in the Release Notes) must match the TensorRT version that was used
when the model was created.

A TensorRT/TensorFlow integrated model is specific to CUDA Compute
Capability and so it is typically necessary to use the {\hyperref[\detokenize{model_configuration:section-model-configuration}]{\sphinxcrossref{\DUrole{std,std-ref}{model
configuration’s}}}} \sphinxstyleemphasis{cc\_model\_filenames}
property as described above.


\subsection{ONNX Models}
\label{\detokenize{model_repository:onnx-models}}
The TensorRT Inference Server cannot directly perform inferencing
using \sphinxhref{http://onnx.ai/}{ONNX} models. An ONNX model must be
converted to either a TensorRT Plan or a Caffe2 NetDef. To convert
your ONNX model to a TensorRT Plan use either the \sphinxhref{https://docs.nvidia.com/deeplearning/sdk/tensorrt-developer-guide/index.html\#api}{ONNX Parser}
included in TensorRT or the \sphinxhref{https://github.com/onnx/onnx-tensorrt}{open-source TensorRT backend for ONNX}. Another option is to
convert your ONNX model to Caffe2 NetDef \sphinxhref{https://github.com/pytorch/pytorch/tree/master/caffe2/python/onnx}{as described here}.


\section{Custom Backends}
\label{\detokenize{model_repository:custom-backends}}\label{\detokenize{model_repository:section-custom-backends}}
A model using a custom backend is represented in the model repository
in the same way as models using a deep-learning framework backend.
Each model version subdirectory must contain at least one shared
library that implements the custom model backend. By default, the name
of this shared library must be \sphinxstylestrong{libcustom.so} but the default name
can be overridden using the \sphinxstyleemphasis{default\_model\_filename} property in the
{\hyperref[\detokenize{model_configuration:section-model-configuration}]{\sphinxcrossref{\DUrole{std,std-ref}{model configuration}}}}.

Optionally, a model can provide multiple shared libraries, each
targeted at a GPU with a different \sphinxhref{https://developer.nvidia.com/cuda-gpus}{Compute Capability}. See the
\sphinxstyleemphasis{cc\_model\_filenames} property in the {\hyperref[\detokenize{model_configuration:section-model-configuration}]{\sphinxcrossref{\DUrole{std,std-ref}{model configuration}}}} for description of how to specify
different shared libraries for different compute capabilities.


\subsection{Custom Backend API}
\label{\detokenize{model_repository:custom-backend-api}}
A custom backend must implement the C interface defined in \sphinxhref{https://github.com/NVIDIA/tensorrt-inference-server/blob/master/src/servables/custom/custom.h}{custom.h}. The
interface is also documented in the \sphinxhref{file:///home/david/dev/github/NVIDIA/tensorrt-inference-server/docs/build/html/cpp\_api/file\_src\_servables\_custom\_custom.h.html}{API Reference}.


\subsection{Example Custom Backend}
\label{\detokenize{model_repository:example-custom-backend}}
An example of a custom backend can be found in the \sphinxhref{https://github.com/NVIDIA/tensorrt-inference-server/blob/master/src/custom/addsub/addsub.cc}{addsub backend}. You
can see the custom backend being used as part of CI testing in
\sphinxhref{https://github.com/NVIDIA/tensorrt-inference-server/tree/master/qa/L0\_infer}{L0\_infer}.


\chapter{Model Configuration}
\label{\detokenize{model_configuration:model-configuration}}\label{\detokenize{model_configuration:section-model-configuration}}\label{\detokenize{model_configuration::doc}}
Each model in a {\hyperref[\detokenize{model_repository:section-model-repository}]{\sphinxcrossref{\DUrole{std,std-ref}{Model Repository}}}} must include a model
configuration that provides required and optional information about
the model. Typically, this configuration is provided in a config.pbtxt
file specified as \DUrole{xref,std,std-doc}{ModelConfig}
protobuf. In some cases, discussed in
{\hyperref[\detokenize{model_configuration:section-generated-model-configuration}]{\sphinxcrossref{\DUrole{std,std-ref}{Generated Model Configuration}}}}, the model configuration
can be generated automatically by the inference server and so does not
need to be provided explicitly.

A minimal model configuration must specify \sphinxcode{\sphinxupquote{name}}, \sphinxcode{\sphinxupquote{platform}},
\sphinxcode{\sphinxupquote{max\_batch\_size}},
\sphinxcode{\sphinxupquote{input}}, and
\sphinxcode{\sphinxupquote{output}}.

As a running example consider a TensorRT model called \sphinxstyleemphasis{mymodel} that
has two inputs, \sphinxstyleemphasis{input0} and \sphinxstyleemphasis{input1}, and one output, \sphinxstyleemphasis{output0}, all
of which are 16 entry float32 tensors. The minimal configuration is:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
name: \PYGZdq{}mymodel\PYGZdq{}
platform: \PYGZdq{}tensorrt\PYGZus{}plan\PYGZdq{}
max\PYGZus{}batch\PYGZus{}size: 8
input [
  \PYGZob{}
    name: \PYGZdq{}input0\PYGZdq{}
    data\PYGZus{}type: TYPE\PYGZus{}FP32
    dims: [ 16 ]
  \PYGZcb{},
  \PYGZob{}
    name: \PYGZdq{}input1\PYGZdq{}
    data\PYGZus{}type: TYPE\PYGZus{}FP32
    dims: [ 16 ]
  \PYGZcb{}
]
output [
  \PYGZob{}
    name: \PYGZdq{}output0\PYGZdq{}
    data\PYGZus{}type: TYPE\PYGZus{}FP32
    dims: [ 16 ]
  \PYGZcb{}
]
\end{sphinxVerbatim}

The name of the model must match the \sphinxcode{\sphinxupquote{name}} of the model repository
directory containing the model. The \sphinxcode{\sphinxupquote{platform}} must be one of
\sphinxstylestrong{tensorrt\_plan}, \sphinxstylestrong{tensorflow\_graphdef}, \sphinxstylestrong{tensorflow\_savedmodel},
\sphinxstylestrong{caffe2\_netdef}, or \sphinxstylestrong{custom}.

The datatypes allowed for input and output tensors varies based on the
type of the model. Section {\hyperref[\detokenize{model_configuration:section-datatypes}]{\sphinxcrossref{\DUrole{std,std-ref}{Datatypes}}}} describes the
allowed datatypes and how they map to the datatypes of each model
type.

For models that support batched inputs the \sphinxcode{\sphinxupquote{max\_batch\_size}} value must be
\textgreater{}= 1. The TensorRT Inference Server assumes that the batching occurs
along a first dimension that is not listed in the inputs or
outputs. For the above example, the server expects to receive input
tensors with shape \sphinxstylestrong{{[} x, 16 {]}} and produces an output tensor with
shape \sphinxstylestrong{{[} x, 16 {]}}, where \sphinxstylestrong{x} is the batch size of the request.

For models that do not support batched inputs the
\sphinxcode{\sphinxupquote{max\_batch\_size}} value must be
zero. If the above example specified a \sphinxcode{\sphinxupquote{max\_batch\_size}} of zero, the
inference server would expect to receive input tensors with shape \sphinxstylestrong{{[}
16 {]}}, and would produce an output tensor with shape \sphinxstylestrong{{[} 16 {]}}.

For models that support input and output tensors with variable-size
dimensions, those dimensions can be listed as -1 in the input and
output configuration. For example, if a model requires a 2-dimensional
input tensor where the first dimension must be size 4 but the second
dimension can be any size, the model configuration for that input
would include \sphinxstylestrong{dims: {[} 4, -1 {]}}. The inference server would then
accept inference requests where that input tensor’s second dimension
was any value \textgreater{}= 1. The model configuration can be more restrictive
than what is allowed by the underlying model. For example, even though
the model allows the second dimension to be any size, the model
configuration could be specific as \sphinxstylestrong{dims: {[} 4, 4 {]}}. In this case,
the inference server would only accept inference requests where the
input tensor’s shape was exactly \sphinxstylestrong{{[} 4, 4 {]}}.


\section{Generated Model Configuration}
\label{\detokenize{model_configuration:generated-model-configuration}}\label{\detokenize{model_configuration:section-generated-model-configuration}}
By default, the model configuration file containing the required
settings must be provided with each model. However, if the server is
started with the --strict-model-config=false option, then in some
cases the required portions of the model configuration file can be
generated automatically by the inference server. The required portion
of the model configuration are those settings shown in the example
minimal configuration above. Specifically:
\begin{itemize}
\item {} 
{\hyperref[\detokenize{model_repository:section-tensorrt-models}]{\sphinxcrossref{\DUrole{std,std-ref}{TensorRT Plan}}}} models do not require
a model configuration file because the inference server can derive
all the required settings automatically.

\item {} 
Some {\hyperref[\detokenize{model_repository:section-tensorflow-models}]{\sphinxcrossref{\DUrole{std,std-ref}{TensorFlow SavedModel}}}} models
do not require a model configuration file. The models must specify
all inputs and outputs as fixed-size tensors (with an optional
initial batch dimension) for the model configuration to be generated
automatically. The easiest way to determine if a particular
SavedModel is supported is to try it with the server and check the
console log and {\hyperref[\detokenize{http_grpc_api:section-api-status}]{\sphinxcrossref{\DUrole{std,std-ref}{Status API}}}} to determine
if the model loaded successfully.

\end{itemize}

When using --strict-model-config=false you can see the model
configuration that was generated for a model by using the {\hyperref[\detokenize{http_grpc_api:section-api-status}]{\sphinxcrossref{\DUrole{std,std-ref}{Status
API}}}}.

The TensorRT Inference Server only generates the required portion of
the model configuration file. You must still provide the optional
portions of the model configuration if necessary, such as
\sphinxcode{\sphinxupquote{version\_policy}},
\sphinxcode{\sphinxupquote{optimization}},
\sphinxcode{\sphinxupquote{dynamic\_batching}},
\sphinxcode{\sphinxupquote{instance\_group}},
\sphinxcode{\sphinxupquote{default\_model\_filename}},
\sphinxcode{\sphinxupquote{cc\_model\_filenames}}, and
\sphinxcode{\sphinxupquote{tags}}.


\section{Datatypes}
\label{\detokenize{model_configuration:datatypes}}\label{\detokenize{model_configuration:section-datatypes}}
The following table shows the tensor datatypes supported by the
TensorRT Inference Server. The first column shows the name of the
datatype as it appears in the model configuration file. The other
columns show the corresponding datatype for the model frameworks
supported by the server and for the Python numpy library. If a model
framework does not have an entry for a given datatype, then the
inference server does not support that datatype for that model.


\begin{savenotes}\sphinxattablestart
\centering
\begin{tabulary}{\linewidth}[t]{|T|T|T|T|T|}
\hline
\sphinxstyletheadfamily 
Type
&\sphinxstyletheadfamily 
TensorRT
&\sphinxstyletheadfamily 
TensorFlow
&\sphinxstyletheadfamily 
Caffe2
&\sphinxstyletheadfamily 
NumPy
\\
\hline
TYPE\_BOOL
&&
DT\_BOOL
&
BOOL
&
bool
\\
\hline
TYPE\_UINT8
&&
DT\_UINT8
&
UINT8
&
uint8
\\
\hline
TYPE\_UINT16
&&
DT\_UINT16
&
UINT16
&
uint16
\\
\hline
TYPE\_UINT32
&&
DT\_UINT32
&&
uint32
\\
\hline
TYPE\_UINT64
&&
DT\_UINT64
&&
uint64
\\
\hline
TYPE\_INT8
&
kINT8
&
DT\_INT8
&
INT8
&
int8
\\
\hline
TYPE\_INT16
&&
DT\_INT16
&
INT16
&
int16
\\
\hline
TYPE\_INT32
&
kINT32
&
DT\_INT32
&
INT32
&
int32
\\
\hline
TYPE\_INT64
&&
DT\_INT64
&
INT64
&
int64
\\
\hline
TYPE\_FP16
&
kHALF
&
DT\_HALF
&
FLOAT16
&
float16
\\
\hline
TYPE\_FP32
&
kFLOAT
&
DT\_FLOAT
&
FLOAT
&
float32
\\
\hline
TYPE\_FP64
&&
DT\_DOUBLE
&
DOUBLE
&
float64
\\
\hline
TYPE\_STRING
&&
DT\_STRING
&&
dtype(object)
\\
\hline
\end{tabulary}
\par
\sphinxattableend\end{savenotes}

For TensorRT each value is in the nvinfer1::DataType namespace. For
example, nvinfer1::DataType::kFLOAT is the 32-bit floating-point
datatype.

For TensorFlow each value is in the tensorflow namespace. For example,
tensorflow::DT\_FLOAT is the 32-bit floating-point value.

For Caffe2 each value is in the caffe2 namespace and is prepended with
{\color{red}\bfseries{}TensorProto\_DataType\_}. For example, caffe2::TensorProto\_DataType\_FLOAT
is the 32-bit floating-point datatype.

For Numpy each value is in the numpy module. For example, numpy.float32
is the 32-bit floating-point datatype.


\section{Version Policy}
\label{\detokenize{model_configuration:version-policy}}\label{\detokenize{model_configuration:section-version-policy}}
Each model can have one or more {\hyperref[\detokenize{model_repository:section-model-versions}]{\sphinxcrossref{\DUrole{std,std-ref}{versions available in the model
repository}}}}. The
\sphinxcode{\sphinxupquote{nvidia::inferenceserver::ModelVersionPolicy}} schema allows
the following policies.
\begin{itemize}
\item {} 
\sphinxcode{\sphinxupquote{All}}: All versions
of the model that are available in the model repository are
available for inferencing.

\item {} 
\sphinxcode{\sphinxupquote{Latest}}: Only the
latest ‘n’ versions of the model in the repository are available for
inferencing. The latest versions of the model are the numerically
greatest version numbers.

\item {} 
\sphinxcode{\sphinxupquote{Specific}}: Only the
specifically listed versions of the model are available for
inferencing.

\end{itemize}

If no version policy is specified, then \sphinxcode{\sphinxupquote{Latest}} (with
num\_version = 1) is used as the default, indicating that only the most
recent version of the model is made available by the inference
server. In all cases, the addition or removal of version
subdirectories from the model repository can change which model
version is used on subsequent inference requests.

Continuing the above example, the following configuration specifies
that all versions of the model will be available from the server:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
name: \PYGZdq{}mymodel\PYGZdq{}
platform: \PYGZdq{}tensorrt\PYGZus{}plan\PYGZdq{}
max\PYGZus{}batch\PYGZus{}size: 8
input [
  \PYGZob{}
    name: \PYGZdq{}input0\PYGZdq{}
    data\PYGZus{}type: TYPE\PYGZus{}FP32
    dims: [ 16 ]
  \PYGZcb{},
  \PYGZob{}
    name: \PYGZdq{}input1\PYGZdq{}
    data\PYGZus{}type: TYPE\PYGZus{}FP32
    dims: [ 16 ]
  \PYGZcb{}
]
output [
  \PYGZob{}
    name: \PYGZdq{}output0\PYGZdq{}
    data\PYGZus{}type: TYPE\PYGZus{}FP32
    dims: [ 16 ]
  \PYGZcb{}
]
version\PYGZus{}policy: \PYGZob{} all \PYGZob{} \PYGZcb{}\PYGZcb{}
\end{sphinxVerbatim}


\section{Instance Groups}
\label{\detokenize{model_configuration:instance-groups}}\label{\detokenize{model_configuration:section-instance-groups}}
The TensorRT Inference Server can provide multiple {\hyperref[\detokenize{architecture:section-concurrent-model-execution}]{\sphinxcrossref{\DUrole{std,std-ref}{execution
instances}}}} of a model so that
multiple simultaneous inference requests for that model can be handled
simultaneously. The model configuration \sphinxcode{\sphinxupquote{ModelInstanceGroup}} is used to specify the
number of execution instances that should be made available and what
compute resource should be used for those instances.

By default, a single execution instance of the model is created for
each GPU available in the system. The instance-group setting can be
used to place multiple execution instances of a model on every GPU or
on only certain GPUs. For example, the following configuration will
place two execution instances of the model to be available on each
system GPU:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
instance\PYGZus{}group [
  \PYGZob{}
    count: 2
    kind: KIND\PYGZus{}GPU
  \PYGZcb{}
]
\end{sphinxVerbatim}

And the following configuration will place one execution instance on
GPU 0 and two execution instances on GPUs 1 and 2:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
instance\PYGZus{}group [
  \PYGZob{}
    count: 1
    kind: KIND\PYGZus{}GPU
    gpus: [ 0 ]
  \PYGZcb{},
  \PYGZob{}
    count: 2
    kind: KIND\PYGZus{}GPU
    gpus: [ 1, 2 ]
  \PYGZcb{}
]
\end{sphinxVerbatim}

The instance group setting is also used to enable exection of a model
on the CPU. The following places two execution instances on the CPU:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
instance\PYGZus{}group [
  \PYGZob{}
    count: 2
    kind: KIND\PYGZus{}CPU
  \PYGZcb{}
]
\end{sphinxVerbatim}


\section{Dynamic Batching}
\label{\detokenize{model_configuration:dynamic-batching}}\label{\detokenize{model_configuration:section-dynamic-batching}}
The TensorRT Inference Server supports batch inferencing by allowing
individual inference requests to specify a batch of inputs. The
inferencing for a batch of inputs is processed at the same time which
is especially important for GPUs since it can greatly increase
inferencing throughput. In many use-cases the individual inference
requests are not batched, therefore, they do not benefit from the
throughput benefits of batching.

Dynamic batching is a feature of the inference server that allows
non-batched inference requests to be combined by the server, so that a
batch is created dynamically, resulting in the same increased
throughput seen for batched inference requests.

Dynamic batching is enabled and configured independently for each
model using the \sphinxcode{\sphinxupquote{ModelDynamicBatching}} settings in the model
configuration. These settings control the preferred size(s) of the
dynamically created batches as well as a maximum time that requests
can be delayed in the scheduler to allow other requests to join the
dynamic batch.

The following configuration enables dynamic batching with preferred
batch sizes of 4 and 8, and a maximum delay time of 100 microseconds:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
dynamic\PYGZus{}batching \PYGZob{}
  preferred\PYGZus{}batch\PYGZus{}size: [ 4, 8 ]
  max\PYGZus{}queue\PYGZus{}delay\PYGZus{}microseconds: 100
\PYGZcb{}
\end{sphinxVerbatim}


\section{Optimization Policy}
\label{\detokenize{model_configuration:optimization-policy}}\label{\detokenize{model_configuration:section-optimization-policy}}
The model configuration \sphinxcode{\sphinxupquote{ModelOptimizationPolicy}} is used to specify
optimization and prioritization settings for a model. These settings
control if/how a model is optimized by the backend framework and how
it is scheduled and executed by the inference server. See the protobuf
documentation for the currently available settings.


\chapter{Inference Server API}
\label{\detokenize{http_grpc_api:inference-server-api}}\label{\detokenize{http_grpc_api:section-inference-server-api}}\label{\detokenize{http_grpc_api::doc}}
The TensorRT Inference Server exposes both HTTP and GRPC
endpoints. Three endpoints with identical functionality are exposed
for each protocol.
\begin{itemize}
\item {} 
{\hyperref[\detokenize{http_grpc_api:section-api-health}]{\sphinxcrossref{\DUrole{std,std-ref}{Health}}}}: The server health API for determining
server liveness and readiness.

\item {} 
{\hyperref[\detokenize{http_grpc_api:section-api-status}]{\sphinxcrossref{\DUrole{std,std-ref}{Status}}}}: The server status API for getting
information about the server and about the models being served.

\item {} 
{\hyperref[\detokenize{http_grpc_api:section-api-inference}]{\sphinxcrossref{\DUrole{std,std-ref}{Inference}}}}: The inference API that accepts model
inputs, runs inference and returns the requested outputs.

\end{itemize}

The inference server also exposes an endpoint based on GRPC streams that is
only available when using the GRPC protocol:
\begin{itemize}
\item {} 
{\hyperref[\detokenize{http_grpc_api:section-api-stream-inference}]{\sphinxcrossref{\DUrole{std,std-ref}{Stream Inference}}}}: The stream inference API is the same
as the Inference API, except that once the connection is established,
the requests are sent in the same connection until it is closed.

\end{itemize}

The HTTP endpoints can be used directly as described in this section,
but for most use-cases, the preferred way to access the inference
server is via the {\hyperref[\detokenize{client:section-client-libraries-and-examples}]{\sphinxcrossref{\DUrole{std,std-ref}{C++ and Python Client libraries}}}}.

The GRPC endpoints can also be used via the {\hyperref[\detokenize{client:section-client-libraries-and-examples}]{\sphinxcrossref{\DUrole{std,std-ref}{C++ and Python Client
libraries}}}} or a GRPC-generated
API can be used directly as shown in the grpc\_image\_client.py example.


\section{Health}
\label{\detokenize{http_grpc_api:health}}\label{\detokenize{http_grpc_api:section-api-health}}
Performing an HTTP GET to /api/health/live returns a 200 status if the
server is able to receive and process requests. Any other status code
indicates that the server is still initializing or has failed in some
way that prevents it from processing requests.

Once the liveness endpoint indicates that the server is active,
performing an HTTP GET to /api/health/ready returns a 200 status if
the server is able to respond to inference requests for some or all
models (based on the inference server’s --strict-readiness option
explained below). Any other status code indicates that the server is
not ready to respond to some or all inference requests.

For GRPC the \sphinxcode{\sphinxupquote{GRPCService}} uses the
\sphinxcode{\sphinxupquote{HealthRequest}} and
\sphinxcode{\sphinxupquote{HealthResponse}}
messages to implement the endpoint.

By default, the readiness endpoint will return success if the server
is responsive and all models loaded successfully. Thus, by default,
success indicates that an inference request for any model can be
handled by the server. For some use cases, you want the readiness
endpoint to return success even if all models are not available. In
this case, use the --strict-readiness=false option to cause the
readiness endpoint to report success as long as the server is
responsive (even if one or more models are not available).


\section{Status}
\label{\detokenize{http_grpc_api:status}}\label{\detokenize{http_grpc_api:section-api-status}}
Performing an HTTP GET to /api/status returns status information about
the server and all the models being served. Performing an HTTP GET to
/api/status/\textless{}model name\textgreater{} returns information about the server and the
single model specified by \textless{}model name\textgreater{}. The server status is returned
in the HTTP response body in either text format (the default) or in
binary format if query parameter format=binary is specified (for
example, /api/status?format=binary). The success or failure of the
status request is indicated in the HTTP response code and the
\sphinxstylestrong{NV-Status} response header. The \sphinxstylestrong{NV-Status} response header
returns a text protobuf formatted \sphinxcode{\sphinxupquote{RequestStatus}} message.

For GRPC the \sphinxcode{\sphinxupquote{GRPCService}} uses the
\sphinxcode{\sphinxupquote{StatusRequest}} and
\sphinxcode{\sphinxupquote{StatusResponse}}
messages to implement the endpoint. The response includes a
\sphinxcode{\sphinxupquote{RequestStatus}}
message indicating success or failure.

For either protocol the status itself is returned as a
\sphinxcode{\sphinxupquote{ServerStatus}}
message.


\section{Inference}
\label{\detokenize{http_grpc_api:inference}}\label{\detokenize{http_grpc_api:section-api-inference}}
Performing an HTTP POST to /api/infer/\textless{}model name\textgreater{} performs inference
using the latest version of the model that is being made available by
the model’s {\hyperref[\detokenize{model_configuration:section-version-policy}]{\sphinxcrossref{\DUrole{std,std-ref}{version policy}}}}. The latest
version is the numerically greatest version number. Performing an HTTP
POST to /api/infer/\textless{}model name\textgreater{}/\textless{}model version\textgreater{} performs inference
using a specific version of the model.

The request uses the \sphinxstylestrong{NV-InferRequest} header to communicate an
\sphinxcode{\sphinxupquote{InferRequestHeader}} message that describes
the input tensors and the requested output tensors. For example, for a
resnet50 model the following \sphinxstylestrong{NV-InferRequest} header indicates that
a batch-size 1 request is being made with a single input named
“input”, and that the result of the tensor named “output” should be
returned as the top-3 classification values:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
NV\PYGZhy{}InferRequest: batch\PYGZus{}size: 1 input \PYGZob{} name: \PYGZdq{}input\PYGZdq{} \PYGZcb{} output \PYGZob{} name: \PYGZdq{}output\PYGZdq{} cls \PYGZob{} count: 3 \PYGZcb{} \PYGZcb{}
\end{sphinxVerbatim}

The input tensor values are communicated in the body of the HTTP POST
request as raw binary in the order as the inputs are listed in the
request header.

The HTTP response includes an \sphinxstylestrong{NV-InferResponse} header that
communicates an \sphinxcode{\sphinxupquote{InferResponseHeader}} message that describes
the outputs. For example the above response could return the
following:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
NV\PYGZhy{}InferResponse: model\PYGZus{}name: \PYGZdq{}mymodel\PYGZdq{} model\PYGZus{}version: 1 batch\PYGZus{}size: 1 output \PYGZob{} name: \PYGZdq{}output\PYGZdq{} raw \PYGZob{} dims: 4 dims: 4 batch\PYGZus{}byte\PYGZus{}size: 64 \PYGZcb{} \PYGZcb{}
\end{sphinxVerbatim}

This response shows that the output in a tensor with shape {[} 4, 4 {]}
and has a size of 64 bytes. The output tensor contents are returned in
the body of the HTTP response to the POST request. For outputs where
full result tensors were requested, the result values are communicated
in the body of the response in the order as the outputs are listed in
the \sphinxstylestrong{NV-InferResponse} header. After those, an
\sphinxcode{\sphinxupquote{InferResponseHeader}} message is appended to
the response body. The \sphinxcode{\sphinxupquote{InferResponseHeader}} message is returned in
either text format (the default) or in binary format if query
parameter format=binary is specified (for example,
/api/infer/foo?format=binary).

For example, assuming an inference request for a model that has ‘n’
outputs, the outputs specified in the \sphinxstylestrong{NV-InferResponse} header in
order are “output{[}0{]}”, …, “output{[}n-1{]}” the response body would
contain:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}raw binary tensor values for output[0] \PYGZgt{}
...
\PYGZlt{}raw binary tensor values for output[n\PYGZhy{}1] \PYGZgt{}
\PYGZlt{}text or binary encoded InferResponseHeader proto\PYGZgt{}
\end{sphinxVerbatim}

The success or failure of the inference request is indicated in the
HTTP response code and the \sphinxstylestrong{NV-Status} response header. The
\sphinxstylestrong{NV-Status} response header returns a text protobuf formatted
\sphinxcode{\sphinxupquote{RequestStatus}}
message.

For GRPC the \sphinxcode{\sphinxupquote{GRPCService}} uses the
\sphinxcode{\sphinxupquote{InferRequest}} and
\sphinxcode{\sphinxupquote{InferResponse}}
messages to implement the endpoint. The response includes a
\sphinxcode{\sphinxupquote{RequestStatus}}
message indicating success or failure, \sphinxcode{\sphinxupquote{InferResponseHeader}} message giving
response meta-data, and the raw output tensors.


\section{Stream Inference}
\label{\detokenize{http_grpc_api:stream-inference}}\label{\detokenize{http_grpc_api:section-api-stream-inference}}
For GRPC the \sphinxcode{\sphinxupquote{GRPCService}} uses the
\sphinxcode{\sphinxupquote{InferRequest}} and
\sphinxcode{\sphinxupquote{InferResponse}}
messages to implement the endpoint. The response includes a
\sphinxcode{\sphinxupquote{RequestStatus}}
message indicating success or failure, \sphinxcode{\sphinxupquote{InferResponseHeader}} message giving
response meta-data, and the raw output tensors.


\chapter{Metrics}
\label{\detokenize{metrics:metrics}}\label{\detokenize{metrics:section-metrics}}\label{\detokenize{metrics::doc}}
The TensorRT Inference server provides \sphinxhref{https://prometheus.io/}{Prometheus} metrics indicating GPU and request
statistics. By default, these metrics are available at
\sphinxurl{http://localhost:8002/metrics}. The inference server --metrics-port
option can be used to select a different port. The following table
describes the available metrics.


\begin{savenotes}\sphinxattablestart
\centering
\begin{tabular}[t]{|*{5}{\X{1}{5}|}}
\hline
\sphinxstyletheadfamily 
Category
&\sphinxstyletheadfamily 
Metric
&\sphinxstyletheadfamily 
Description
&\sphinxstyletheadfamily 
Granularity
&\sphinxstyletheadfamily 
Frequency
\\
\hline\sphinxmultirow{4}{6}{%
\begin{varwidth}[t]{\sphinxcolwidth{1}{5}}

\begin{DUlineblock}{0em}
\item[] GPU
\item[] Utilization
\end{DUlineblock}
\par
\vskip-\baselineskip\vbox{\hbox{\strut}}\end{varwidth}%
}%
&
Power Usage
&
GPU instantaneous power
&
Per GPU
&
Per second
\\
\cline{2-5}\sphinxtablestrut{6}&
Power Limit
&
Maximum GPU power limit
&
Per GPU
&
Per second
\\
\cline{2-5}\sphinxtablestrut{6}&
\begin{DUlineblock}{0em}
\item[] Energy
\item[] Consumption
\end{DUlineblock}
&
\begin{DUlineblock}{0em}
\item[] GPU energy consumption in joules
\item[] since the server started
\end{DUlineblock}
&
Per GPU
&
Per second
\\
\cline{2-5}\sphinxtablestrut{6}&
GPU Utilization
&
\begin{DUlineblock}{0em}
\item[] GPU utilization rate
\item[] (0.0 - 1.0)
\end{DUlineblock}
&
Per GPU
&
Per second
\\
\hline\sphinxmultirow{2}{23}{%
\begin{varwidth}[t]{\sphinxcolwidth{1}{5}}

\begin{DUlineblock}{0em}
\item[] GPU
\item[] Memory
\end{DUlineblock}
\par
\vskip-\baselineskip\vbox{\hbox{\strut}}\end{varwidth}%
}%
&
\begin{DUlineblock}{0em}
\item[] GPU Total
\item[] Memory
\end{DUlineblock}
&
\begin{DUlineblock}{0em}
\item[] Total GPU memory, in bytes
\end{DUlineblock}
&
Per GPU
&
Per second
\\
\cline{2-5}\sphinxtablestrut{23}&
\begin{DUlineblock}{0em}
\item[] GPU Used
\item[] Memory
\end{DUlineblock}
&
\begin{DUlineblock}{0em}
\item[] Used GPU memory, in bytes
\end{DUlineblock}
&
Per GPU
&
Per second
\\
\hline\sphinxmultirow{3}{32}{%
\begin{varwidth}[t]{\sphinxcolwidth{1}{5}}
Count
\par
\vskip-\baselineskip\vbox{\hbox{\strut}}\end{varwidth}%
}%
&
Request Count
&
\begin{DUlineblock}{0em}
\item[] Number of inference requests
\end{DUlineblock}
&
Per model
&
Per request
\\
\cline{2-5}\sphinxtablestrut{32}&
Execution Count
&
\begin{DUlineblock}{0em}
\item[] Number of inference executions
\item[] (request count / execution count
\item[] = average dynamic batch size)
\end{DUlineblock}
&
Per model
&
Per request
\\
\cline{2-5}\sphinxtablestrut{32}&
Inference Count
&
\begin{DUlineblock}{0em}
\item[] Number of inferences performed
\item[] (one request counts as
\item[] “batch size” inferences)
\end{DUlineblock}
&
Per model
&
Per request
\\
\hline\sphinxmultirow{3}{45}{%
\begin{varwidth}[t]{\sphinxcolwidth{1}{5}}
Latency
\par
\vskip-\baselineskip\vbox{\hbox{\strut}}\end{varwidth}%
}%
&
Request Time
&
\begin{DUlineblock}{0em}
\item[] End-to-end inference request
\item[] handling time
\end{DUlineblock}
&
Per model
&
Per request
\\
\cline{2-5}\sphinxtablestrut{45}&
Compute Time
&
\begin{DUlineblock}{0em}
\item[] Time a request spends executing
\item[] the inference model (in the
\item[] framework backend)
\end{DUlineblock}
&
Per model
&
Per request
\\
\cline{2-5}\sphinxtablestrut{45}&
Queue Time
&
\begin{DUlineblock}{0em}
\item[] Time a request spends waiting
\item[] in the queue
\end{DUlineblock}
&
Per model
&
Per request
\\
\hline
\end{tabular}
\par
\sphinxattableend\end{savenotes}


\chapter{Architecture}
\label{\detokenize{architecture:architecture}}\label{\detokenize{architecture::doc}}
The following figure shows the TensorRT Inference Server high-level
architecture. The {\hyperref[\detokenize{model_repository:section-model-repository}]{\sphinxcrossref{\DUrole{std,std-ref}{model repository}}}}
is a file-system based store of the models that the inference server
will make available for inferencing. Inference requests arrive at the
server via either {\hyperref[\detokenize{http_grpc_api:section-inference-server-api}]{\sphinxcrossref{\DUrole{std,std-ref}{HTTP or GRPC}}}}
and are then routed to the appropriate per-model scheduler queue. The
scheduler performs fair scheduling and dynamic batching for each
model’s requests. The schedule passes each request to the framework
backend corresponding to the model type. The framework backend
performs inferencing using the inputs provided in the request to
produce the requested outputs. The outputs are then formatted and a
response is sent.

\noindent\sphinxincludegraphics[width=1999\sphinxpxdimen,height=1999\sphinxpxdimen]{{arch}.png}


\section{Concurrent Model Execution}
\label{\detokenize{architecture:concurrent-model-execution}}\label{\detokenize{architecture:section-concurrent-model-execution}}
The TensorRT Inference Server architecture allows multiple models
and/or multiple instances of the same model to execute in parallel on
a single GPU. The following figure shows an example with two models;
model0 and model1. Assuming the server is not currently processing any
request, when two requests arrive simultaneously, one for each model,
the server immediately schedules both of them onto the GPU and the
GPU’s hardware scheduler begins working on both computations in
parallel.

\noindent\sphinxincludegraphics[width=534\sphinxpxdimen,height=273\sphinxpxdimen]{{multi_model_exec}.png}

By default, if multiple requests for the same model arrive at the same
time, the inference server will serialize their execution by
scheduling only one at a time on the GPU, as shown in the following
figure.

\noindent\sphinxincludegraphics[width=553\sphinxpxdimen,height=283\sphinxpxdimen]{{multi_model_serial_exec}.png}

The TensorRT inference server provides an {\hyperref[\detokenize{model_configuration:section-instance-groups}]{\sphinxcrossref{\DUrole{std,std-ref}{instance-group}}}} feature that allows each model to specify
how many parallel executions of that model should be allowed. Each
such enabled parallel execution is referred to as an \sphinxstyleemphasis{execution
instance}. By default, the server gives each model a single execution
instance, which means that only a single execution of the model is
allowed to be in progress at a time as shown in the above figure. By
using instance-group the number of execution instances for a model can
be increased. The following figure shows model execution when model1
is configured to allow three execution instances. As shown in the
figure, the first three model1 inference requests are immediately
executed in parallel on the GPU. The fourth model1 inference request
must wait until one of the first three executions completes before
beginning.

\noindent\sphinxincludegraphics[width=531\sphinxpxdimen,height=326\sphinxpxdimen]{{multi_model_parallel_exec}.png}

To provide the current model execution capabilities shown in the above
figures, the inference server uses \sphinxhref{https://devblogs.nvidia.com/gpu-pro-tip-cuda-7-streams-simplify-concurrency/}{CUDA streams}
to exploit the GPU’s hardware scheduling capabilities. CUDA streams
allow the server to communicate independent sequences of memory-copy
and kernel executions to the GPU. The hardware scheduler in the GPU
takes advantage of the independent execution streams to fill the GPU
with independent memory-copy and kernel executions. For example, using
streams allows the GPU to execute a memory-copy for one model, a
kernel for another model, and a different kernel for yet another model
at the same time.

The following figure shows some details of how this works within the
TensorRT Inference Server. Each framework backend (TensorRT,
TensorFlow, Caffe2) provides an API for creating an execution context
that is used to execute a given model (each framework uses different
terminology for this concept but here we refer to them generally as
execution contexts). Each framework allows an execution context to be
associated with a CUDA stream. This CUDA stream is used by the
framework to execute all memory copies and kernels needed for the
model associated with the execution context. For a given model, the
inference server creates one execution context for each execution
instance specified for the model. When an inference request arrives
for a given model, that request is queued in the model scheduler
associated with that model. The model scheduler waits for any
execution context associated with that model to be idle and then sends
the queued request to the context. The execution context then issues
all the memory copies and kernel executions required to execute the
model to the CUDA stream associated with that execution context. The
memory copies and kernels in each CUDA stream are independent of
memory copies and kernels in other CUDA streams. The GPU hardware
scheduler looks across all CUDA streams to find independent memory
copies and kernels to execute on the GPU.

\noindent\sphinxincludegraphics[width=657\sphinxpxdimen,height=226\sphinxpxdimen]{{cuda_stream_exec}.png}


\chapter{Contributing}
\label{\detokenize{contribute:contributing}}\label{\detokenize{contribute::doc}}
Contributions to TensorRT Inference Server are more than welcome. To
contribute make a pull request and follow the guidelines outlined in
the \sphinxhref{https://github.com/NVIDIA/tensorrt-inference-server/blob/master/CONTRIBUTING.md}{CONTRIBUTING}
document.


\section{Coding Convention}
\label{\detokenize{contribute:coding-convention}}
Use clang-format to format all source files (*.h, *.cc, *.proto) to
a consistent format. You should run clang-format on all source files
before submitting a pull request:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZdl{} apt\PYGZhy{}get install clang\PYGZhy{}format clang\PYGZhy{}format\PYGZhy{}6.0
\PYGZdl{} clang\PYGZhy{}format\PYGZhy{}6.0 \PYGZhy{}\PYGZhy{}style=file \PYGZhy{}i *.proto *.cc *.h
\end{sphinxVerbatim}

For convenience there is a format.py script in tools/ that can be used
to clang-format all files within the repo:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZdl{} cd .../tensorrt\PYGZhy{}inference\PYGZhy{}server    \PYGZsh{} top\PYGZhy{}level of repo
\PYGZdl{} python format.py *
\end{sphinxVerbatim}


\chapter{Building}
\label{\detokenize{build:building}}\label{\detokenize{build::doc}}
The TensorRT Inference Server is built using Docker and the TensorFlow
and PyTorch containers from \sphinxhref{https://ngc.nvidia.com}{NVIDIA GPU Cloud (NGC)}. Before building you must install Docker
and nvidia-docker and login to the NGC registry by following the
instructions in {\hyperref[\detokenize{install:section-installing-prebuilt-containers}]{\sphinxcrossref{\DUrole{std,std-ref}{Installing Prebuilt Containers}}}}.


\section{Building the Server}
\label{\detokenize{build:building-the-server}}\label{\detokenize{build:section-building-the-server}}
To build a release version of the TensorRT Inference Server container,
change directory to the root of the repo and issue the following
command:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZdl{} docker build \PYGZhy{}\PYGZhy{}pull \PYGZhy{}t tensorrtserver .
\end{sphinxVerbatim}


\subsection{Incremental Builds}
\label{\detokenize{build:incremental-builds}}
For typical development you will want to run the \sphinxstyleemphasis{build} container
with your local repo’s source files mounted so that your local changes
can be incrementally built. This is done by first building the
\sphinxstyleemphasis{tensorrtserver\_build} container:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZdl{} docker build \PYGZhy{}\PYGZhy{}pull \PYGZhy{}t tensorrtserver\PYGZus{}build \PYGZhy{}\PYGZhy{}target trtserver\PYGZus{}build .
\end{sphinxVerbatim}

By mounting /path/to/tensorrtserver/src into the container at
/workspace/src, changes to your local repo will be reflected in the
container:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZdl{} nvidia\PYGZhy{}docker run \PYGZhy{}it \PYGZhy{}\PYGZhy{}rm \PYGZhy{}v/path/to/tensorrtserver/src:/workspace/src tensorrtserver\PYGZus{}build
\end{sphinxVerbatim}

Within the container you can perform an incremental server build
with:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZsh{} cd /workspace
\PYGZsh{} bazel build \PYGZhy{}c opt \PYGZhy{}\PYGZhy{}config=cuda src/servers/trtserver
\PYGZsh{} cp /workspace/bazel\PYGZhy{}bin/src/servers/trtserver /opt/tensorrtserver/bin/trtserver
\end{sphinxVerbatim}

Similarly, within the container you can perform an incremental build
of the C++ and Python client libraries and example executables with:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZsh{} cd /workspace
\PYGZsh{} bazel build \PYGZhy{}c opt \PYGZhy{}\PYGZhy{}config=cuda src/clients/…
\PYGZsh{} mkdir \PYGZhy{}p /opt/tensorrtserver/bin
\PYGZsh{} cp bazel\PYGZhy{}bin/src/clients/c++/image\PYGZus{}client /opt/tensorrtserver/bin/.
\PYGZsh{} cp bazel\PYGZhy{}bin/src/clients/c++/perf\PYGZus{}client /opt/tensorrtserver/bin/.
\PYGZsh{} cp bazel\PYGZhy{}bin/src/clients/c++/simple\PYGZus{}client /opt/tensorrtserver/bin/.
\PYGZsh{} mkdir \PYGZhy{}p /opt/tensorrtserver/lib
\PYGZsh{} cp bazel\PYGZhy{}bin/src/clients/c++/librequest.so /opt/tensorrtserver/lib/.
\PYGZsh{} cp bazel\PYGZhy{}bin/src/clients/c++/librequest.a /opt/tensorrtserver/lib/.
\PYGZsh{} mkdir \PYGZhy{}p /opt/tensorrtserver/pip
\PYGZsh{} bazel\PYGZhy{}bin/src/clients/python/build\PYGZus{}pip /opt/tensorrtserver/pip/.
\end{sphinxVerbatim}

Some source changes seem to cause bazel to get confused and not
correctly rebuild all required sources. You can force bazel to rebuild
all of the inference server source without requiring a complete
rebuild of the TensorFlow and Caffe2 components by doing the following
before issuing the above build command:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZsh{} rm \PYGZhy{}fr bazel\PYGZhy{}bin/src
\end{sphinxVerbatim}


\section{Building the Client Libraries and Examples}
\label{\detokenize{build:building-the-client-libraries-and-examples}}\label{\detokenize{build:section-building-the-client-libraries-and-examples}}
The provided Dockerfile can be used to build just the client libraries
and examples. Issue the following command to build the C++ client
library, C++ and Python examples, and a Python wheel file for the
Python client library:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZdl{} docker build \PYGZhy{}t tensorrtserver\PYGZus{}clients \PYGZhy{}\PYGZhy{}target trtserver\PYGZus{}build \PYGZhy{}\PYGZhy{}build\PYGZhy{}arg \PYGZdq{}BUILD\PYGZus{}CLIENTS\PYGZus{}ONLY=1\PYGZdq{} .
\end{sphinxVerbatim}

You can optionally add \sphinxstyleemphasis{--build-arg “PYVER=\textless{}ver\textgreater{}”} to set the Python
version that you want the Python client library built for. Supported
values for \sphinxstyleemphasis{\textless{}ver\textgreater{}} are 2.6 and 3.5, with 3.5 being the default.

After the build completes the tensorrtserver\_clients docker image will
contain the built client libraries and examples. The easiest way to
try the examples described in the following sections is to run the
client image with --net=host so that the client examples can access
the inference server running in its own container (see
{\hyperref[\detokenize{run:section-running-the-inference-server}]{\sphinxcrossref{\DUrole{std,std-ref}{Running The Inference Server}}}} for more information about
running the inference server):

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZdl{} docker run \PYGZhy{}it \PYGZhy{}\PYGZhy{}rm \PYGZhy{}\PYGZhy{}net=host tensorrtserver\PYGZus{}clients
\end{sphinxVerbatim}

In the client image you can find the example executables in
/opt/tensorrtserver/bin, and the Python wheel in
/opt/tensorrtserver/pip.

If your host sytem is Ubuntu-16.04, an alternative to running the
examples within the tensorrtserver\_clients container is to instead
copy the libraries and examples from the docker image to the host
system:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZdl{} docker run \PYGZhy{}it \PYGZhy{}\PYGZhy{}rm \PYGZhy{}v/tmp:/tmp/host tensorrtserver\PYGZus{}clients
\PYGZsh{} cp /opt/tensorrtserver/bin/image\PYGZus{}client /tmp/host/.
\PYGZsh{} cp /opt/tensorrtserver/bin/perf\PYGZus{}client /tmp/host/.
\PYGZsh{} cp /opt/tensorrtserver/bin/simple\PYGZus{}client /tmp/host/.
\PYGZsh{} cp /opt/tensorrtserver/pip/tensorrtserver\PYGZhy{}*.whl /tmp/host/.
\PYGZsh{} cp /opt/tensorrtserver/lib/librequest.* /tmp/host/.
\end{sphinxVerbatim}

You can now access the files from /tmp on the host system. To run the
C++ examples you must install some dependencies on your host system:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZdl{} apt\PYGZhy{}get install curl libcurl3\PYGZhy{}dev libopencv\PYGZhy{}dev libopencv\PYGZhy{}core\PYGZhy{}dev
\end{sphinxVerbatim}

To run the Python examples you will need to additionally install the
client whl file and some other dependencies:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZdl{} apt\PYGZhy{}get install python3 python3\PYGZhy{}pip
\PYGZdl{} pip3 install \PYGZhy{}\PYGZhy{}user \PYGZhy{}\PYGZhy{}upgrade tensorrtserver\PYGZhy{}*.whl numpy pillow
\end{sphinxVerbatim}


\section{Building the Documentation}
\label{\detokenize{build:building-the-documentation}}
The inference server documentation is found in the docs/ directory and
is based on \sphinxhref{http://www.sphinx-doc.org}{Sphinx}. \sphinxhref{http://www.doxygen.org/}{Doxygen} integrated with \sphinxhref{https://github.com/svenevs/exhale}{Exhale} is used for C++ API
docuementation.

To build the docs install the required dependencies:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZdl{} apt\PYGZhy{}get update
\PYGZdl{} apt\PYGZhy{}get install \PYGZhy{}y \PYGZhy{}\PYGZhy{}no\PYGZhy{}install\PYGZhy{}recommends doxygen
\PYGZdl{} pip install \PYGZhy{}\PYGZhy{}upgrade sphinx sphinx\PYGZhy{}rtd\PYGZhy{}theme nbsphinx exhale
\end{sphinxVerbatim}

To get the Python client library API docs the TensorRT Inference
Server Python package must be installed:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZdl{} pip install \PYGZhy{}\PYGZhy{}upgrade tensorrtserver\PYGZhy{}*.whl
\end{sphinxVerbatim}

Then use Sphinx to build the documentation into the build/html
directory:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZdl{} cd docs
\PYGZdl{} make clean html
\end{sphinxVerbatim}

To build the PDF version of documentation \sphinxtitleref{LaTEX \textless{}https://www.tug.org/texlive/quickinstall.html\textgreater{}}
needs to be installed first. Additional requirements of python modules
can be met by simply running the following commands in docs directory:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZdl{} pip install \PYGZhy{}r requirements.txt
\end{sphinxVerbatim}

Once latex and python modules have been installed and updated single PDF for documentation
can be generated by running the following command. It will generate \sphinxstyleemphasis{NVIDIATRTIS.pdf} in \sphinxstyleemphasis{build/latex}
directory:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZdl{}make clean latexpdf
\end{sphinxVerbatim}

Corrections and enhancements in Documentation are always welcome however it is advised that before
creating a pull request for the changes these are validated locally. A simple way to do it is to run web server
inside \sphinxstyleemphasis{docs/build/html} directory with following command and navigate through the modified documentation in the browser
at \sphinxurl{http://localhost:8000}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZdl{} python \PYGZhy{}m SimpleHTTPServer          \PYGZsh{} run inside docs/build/html
\end{sphinxVerbatim}


\chapter{Testing}
\label{\detokenize{test:testing}}\label{\detokenize{test::doc}}
Currently there is no CI testing enabled for the open-source version
of the TensorRT Inference Server. We will enable CI testing in a
future update.

There is a set of tests in the qa/ directory that can be run manually
to provide some testing. Before running these tests you must first
generate a couple of test model repositories containing the models
needed by the tests.


\section{Generate QA Model Repositories}
\label{\detokenize{test:generate-qa-model-repositories}}
The QA model repositories contain some simple models that are used to
verify the correctness of the inference server. To generate the QA
model repositories:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZdl{} cd qa/common
\PYGZdl{} ./gen\PYGZus{}qa\PYGZus{}model\PYGZus{}repository
\end{sphinxVerbatim}

This will generate two model repositories: in /tmp/qa\_model\_repository
and /tmp/qa\_variable\_model\_repository.  The TensorRT models will be
created for the GPU on the system that CUDA considers device 0
(zero). If you have multiple GPUs on your system see the documentation
in the script for how to target a specific GPU.


\section{Build QA Container}
\label{\detokenize{test:build-qa-container}}
Next you need to build a QA version of the inference server
container. This container will contain the inference server, the QA
tests, and all the dependencies needed to run the QA tests. You must
first build the tensorrtserver\_build and tensorrtserver containers as
described in {\hyperref[\detokenize{build:section-building-the-server}]{\sphinxcrossref{\DUrole{std,std-ref}{Building the Server}}}} and then build the QA
container:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZdl{} docker build \PYGZhy{}t tensorrtserver\PYGZus{}qa \PYGZhy{}f Dockerfile.QA .
\end{sphinxVerbatim}


\section{Run QA Container}
\label{\detokenize{test:run-qa-container}}
Now run the QA container and mount the QA model repository into the
container so the tests will be able to access it:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZdl{} nvidia\PYGZhy{}docker run \PYGZhy{}it \PYGZhy{}\PYGZhy{}rm \PYGZhy{}v/tmp:/data/inferenceserver tensorrtserver\PYGZus{}qa
\end{sphinxVerbatim}

Within the container the QA tests are in /opt/tensorrtserver/qa. To run a test:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZdl{} cd \PYGZlt{}test directory\PYGZgt{}
\PYGZdl{} ./test.sh
\end{sphinxVerbatim}


\chapter{Protobuf API}
\label{\detokenize{protobuf_api/protobuf_api_root:protobuf-api}}\label{\detokenize{protobuf_api/protobuf_api_root::doc}}

\section{HTTP/GRPC API}
\label{\detokenize{protobuf_api/protobuf_api_root:http-grpc-api}}\begin{itemize}
\item {} 
\DUrole{xref,std,std-doc}{src/core/api.proto}

\item {} 
\DUrole{xref,std,std-doc}{src/core/grpc\_service.proto}

\item {} 
\DUrole{xref,std,std-doc}{src/core/request\_status.proto}

\end{itemize}


\section{Model Configuration}
\label{\detokenize{protobuf_api/protobuf_api_root:model-configuration}}\begin{itemize}
\item {} 
\DUrole{xref,std,std-doc}{src/core/model\_config.proto}

\end{itemize}


\section{Status}
\label{\detokenize{protobuf_api/protobuf_api_root:status}}\begin{itemize}
\item {} 
\DUrole{xref,std,std-doc}{src/core/server\_status.proto}

\end{itemize}


\chapter{C++ API}
\label{\detokenize{cpp_api/cpp_api_root:c-api}}\label{\detokenize{cpp_api/cpp_api_root::doc}}

\section{Class Hierarchy}
\label{\detokenize{cpp_api/cpp_api_root:class-hierarchy}}



\section{File Hierarchy}
\label{\detokenize{cpp_api/cpp_api_root:file-hierarchy}}



\section{Full API}
\label{\detokenize{cpp_api/cpp_api_root:full-api}}

\subsection{Namespaces}
\label{\detokenize{cpp_api/cpp_api_root:namespaces}}

\subsubsection{Namespace nvidia}
\label{\detokenize{cpp_api/namespace_nvidia:namespace-nvidia}}\label{\detokenize{cpp_api/namespace_nvidia:id1}}\label{\detokenize{cpp_api/namespace_nvidia::doc}}
\begin{sphinxShadowBox}
\sphinxstyletopictitle{Contents}
\begin{itemize}
\item {} 
\phantomsection\label{\detokenize{cpp_api/namespace_nvidia:id2}}{\hyperref[\detokenize{cpp_api/namespace_nvidia:namespaces}]{\sphinxcrossref{Namespaces}}}

\end{itemize}
\end{sphinxShadowBox}


\paragraph{Namespaces}
\label{\detokenize{cpp_api/namespace_nvidia:namespaces}}\begin{itemize}
\item {} 
{\hyperref[\detokenize{cpp_api/namespace_nvidia__inferenceserver:namespace-nvidia-inferenceserver}]{\sphinxcrossref{\DUrole{std,std-ref}{Namespace nvidia::inferenceserver}}}}

\end{itemize}


\subsubsection{Namespace nvidia::inferenceserver}
\label{\detokenize{cpp_api/namespace_nvidia__inferenceserver:namespace-nvidia-inferenceserver}}\label{\detokenize{cpp_api/namespace_nvidia__inferenceserver:id1}}\label{\detokenize{cpp_api/namespace_nvidia__inferenceserver::doc}}
\begin{sphinxShadowBox}
\sphinxstyletopictitle{Contents}
\begin{itemize}
\item {} 
\phantomsection\label{\detokenize{cpp_api/namespace_nvidia__inferenceserver:id2}}{\hyperref[\detokenize{cpp_api/namespace_nvidia__inferenceserver:namespaces}]{\sphinxcrossref{Namespaces}}}

\end{itemize}
\end{sphinxShadowBox}


\paragraph{Namespaces}
\label{\detokenize{cpp_api/namespace_nvidia__inferenceserver:namespaces}}\begin{itemize}
\item {} 
{\hyperref[\detokenize{cpp_api/namespace_nvidia__inferenceserver__client:namespace-nvidia-inferenceserver-client}]{\sphinxcrossref{\DUrole{std,std-ref}{Namespace nvidia::inferenceserver::client}}}}

\end{itemize}


\subsubsection{Namespace nvidia::inferenceserver::client}
\label{\detokenize{cpp_api/namespace_nvidia__inferenceserver__client:namespace-nvidia-inferenceserver-client}}\label{\detokenize{cpp_api/namespace_nvidia__inferenceserver__client:id1}}\label{\detokenize{cpp_api/namespace_nvidia__inferenceserver__client::doc}}
\begin{sphinxShadowBox}
\sphinxstyletopictitle{Contents}
\begin{itemize}
\item {} 
\phantomsection\label{\detokenize{cpp_api/namespace_nvidia__inferenceserver__client:id2}}{\hyperref[\detokenize{cpp_api/namespace_nvidia__inferenceserver__client:classes}]{\sphinxcrossref{Classes}}}

\item {} 
\phantomsection\label{\detokenize{cpp_api/namespace_nvidia__inferenceserver__client:id3}}{\hyperref[\detokenize{cpp_api/namespace_nvidia__inferenceserver__client:functions}]{\sphinxcrossref{Functions}}}

\end{itemize}
\end{sphinxShadowBox}


\paragraph{Classes}
\label{\detokenize{cpp_api/namespace_nvidia__inferenceserver__client:classes}}\begin{itemize}
\item {} 
{\hyperref[\detokenize{cpp_api/structnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result_1_1ClassResult:exhale-struct-structnvidia-1-1inferenceserver-1-1client-1-1infercontext-1-1result-1-1classresult}]{\sphinxcrossref{\DUrole{std,std-ref}{Struct Result::ClassResult}}}}

\item {} 
{\hyperref[\detokenize{cpp_api/structnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Stat:exhale-struct-structnvidia-1-1inferenceserver-1-1client-1-1infercontext-1-1stat}]{\sphinxcrossref{\DUrole{std,std-ref}{Struct InferContext::Stat}}}}

\item {} 
{\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1Error:exhale-class-classnvidia-1-1inferenceserver-1-1client-1-1error}]{\sphinxcrossref{\DUrole{std,std-ref}{Class Error}}}}

\item {} 
{\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:exhale-class-classnvidia-1-1inferenceserver-1-1client-1-1infercontext}]{\sphinxcrossref{\DUrole{std,std-ref}{Class InferContext}}}}

\item {} 
{\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Input:exhale-class-classnvidia-1-1inferenceserver-1-1client-1-1infercontext-1-1input}]{\sphinxcrossref{\DUrole{std,std-ref}{Class InferContext::Input}}}}

\item {} 
{\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Options:exhale-class-classnvidia-1-1inferenceserver-1-1client-1-1infercontext-1-1options}]{\sphinxcrossref{\DUrole{std,std-ref}{Class InferContext::Options}}}}

\item {} 
{\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Output:exhale-class-classnvidia-1-1inferenceserver-1-1client-1-1infercontext-1-1output}]{\sphinxcrossref{\DUrole{std,std-ref}{Class InferContext::Output}}}}

\item {} 
{\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Request:exhale-class-classnvidia-1-1inferenceserver-1-1client-1-1infercontext-1-1request}]{\sphinxcrossref{\DUrole{std,std-ref}{Class InferContext::Request}}}}

\item {} 
{\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1RequestTimers:exhale-class-classnvidia-1-1inferenceserver-1-1client-1-1infercontext-1-1requesttimers}]{\sphinxcrossref{\DUrole{std,std-ref}{Class InferContext::RequestTimers}}}}

\item {} 
{\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result:exhale-class-classnvidia-1-1inferenceserver-1-1client-1-1infercontext-1-1result}]{\sphinxcrossref{\DUrole{std,std-ref}{Class InferContext::Result}}}}

\item {} 
{\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferGrpcContext:exhale-class-classnvidia-1-1inferenceserver-1-1client-1-1infergrpccontext}]{\sphinxcrossref{\DUrole{std,std-ref}{Class InferGrpcContext}}}}

\item {} 
{\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferGrpcStreamContext:exhale-class-classnvidia-1-1inferenceserver-1-1client-1-1infergrpcstreamcontext}]{\sphinxcrossref{\DUrole{std,std-ref}{Class InferGrpcStreamContext}}}}

\item {} 
{\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferHttpContext:exhale-class-classnvidia-1-1inferenceserver-1-1client-1-1inferhttpcontext}]{\sphinxcrossref{\DUrole{std,std-ref}{Class InferHttpContext}}}}

\item {} 
{\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ProfileContext:exhale-class-classnvidia-1-1inferenceserver-1-1client-1-1profilecontext}]{\sphinxcrossref{\DUrole{std,std-ref}{Class ProfileContext}}}}

\item {} 
{\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ProfileGrpcContext:exhale-class-classnvidia-1-1inferenceserver-1-1client-1-1profilegrpccontext}]{\sphinxcrossref{\DUrole{std,std-ref}{Class ProfileGrpcContext}}}}

\item {} 
{\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ProfileHttpContext:exhale-class-classnvidia-1-1inferenceserver-1-1client-1-1profilehttpcontext}]{\sphinxcrossref{\DUrole{std,std-ref}{Class ProfileHttpContext}}}}

\item {} 
{\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ServerHealthContext:exhale-class-classnvidia-1-1inferenceserver-1-1client-1-1serverhealthcontext}]{\sphinxcrossref{\DUrole{std,std-ref}{Class ServerHealthContext}}}}

\item {} 
{\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ServerHealthGrpcContext:exhale-class-classnvidia-1-1inferenceserver-1-1client-1-1serverhealthgrpccontext}]{\sphinxcrossref{\DUrole{std,std-ref}{Class ServerHealthGrpcContext}}}}

\item {} 
{\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ServerHealthHttpContext:exhale-class-classnvidia-1-1inferenceserver-1-1client-1-1serverhealthhttpcontext}]{\sphinxcrossref{\DUrole{std,std-ref}{Class ServerHealthHttpContext}}}}

\item {} 
{\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ServerStatusContext:exhale-class-classnvidia-1-1inferenceserver-1-1client-1-1serverstatuscontext}]{\sphinxcrossref{\DUrole{std,std-ref}{Class ServerStatusContext}}}}

\item {} 
{\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ServerStatusGrpcContext:exhale-class-classnvidia-1-1inferenceserver-1-1client-1-1serverstatusgrpccontext}]{\sphinxcrossref{\DUrole{std,std-ref}{Class ServerStatusGrpcContext}}}}

\item {} 
{\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ServerStatusHttpContext:exhale-class-classnvidia-1-1inferenceserver-1-1client-1-1serverstatushttpcontext}]{\sphinxcrossref{\DUrole{std,std-ref}{Class ServerStatusHttpContext}}}}

\end{itemize}


\paragraph{Functions}
\label{\detokenize{cpp_api/namespace_nvidia__inferenceserver__client:functions}}\begin{itemize}
\item {} 
{\hyperref[\detokenize{cpp_api/function_request_8h_1a19d352bb13848c14b5f03274b25ecee3:exhale-function-request-8h-1a19d352bb13848c14b5f03274b25ecee3}]{\sphinxcrossref{\DUrole{std,std-ref}{Function nvidia::inferenceserver::client::operator\textless{}\textless{}}}}}

\end{itemize}


\subsection{Classes and Structs}
\label{\detokenize{cpp_api/cpp_api_root:classes-and-structs}}

\subsubsection{Struct custom\_payload\_struct}
\label{\detokenize{cpp_api/structcustom__payload__struct:struct-custom-payload-struct}}\label{\detokenize{cpp_api/structcustom__payload__struct:exhale-struct-structcustom-payload-struct}}\label{\detokenize{cpp_api/structcustom__payload__struct::doc}}\begin{itemize}
\item {} 
Defined in {\hyperref[\detokenize{cpp_api/file_src_servables_custom_custom.h:file-src-servables-custom-custom-h}]{\sphinxcrossref{\DUrole{std,std-ref}{File custom.h}}}}

\end{itemize}


\paragraph{Struct Documentation}
\label{\detokenize{cpp_api/structcustom__payload__struct:struct-documentation}}\index{custom\_payload\_struct (C++ class)@\spxentry{custom\_payload\_struct}\spxextra{C++ class}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/structcustom__payload__struct:_CPPv421custom_payload_struct}}%
\pysigstartmultiline
\phantomsection\label{\detokenize{cpp_api/structcustom__payload__struct:structcustom__payload__struct}}\pysigline{\sphinxbfcode{\sphinxupquote{struct }}\sphinxbfcode{\sphinxupquote{custom\_payload\_struct}}}%
\pysigstopmultiline~\subsubsection*{Public Members}
\index{custom\_payload\_struct::batch\_size (C++ member)@\spxentry{custom\_payload\_struct::batch\_size}\spxextra{C++ member}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/structcustom__payload__struct:_CPPv4N21custom_payload_struct10batch_sizeE}}%
\pysigstartmultiline
\pysigline{\phantomsection\label{\detokenize{cpp_api/structcustom__payload__struct:structcustom__payload__struct_1ade508a785f227e1b75cda7e9b243ba7c}}uint32\_t \sphinxbfcode{\sphinxupquote{batch\_size}}}%
\pysigstopmultiline
\end{fulllineitems}

\index{custom\_payload\_struct::input\_cnt (C++ member)@\spxentry{custom\_payload\_struct::input\_cnt}\spxextra{C++ member}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/structcustom__payload__struct:_CPPv4N21custom_payload_struct9input_cntE}}%
\pysigstartmultiline
\pysigline{\phantomsection\label{\detokenize{cpp_api/structcustom__payload__struct:structcustom__payload__struct_1a15758b19c2bd946fd4db9252441ef92d}}uint32\_t \sphinxbfcode{\sphinxupquote{input\_cnt}}}%
\pysigstopmultiline
\end{fulllineitems}

\index{custom\_payload\_struct::input\_names (C++ member)@\spxentry{custom\_payload\_struct::input\_names}\spxextra{C++ member}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/structcustom__payload__struct:_CPPv4N21custom_payload_struct11input_namesE}}%
\pysigstartmultiline
\pysigline{\phantomsection\label{\detokenize{cpp_api/structcustom__payload__struct:structcustom__payload__struct_1a073a407347690555e3e17730465bad13}}\sphinxbfcode{\sphinxupquote{const}} char **\sphinxbfcode{\sphinxupquote{input\_names}}}%
\pysigstopmultiline
\end{fulllineitems}

\index{custom\_payload\_struct::input\_shape\_dim\_cnts (C++ member)@\spxentry{custom\_payload\_struct::input\_shape\_dim\_cnts}\spxextra{C++ member}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/structcustom__payload__struct:_CPPv4N21custom_payload_struct20input_shape_dim_cntsE}}%
\pysigstartmultiline
\pysigline{\phantomsection\label{\detokenize{cpp_api/structcustom__payload__struct:structcustom__payload__struct_1a0681cfa6856b6ec8d092157fc2bcf0ea}}\sphinxbfcode{\sphinxupquote{const}} size\_t *\sphinxbfcode{\sphinxupquote{input\_shape\_dim\_cnts}}}%
\pysigstopmultiline
\end{fulllineitems}

\index{custom\_payload\_struct::input\_shape\_dims (C++ member)@\spxentry{custom\_payload\_struct::input\_shape\_dims}\spxextra{C++ member}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/structcustom__payload__struct:_CPPv4N21custom_payload_struct16input_shape_dimsE}}%
\pysigstartmultiline
\pysigline{\phantomsection\label{\detokenize{cpp_api/structcustom__payload__struct:structcustom__payload__struct_1a861c79460be64f41742867fa07657b85}}\sphinxbfcode{\sphinxupquote{const}} int64\_t **\sphinxbfcode{\sphinxupquote{input\_shape\_dims}}}%
\pysigstopmultiline
\end{fulllineitems}

\index{custom\_payload\_struct::output\_cnt (C++ member)@\spxentry{custom\_payload\_struct::output\_cnt}\spxextra{C++ member}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/structcustom__payload__struct:_CPPv4N21custom_payload_struct10output_cntE}}%
\pysigstartmultiline
\pysigline{\phantomsection\label{\detokenize{cpp_api/structcustom__payload__struct:structcustom__payload__struct_1afdeb3c627c80586d06719ca202f4011e}}uint32\_t \sphinxbfcode{\sphinxupquote{output\_cnt}}}%
\pysigstopmultiline
\end{fulllineitems}

\index{custom\_payload\_struct::required\_output\_names (C++ member)@\spxentry{custom\_payload\_struct::required\_output\_names}\spxextra{C++ member}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/structcustom__payload__struct:_CPPv4N21custom_payload_struct21required_output_namesE}}%
\pysigstartmultiline
\pysigline{\phantomsection\label{\detokenize{cpp_api/structcustom__payload__struct:structcustom__payload__struct_1a53eaa3fc131599bc84944a407844e93c}}\sphinxbfcode{\sphinxupquote{const}} char **\sphinxbfcode{\sphinxupquote{required\_output\_names}}}%
\pysigstopmultiline
\end{fulllineitems}

\index{custom\_payload\_struct::input\_context (C++ member)@\spxentry{custom\_payload\_struct::input\_context}\spxextra{C++ member}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/structcustom__payload__struct:_CPPv4N21custom_payload_struct13input_contextE}}%
\pysigstartmultiline
\pysigline{\phantomsection\label{\detokenize{cpp_api/structcustom__payload__struct:structcustom__payload__struct_1a4e032638d78e9885b1f5e41f01f0463e}}void *\sphinxbfcode{\sphinxupquote{input\_context}}}%
\pysigstopmultiline
\end{fulllineitems}

\index{custom\_payload\_struct::output\_context (C++ member)@\spxentry{custom\_payload\_struct::output\_context}\spxextra{C++ member}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/structcustom__payload__struct:_CPPv4N21custom_payload_struct14output_contextE}}%
\pysigstartmultiline
\pysigline{\phantomsection\label{\detokenize{cpp_api/structcustom__payload__struct:structcustom__payload__struct_1a4f2283e50e0e827f0f20193f578445d6}}void *\sphinxbfcode{\sphinxupquote{output\_context}}}%
\pysigstopmultiline
\end{fulllineitems}

\index{custom\_payload\_struct::error\_code (C++ member)@\spxentry{custom\_payload\_struct::error\_code}\spxextra{C++ member}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/structcustom__payload__struct:_CPPv4N21custom_payload_struct10error_codeE}}%
\pysigstartmultiline
\pysigline{\phantomsection\label{\detokenize{cpp_api/structcustom__payload__struct:structcustom__payload__struct_1abc47d48363b2b6a175588fb570a128de}}int \sphinxbfcode{\sphinxupquote{error\_code}}}%
\pysigstopmultiline
\end{fulllineitems}


\end{fulllineitems}



\subsubsection{Struct Result::ClassResult}
\label{\detokenize{cpp_api/structnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result_1_1ClassResult:struct-result-classresult}}\label{\detokenize{cpp_api/structnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result_1_1ClassResult:exhale-struct-structnvidia-1-1inferenceserver-1-1client-1-1infercontext-1-1result-1-1classresult}}\label{\detokenize{cpp_api/structnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result_1_1ClassResult::doc}}\begin{itemize}
\item {} 
Defined in {\hyperref[\detokenize{cpp_api/file_src_clients_c++_request.h:file-src-clients-c-request-h}]{\sphinxcrossref{\DUrole{std,std-ref}{File request.h}}}}

\end{itemize}


\paragraph{Nested Relationships}
\label{\detokenize{cpp_api/structnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result_1_1ClassResult:nested-relationships}}
This struct is a nested type of {\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result:exhale-class-classnvidia-1-1inferenceserver-1-1client-1-1infercontext-1-1result}]{\sphinxcrossref{\DUrole{std,std-ref}{Class InferContext::Result}}}}.


\paragraph{Struct Documentation}
\label{\detokenize{cpp_api/structnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result_1_1ClassResult:struct-documentation}}\index{nvidia::inferenceserver::client::InferContext::Result::ClassResult (C++ class)@\spxentry{nvidia::inferenceserver::client::InferContext::Result::ClassResult}\spxextra{C++ class}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/structnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result_1_1ClassResult:_CPPv4N6nvidia15inferenceserver6client12InferContext6Result11ClassResultE}}%
\pysigstartmultiline
\phantomsection\label{\detokenize{cpp_api/structnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result_1_1ClassResult:structnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result_1_1ClassResult}}\pysigline{\sphinxbfcode{\sphinxupquote{struct }}\sphinxbfcode{\sphinxupquote{ClassResult}}}%
\pysigstopmultiline
The result value for CLASS format results. 
\subsubsection*{Public Members}
\index{nvidia::inferenceserver::client::InferContext::Result::ClassResult::idx (C++ member)@\spxentry{nvidia::inferenceserver::client::InferContext::Result::ClassResult::idx}\spxextra{C++ member}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/structnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result_1_1ClassResult:_CPPv4N6nvidia15inferenceserver6client12InferContext6Result11ClassResult3idxE}}%
\pysigstartmultiline
\pysigline{\phantomsection\label{\detokenize{cpp_api/structnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result_1_1ClassResult:structnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result_1_1ClassResult_1a8480131c8f15b37e32f19400ec5a3d5d}}size\_t \sphinxbfcode{\sphinxupquote{idx}}}%
\pysigstopmultiline
The index of the class in the result vector. 

\end{fulllineitems}

\index{nvidia::inferenceserver::client::InferContext::Result::ClassResult::value (C++ member)@\spxentry{nvidia::inferenceserver::client::InferContext::Result::ClassResult::value}\spxextra{C++ member}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/structnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result_1_1ClassResult:_CPPv4N6nvidia15inferenceserver6client12InferContext6Result11ClassResult5valueE}}%
\pysigstartmultiline
\pysigline{\phantomsection\label{\detokenize{cpp_api/structnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result_1_1ClassResult:structnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result_1_1ClassResult_1af0c1a1ef9932a40cd1fbbea3e5ae7b38}}float \sphinxbfcode{\sphinxupquote{value}}}%
\pysigstopmultiline
The value of the class. 

\end{fulllineitems}

\index{nvidia::inferenceserver::client::InferContext::Result::ClassResult::label (C++ member)@\spxentry{nvidia::inferenceserver::client::InferContext::Result::ClassResult::label}\spxextra{C++ member}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/structnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result_1_1ClassResult:_CPPv4N6nvidia15inferenceserver6client12InferContext6Result11ClassResult5labelE}}%
\pysigstartmultiline
\pysigline{\phantomsection\label{\detokenize{cpp_api/structnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result_1_1ClassResult:structnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result_1_1ClassResult_1a517c52eae642ee9a37102070da66972a}}std::string \sphinxbfcode{\sphinxupquote{label}}}%
\pysigstopmultiline
The label for the class, if provided by the model. 

\end{fulllineitems}


\end{fulllineitems}



\subsubsection{Struct InferContext::Stat}
\label{\detokenize{cpp_api/structnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Stat:struct-infercontext-stat}}\label{\detokenize{cpp_api/structnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Stat:exhale-struct-structnvidia-1-1inferenceserver-1-1client-1-1infercontext-1-1stat}}\label{\detokenize{cpp_api/structnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Stat::doc}}\begin{itemize}
\item {} 
Defined in {\hyperref[\detokenize{cpp_api/file_src_clients_c++_request.h:file-src-clients-c-request-h}]{\sphinxcrossref{\DUrole{std,std-ref}{File request.h}}}}

\end{itemize}


\paragraph{Nested Relationships}
\label{\detokenize{cpp_api/structnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Stat:nested-relationships}}
This struct is a nested type of {\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:exhale-class-classnvidia-1-1inferenceserver-1-1client-1-1infercontext}]{\sphinxcrossref{\DUrole{std,std-ref}{Class InferContext}}}}.


\paragraph{Struct Documentation}
\label{\detokenize{cpp_api/structnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Stat:struct-documentation}}\index{nvidia::inferenceserver::client::InferContext::Stat (C++ class)@\spxentry{nvidia::inferenceserver::client::InferContext::Stat}\spxextra{C++ class}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/structnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Stat:_CPPv4N6nvidia15inferenceserver6client12InferContext4StatE}}%
\pysigstartmultiline
\phantomsection\label{\detokenize{cpp_api/structnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Stat:structnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Stat}}\pysigline{\sphinxbfcode{\sphinxupquote{struct }}\sphinxbfcode{\sphinxupquote{Stat}}}%
\pysigstopmultiline
Cumulative statistic of the {\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:classnvidia_1_1inferenceserver_1_1client_1_1InferContext}]{\sphinxcrossref{\DUrole{std,std-ref}{InferContext}}}}. 

\begin{description}
\item[{\sphinxstylestrong{Note}}] \leavevmode
For GRPC protocol, ‘cumulative\_send\_time\_ns’ represents the time for marshaling infer request. ‘cumulative\_receive\_time\_ns’ represents the time for unmarshaling infer response. 

\end{description}

\subsubsection*{Public Functions}
\index{nvidia::inferenceserver::client::InferContext::Stat::Stat (C++ function)@\spxentry{nvidia::inferenceserver::client::InferContext::Stat::Stat}\spxextra{C++ function}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/structnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Stat:_CPPv4N6nvidia15inferenceserver6client12InferContext4Stat4StatEv}}%
\pysigstartmultiline
\phantomsection\label{\detokenize{cpp_api/structnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Stat:structnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Stat_1aa1c18a68160ee9ccb7002fdb40e7b15c}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{Stat}}}{}{}%
\pysigstopmultiline
Create a new {\hyperref[\detokenize{cpp_api/structnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Stat:structnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Stat}]{\sphinxcrossref{\DUrole{std,std-ref}{Stat}}}} object with zero-ed statistics. 

\end{fulllineitems}

\subsubsection*{Public Members}
\index{nvidia::inferenceserver::client::InferContext::Stat::completed\_request\_count (C++ member)@\spxentry{nvidia::inferenceserver::client::InferContext::Stat::completed\_request\_count}\spxextra{C++ member}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/structnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Stat:_CPPv4N6nvidia15inferenceserver6client12InferContext4Stat23completed_request_countE}}%
\pysigstartmultiline
\pysigline{\phantomsection\label{\detokenize{cpp_api/structnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Stat:structnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Stat_1ad76e90f2ef845eb146acab963bbffd1a}}size\_t \sphinxbfcode{\sphinxupquote{completed\_request\_count}}}%
\pysigstopmultiline
Total number of requests completed. 

\end{fulllineitems}

\index{nvidia::inferenceserver::client::InferContext::Stat::cumulative\_total\_request\_time\_ns (C++ member)@\spxentry{nvidia::inferenceserver::client::InferContext::Stat::cumulative\_total\_request\_time\_ns}\spxextra{C++ member}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/structnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Stat:_CPPv4N6nvidia15inferenceserver6client12InferContext4Stat32cumulative_total_request_time_nsE}}%
\pysigstartmultiline
\pysigline{\phantomsection\label{\detokenize{cpp_api/structnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Stat:structnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Stat_1abf5e228bd9de072b92cdbfd06b10c8cf}}uint64\_t \sphinxbfcode{\sphinxupquote{cumulative\_total\_request\_time\_ns}}}%
\pysigstopmultiline
Time from the request start until the response is completely received. 

\end{fulllineitems}

\index{nvidia::inferenceserver::client::InferContext::Stat::cumulative\_send\_time\_ns (C++ member)@\spxentry{nvidia::inferenceserver::client::InferContext::Stat::cumulative\_send\_time\_ns}\spxextra{C++ member}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/structnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Stat:_CPPv4N6nvidia15inferenceserver6client12InferContext4Stat23cumulative_send_time_nsE}}%
\pysigstartmultiline
\pysigline{\phantomsection\label{\detokenize{cpp_api/structnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Stat:structnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Stat_1a5b8451d591204705cfb9d8854f132b18}}uint64\_t \sphinxbfcode{\sphinxupquote{cumulative\_send\_time\_ns}}}%
\pysigstopmultiline
Time from the request start until the last byte is sent. 

\end{fulllineitems}

\index{nvidia::inferenceserver::client::InferContext::Stat::cumulative\_receive\_time\_ns (C++ member)@\spxentry{nvidia::inferenceserver::client::InferContext::Stat::cumulative\_receive\_time\_ns}\spxextra{C++ member}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/structnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Stat:_CPPv4N6nvidia15inferenceserver6client12InferContext4Stat26cumulative_receive_time_nsE}}%
\pysigstartmultiline
\pysigline{\phantomsection\label{\detokenize{cpp_api/structnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Stat:structnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Stat_1ae113efbf699abdd52d45996dcc7d4689}}uint64\_t \sphinxbfcode{\sphinxupquote{cumulative\_receive\_time\_ns}}}%
\pysigstopmultiline
Time from receiving first byte of the response until the response is completely received. 

\end{fulllineitems}


\end{fulllineitems}



\subsubsection{Class Error}
\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1Error:class-error}}\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1Error:exhale-class-classnvidia-1-1inferenceserver-1-1client-1-1error}}\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1Error::doc}}\begin{itemize}
\item {} 
Defined in {\hyperref[\detokenize{cpp_api/file_src_clients_c++_request.h:file-src-clients-c-request-h}]{\sphinxcrossref{\DUrole{std,std-ref}{File request.h}}}}

\end{itemize}


\paragraph{Class Documentation}
\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1Error:class-documentation}}\index{nvidia::inferenceserver::client::Error (C++ class)@\spxentry{nvidia::inferenceserver::client::Error}\spxextra{C++ class}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1Error:_CPPv4N6nvidia15inferenceserver6client5ErrorE}}%
\pysigstartmultiline
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1Error:classnvidia_1_1inferenceserver_1_1client_1_1Error}}\pysigline{\sphinxbfcode{\sphinxupquote{class }}\sphinxbfcode{\sphinxupquote{Error}}}%
\pysigstopmultiline
{\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1Error:classnvidia_1_1inferenceserver_1_1client_1_1Error}]{\sphinxcrossref{\DUrole{std,std-ref}{Error}}}} status reported by client API. 
\subsubsection*{Public Functions}
\index{nvidia::inferenceserver::client::Error::Error (C++ function)@\spxentry{nvidia::inferenceserver::client::Error::Error}\spxextra{C++ function}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1Error:_CPPv4N6nvidia15inferenceserver6client5Error5ErrorERK13RequestStatus}}%
\pysigstartmultiline
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1Error:classnvidia_1_1inferenceserver_1_1client_1_1Error_1abd0e1c07fa0d178b5f9c468a772de6e7}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{Error}}}{\sphinxbfcode{\sphinxupquote{const}} RequestStatus \&\sphinxstyleemphasis{status}}{}%
\pysigstopmultiline
Create an error from a RequestStatus. 

\begin{description}
\item[{\sphinxstylestrong{Parameters}}] \leavevmode\begin{itemize}
\item {} 
\sphinxcode{\sphinxupquote{status}}: The RequestStatus object 

\end{itemize}

\end{description}


\end{fulllineitems}

\index{nvidia::inferenceserver::client::Error::Error (C++ function)@\spxentry{nvidia::inferenceserver::client::Error::Error}\spxextra{C++ function}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1Error:_CPPv4N6nvidia15inferenceserver6client5Error5ErrorE17RequestStatusCode}}%
\pysigstartmultiline
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1Error:classnvidia_1_1inferenceserver_1_1client_1_1Error_1ab28352e8abccc6c246c06b817c071924}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{Error}}}{RequestStatusCode \sphinxstyleemphasis{code} = RequestStatusCode::SUCCESS}{}%
\pysigstopmultiline
Create an error from a RequestStatusCode. 

\begin{description}
\item[{\sphinxstylestrong{Parameters}}] \leavevmode\begin{itemize}
\item {} 
\sphinxcode{\sphinxupquote{code}}: The status code for the error 

\end{itemize}

\end{description}


\end{fulllineitems}

\index{nvidia::inferenceserver::client::Error::Error (C++ function)@\spxentry{nvidia::inferenceserver::client::Error::Error}\spxextra{C++ function}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1Error:_CPPv4N6nvidia15inferenceserver6client5Error5ErrorE17RequestStatusCodeRKNSt6stringE}}%
\pysigstartmultiline
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1Error:classnvidia_1_1inferenceserver_1_1client_1_1Error_1a61c56c1f9b3112ffae6c1573a015e29f}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{Error}}}{RequestStatusCode \sphinxstyleemphasis{code}, \sphinxbfcode{\sphinxupquote{const}} std::string \&\sphinxstyleemphasis{msg}}{}%
\pysigstopmultiline
Create an error from a RequestStatusCode and a detailed message. 

\begin{description}
\item[{\sphinxstylestrong{Parameters}}] \leavevmode\begin{itemize}
\item {} 
\sphinxcode{\sphinxupquote{code}}: The status code for the error 

\item {} 
\sphinxcode{\sphinxupquote{msg}}: The detailed message for the error 

\end{itemize}

\end{description}


\end{fulllineitems}

\index{nvidia::inferenceserver::client::Error::Code (C++ function)@\spxentry{nvidia::inferenceserver::client::Error::Code}\spxextra{C++ function}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1Error:_CPPv4NK6nvidia15inferenceserver6client5Error4CodeEv}}%
\pysigstartmultiline
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1Error:classnvidia_1_1inferenceserver_1_1client_1_1Error_1a1f4a3b6c8519dd1decea149546da93d0}}\pysiglinewithargsret{RequestStatusCode \sphinxbfcode{\sphinxupquote{Code}}}{}{ \sphinxbfcode{\sphinxupquote{const}}}%
\pysigstopmultiline
Accessor for the RequestStatusCode of this error. 

\begin{description}
\item[{\sphinxstylestrong{Return}}] \leavevmode
The RequestStatusCode for the error. 

\end{description}


\end{fulllineitems}

\index{nvidia::inferenceserver::client::Error::Message (C++ function)@\spxentry{nvidia::inferenceserver::client::Error::Message}\spxextra{C++ function}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1Error:_CPPv4NK6nvidia15inferenceserver6client5Error7MessageEv}}%
\pysigstartmultiline
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1Error:classnvidia_1_1inferenceserver_1_1client_1_1Error_1a6aa771500474272751705b9ceab0df69}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{const}} std::string \&\sphinxbfcode{\sphinxupquote{Message}}}{}{ \sphinxbfcode{\sphinxupquote{const}}}%
\pysigstopmultiline
Accessor for the message of this error. 

\begin{description}
\item[{\sphinxstylestrong{Return}}] \leavevmode
The detailed messsage for the error. Empty if no detailed message. 

\end{description}


\end{fulllineitems}

\index{nvidia::inferenceserver::client::Error::ServerId (C++ function)@\spxentry{nvidia::inferenceserver::client::Error::ServerId}\spxextra{C++ function}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1Error:_CPPv4NK6nvidia15inferenceserver6client5Error8ServerIdEv}}%
\pysigstartmultiline
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1Error:classnvidia_1_1inferenceserver_1_1client_1_1Error_1a46d23219fef60a764ad9ba638321179a}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{const}} std::string \&\sphinxbfcode{\sphinxupquote{ServerId}}}{}{ \sphinxbfcode{\sphinxupquote{const}}}%
\pysigstopmultiline
Accessor for the ID of the inference server associated with this error. 

\begin{description}
\item[{\sphinxstylestrong{Return}}] \leavevmode
The ID of the inference server associated with this error, or empty-string if no inference server is associated with the error. 

\end{description}


\end{fulllineitems}

\index{nvidia::inferenceserver::client::Error::RequestId (C++ function)@\spxentry{nvidia::inferenceserver::client::Error::RequestId}\spxextra{C++ function}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1Error:_CPPv4NK6nvidia15inferenceserver6client5Error9RequestIdEv}}%
\pysigstartmultiline
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1Error:classnvidia_1_1inferenceserver_1_1client_1_1Error_1ad3f77dde3f3a2c3ea8230ca85a6b19ce}}\pysiglinewithargsret{uint64\_t \sphinxbfcode{\sphinxupquote{RequestId}}}{}{ \sphinxbfcode{\sphinxupquote{const}}}%
\pysigstopmultiline
Accessor for the ID of the request associated with this error. 

\begin{description}
\item[{\sphinxstylestrong{Return}}] \leavevmode
The ID of the request associated with this error, or 0 (zero) if no request ID is associated with the error. 

\end{description}


\end{fulllineitems}

\index{nvidia::inferenceserver::client::Error::IsOk (C++ function)@\spxentry{nvidia::inferenceserver::client::Error::IsOk}\spxextra{C++ function}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1Error:_CPPv4NK6nvidia15inferenceserver6client5Error4IsOkEv}}%
\pysigstartmultiline
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1Error:classnvidia_1_1inferenceserver_1_1client_1_1Error_1ae9f38b1f7ff0986190433ec2f8d97602}}\pysiglinewithargsret{bool \sphinxbfcode{\sphinxupquote{IsOk}}}{}{ \sphinxbfcode{\sphinxupquote{const}}}%
\pysigstopmultiline
Does this error indicate OK status? 

\begin{description}
\item[{\sphinxstylestrong{Return}}] \leavevmode
True if this error indicates “ok”/”success”, false if error indicates a failure. 

\end{description}


\end{fulllineitems}

\subsubsection*{Public Static Attributes}
\index{nvidia::inferenceserver::client::Error::Success (C++ member)@\spxentry{nvidia::inferenceserver::client::Error::Success}\spxextra{C++ member}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1Error:_CPPv4N6nvidia15inferenceserver6client5Error7SuccessE}}%
\pysigstartmultiline
\pysigline{\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1Error:classnvidia_1_1inferenceserver_1_1client_1_1Error_1a5cd97dd0cb13fae9dcf2ff392f46d127}}\sphinxbfcode{\sphinxupquote{const}} {\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1Error:_CPPv4N6nvidia15inferenceserver6client5ErrorE}]{\sphinxcrossref{Error}}} \sphinxbfcode{\sphinxupquote{Success}}}%
\pysigstopmultiline
Convenience “success” value. 

Can be used as {\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1Error:classnvidia_1_1inferenceserver_1_1client_1_1Error_1a5cd97dd0cb13fae9dcf2ff392f46d127}]{\sphinxcrossref{\DUrole{std,std-ref}{Error::Success}}}} to indicate no error. 

\end{fulllineitems}


\end{fulllineitems}



\subsubsection{Class InferContext}
\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:class-infercontext}}\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:exhale-class-classnvidia-1-1inferenceserver-1-1client-1-1infercontext}}\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext::doc}}\begin{itemize}
\item {} 
Defined in {\hyperref[\detokenize{cpp_api/file_src_clients_c++_request.h:file-src-clients-c-request-h}]{\sphinxcrossref{\DUrole{std,std-ref}{File request.h}}}}

\end{itemize}


\paragraph{Nested Relationships}
\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:nested-relationships}}

\subparagraph{Nested Types}
\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:nested-types}}\begin{itemize}
\item {} 
{\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Input:exhale-class-classnvidia-1-1inferenceserver-1-1client-1-1infercontext-1-1input}]{\sphinxcrossref{\DUrole{std,std-ref}{Class InferContext::Input}}}}

\item {} 
{\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Options:exhale-class-classnvidia-1-1inferenceserver-1-1client-1-1infercontext-1-1options}]{\sphinxcrossref{\DUrole{std,std-ref}{Class InferContext::Options}}}}

\item {} 
{\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Output:exhale-class-classnvidia-1-1inferenceserver-1-1client-1-1infercontext-1-1output}]{\sphinxcrossref{\DUrole{std,std-ref}{Class InferContext::Output}}}}

\item {} 
{\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Request:exhale-class-classnvidia-1-1inferenceserver-1-1client-1-1infercontext-1-1request}]{\sphinxcrossref{\DUrole{std,std-ref}{Class InferContext::Request}}}}

\item {} 
{\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1RequestTimers:exhale-class-classnvidia-1-1inferenceserver-1-1client-1-1infercontext-1-1requesttimers}]{\sphinxcrossref{\DUrole{std,std-ref}{Class InferContext::RequestTimers}}}}

\item {} 
{\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result:exhale-class-classnvidia-1-1inferenceserver-1-1client-1-1infercontext-1-1result}]{\sphinxcrossref{\DUrole{std,std-ref}{Class InferContext::Result}}}}

\item {} 
{\hyperref[\detokenize{cpp_api/structnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result_1_1ClassResult:exhale-struct-structnvidia-1-1inferenceserver-1-1client-1-1infercontext-1-1result-1-1classresult}]{\sphinxcrossref{\DUrole{std,std-ref}{Struct Result::ClassResult}}}}

\item {} 
{\hyperref[\detokenize{cpp_api/structnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Stat:exhale-struct-structnvidia-1-1inferenceserver-1-1client-1-1infercontext-1-1stat}]{\sphinxcrossref{\DUrole{std,std-ref}{Struct InferContext::Stat}}}}

\end{itemize}


\paragraph{Inheritance Relationships}
\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:inheritance-relationships}}

\subparagraph{Derived Types}
\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:derived-types}}\begin{itemize}
\item {} 
\sphinxcode{\sphinxupquote{public nvidia::inferenceserver::client::InferGrpcContext}} ({\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferGrpcContext:exhale-class-classnvidia-1-1inferenceserver-1-1client-1-1infergrpccontext}]{\sphinxcrossref{\DUrole{std,std-ref}{Class InferGrpcContext}}}})

\item {} 
\sphinxcode{\sphinxupquote{public nvidia::inferenceserver::client::InferHttpContext}} ({\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferHttpContext:exhale-class-classnvidia-1-1inferenceserver-1-1client-1-1inferhttpcontext}]{\sphinxcrossref{\DUrole{std,std-ref}{Class InferHttpContext}}}})

\end{itemize}


\paragraph{Class Documentation}
\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:class-documentation}}\index{nvidia::inferenceserver::client::InferContext (C++ class)@\spxentry{nvidia::inferenceserver::client::InferContext}\spxextra{C++ class}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:_CPPv4N6nvidia15inferenceserver6client12InferContextE}}%
\pysigstartmultiline
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:classnvidia_1_1inferenceserver_1_1client_1_1InferContext}}\pysigline{\sphinxbfcode{\sphinxupquote{class }}\sphinxbfcode{\sphinxupquote{InferContext}}}%
\pysigstopmultiline
An {\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:classnvidia_1_1inferenceserver_1_1client_1_1InferContext}]{\sphinxcrossref{\DUrole{std,std-ref}{InferContext}}}} object is used to run inference on an inference server for a specific model. 

Once created an {\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:classnvidia_1_1inferenceserver_1_1client_1_1InferContext}]{\sphinxcrossref{\DUrole{std,std-ref}{InferContext}}}} object can be used repeatedly to perform inference using the model. {\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Options:classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Options}]{\sphinxcrossref{\DUrole{std,std-ref}{Options}}}} that control how inference is performed can be changed in between inference runs.

A {\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:classnvidia_1_1inferenceserver_1_1client_1_1InferContext}]{\sphinxcrossref{\DUrole{std,std-ref}{InferContext}}}} object can use either HTTP protocol or GRPC protocol depending on the Create function ({\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferHttpContext:classnvidia_1_1inferenceserver_1_1client_1_1InferHttpContext_1a7c210c15455fde64a31c6925f3a8b906}]{\sphinxcrossref{\DUrole{std,std-ref}{InferHttpContext::Create}}}} or {\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferGrpcContext:classnvidia_1_1inferenceserver_1_1client_1_1InferGrpcContext_1a432bf0981eb512786263764f090ee83d}]{\sphinxcrossref{\DUrole{std,std-ref}{InferGrpcContext::Create}}}}). For example:


\begin{sphinxVerbatim}[commandchars=\\\{\}]
std::unique\PYGZus{}ptr\PYGZlt{}InferContext\PYGZgt{} ctx;
InferHttpContext::Create(\PYGZam{}ctx, \PYGZdq{}localhost:8000\PYGZdq{}, \PYGZdq{}mnist\PYGZdq{});
...
std::unique\PYGZus{}ptr\PYGZlt{}Options\PYGZgt{} options0;
Options::Create(\PYGZam{}options0);
options\PYGZhy{}\PYGZgt{}SetBatchSize(b);
options\PYGZhy{}\PYGZgt{}AddClassResult(output, topk);
ctx\PYGZhy{}\PYGZgt{}SetRunOptions(*options0);
...
ctx\PYGZhy{}\PYGZgt{}Run(\PYGZam{}results0);  // run using options0
ctx\PYGZhy{}\PYGZgt{}Run(\PYGZam{}results1);  // run using options0
...
std::unique\PYGZus{}ptr\PYGZlt{}Options\PYGZgt{} options1;
Options::Create(\PYGZam{}options1);
options\PYGZhy{}\PYGZgt{}AddRawResult(output);
ctx\PYGZhy{}\PYGZgt{}SetRunOptions(*options);
...
ctx\PYGZhy{}\PYGZgt{}Run(\PYGZam{}results2);  // run using options1
ctx\PYGZhy{}\PYGZgt{}Run(\PYGZam{}results3);  // run using options1
...
\end{sphinxVerbatim}


\begin{description}
\item[{\sphinxstylestrong{Note}}] \leavevmode
InferContext::Create methods are thread-safe. All other {\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:classnvidia_1_1inferenceserver_1_1client_1_1InferContext}]{\sphinxcrossref{\DUrole{std,std-ref}{InferContext}}}} methods, and nested class methods are not thread-safe. 

\item[{\sphinxstylestrong{}}] \leavevmode
The {\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1a4cac8002817c9259851305b14c258f36}]{\sphinxcrossref{\DUrole{std,std-ref}{Run()}}}} calls are not thread-safe but a new {\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1a4cac8002817c9259851305b14c258f36}]{\sphinxcrossref{\DUrole{std,std-ref}{Run()}}}} can be invoked as soon as the previous completes. The returned result objects are owned by the caller and may be retained and accessed even after the {\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:classnvidia_1_1inferenceserver_1_1client_1_1InferContext}]{\sphinxcrossref{\DUrole{std,std-ref}{InferContext}}}} object is destroyed. 

\item[{\sphinxstylestrong{}}] \leavevmode
{\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1a255a59df29a53d6e45ce86663887615f}]{\sphinxcrossref{\DUrole{std,std-ref}{AsyncRun()}}}} and GetAsyncRunStatus() calls are not thread-safe. What’s more, calling one method while the other one is running will result in undefined behavior given that they will modify the shared data internally. 

\item[{\sphinxstylestrong{}}] \leavevmode
For more parallelism multiple {\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:classnvidia_1_1inferenceserver_1_1client_1_1InferContext}]{\sphinxcrossref{\DUrole{std,std-ref}{InferContext}}}} objects can access the same inference server with no serialization requirements across those objects.  

\end{description}


Subclassed by {\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferGrpcContext:classnvidia_1_1inferenceserver_1_1client_1_1InferGrpcContext}]{\sphinxcrossref{\DUrole{std,std-ref}{nvidia::inferenceserver::client::InferGrpcContext}}}}, {\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferHttpContext:classnvidia_1_1inferenceserver_1_1client_1_1InferHttpContext}]{\sphinxcrossref{\DUrole{std,std-ref}{nvidia::inferenceserver::client::InferHttpContext}}}}
\subsubsection*{Public Types}
\index{nvidia::inferenceserver::client::InferContext::ResultMap (C++ type)@\spxentry{nvidia::inferenceserver::client::InferContext::ResultMap}\spxextra{C++ type}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:_CPPv4N6nvidia15inferenceserver6client12InferContext9ResultMapE}}%
\pysigstartmultiline
\pysigline{\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1a8540194150819719c190049fab1e125c}}\sphinxbfcode{\sphinxupquote{using }}\sphinxbfcode{\sphinxupquote{ResultMap}} = std::map\textless{}std::string, std::unique\_ptr\textless{}{\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:_CPPv4N6nvidia15inferenceserver6client12InferContext6ResultE}]{\sphinxcrossref{Result}}}\textgreater{}\textgreater{}}%
\pysigstopmultiline
\end{fulllineitems}

\subsubsection*{Public Functions}
\index{nvidia::inferenceserver::client::InferContext::\textasciitilde{}InferContext (C++ function)@\spxentry{nvidia::inferenceserver::client::InferContext::\textasciitilde{}InferContext}\spxextra{C++ function}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:_CPPv4N6nvidia15inferenceserver6client12InferContextD0Ev}}%
\pysigstartmultiline
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1a3a996794a2f69c24ab50391fcc36ceec}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{virtual}} \sphinxbfcode{\sphinxupquote{\textasciitilde{}InferContext}}}{}{}%
\pysigstopmultiline
Destroy the inference context. 

\end{fulllineitems}

\index{nvidia::inferenceserver::client::InferContext::ModelName (C++ function)@\spxentry{nvidia::inferenceserver::client::InferContext::ModelName}\spxextra{C++ function}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:_CPPv4NK6nvidia15inferenceserver6client12InferContext9ModelNameEv}}%
\pysigstartmultiline
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1a6b01d3aece1500914c773fb9d88cb44b}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{const}} std::string \&\sphinxbfcode{\sphinxupquote{ModelName}}}{}{ \sphinxbfcode{\sphinxupquote{const}}}%
\pysigstopmultiline
\begin{description}
\item[{\sphinxstylestrong{Return}}] \leavevmode
The name of the model being used for this context. 

\end{description}


\end{fulllineitems}

\index{nvidia::inferenceserver::client::InferContext::ModelVersion (C++ function)@\spxentry{nvidia::inferenceserver::client::InferContext::ModelVersion}\spxextra{C++ function}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:_CPPv4NK6nvidia15inferenceserver6client12InferContext12ModelVersionEv}}%
\pysigstartmultiline
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1a88642030870fb82b452a48a1676755d0}}\pysiglinewithargsret{int64\_t \sphinxbfcode{\sphinxupquote{ModelVersion}}}{}{ \sphinxbfcode{\sphinxupquote{const}}}%
\pysigstopmultiline
\begin{description}
\item[{\sphinxstylestrong{Return}}] \leavevmode
The version of the model being used for this context. -1 indicates that the latest (i.e. highest version number) version of that model is being used. 

\end{description}


\end{fulllineitems}

\index{nvidia::inferenceserver::client::InferContext::MaxBatchSize (C++ function)@\spxentry{nvidia::inferenceserver::client::InferContext::MaxBatchSize}\spxextra{C++ function}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:_CPPv4NK6nvidia15inferenceserver6client12InferContext12MaxBatchSizeEv}}%
\pysigstartmultiline
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1ac9bd9ba3234a0c282383cdd939ca16dc}}\pysiglinewithargsret{uint64\_t \sphinxbfcode{\sphinxupquote{MaxBatchSize}}}{}{ \sphinxbfcode{\sphinxupquote{const}}}%
\pysigstopmultiline
\begin{description}
\item[{\sphinxstylestrong{Return}}] \leavevmode
The maximum batch size supported by the context. A maximum batch size indicates that the context does not support batching and so only a single inference at a time can be performed. 

\end{description}


\end{fulllineitems}

\index{nvidia::inferenceserver::client::InferContext::Inputs (C++ function)@\spxentry{nvidia::inferenceserver::client::InferContext::Inputs}\spxextra{C++ function}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:_CPPv4NK6nvidia15inferenceserver6client12InferContext6InputsEv}}%
\pysigstartmultiline
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1a7b98f4f0fa2fe0f948f2a06d91ec92f3}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{const}} std::vector\textless{}std::shared\_ptr\textless{}{\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:_CPPv4N6nvidia15inferenceserver6client12InferContext5InputE}]{\sphinxcrossref{Input}}}\textgreater{}\textgreater{} \&\sphinxbfcode{\sphinxupquote{Inputs}}}{}{ \sphinxbfcode{\sphinxupquote{const}}}%
\pysigstopmultiline
\begin{description}
\item[{\sphinxstylestrong{Return}}] \leavevmode
The inputs of the model. 

\end{description}


\end{fulllineitems}

\index{nvidia::inferenceserver::client::InferContext::Outputs (C++ function)@\spxentry{nvidia::inferenceserver::client::InferContext::Outputs}\spxextra{C++ function}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:_CPPv4NK6nvidia15inferenceserver6client12InferContext7OutputsEv}}%
\pysigstartmultiline
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1a4a783616ac9f48e21bd89abb69d0b21a}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{const}} std::vector\textless{}std::shared\_ptr\textless{}{\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:_CPPv4N6nvidia15inferenceserver6client12InferContext6OutputE}]{\sphinxcrossref{Output}}}\textgreater{}\textgreater{} \&\sphinxbfcode{\sphinxupquote{Outputs}}}{}{ \sphinxbfcode{\sphinxupquote{const}}}%
\pysigstopmultiline
\begin{description}
\item[{\sphinxstylestrong{Return}}] \leavevmode
The outputs of the model. 

\end{description}


\end{fulllineitems}

\index{nvidia::inferenceserver::client::InferContext::GetInput (C++ function)@\spxentry{nvidia::inferenceserver::client::InferContext::GetInput}\spxextra{C++ function}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:_CPPv4NK6nvidia15inferenceserver6client12InferContext8GetInputERKNSt6stringEPNSt10shared_ptrI5InputEE}}%
\pysigstartmultiline
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1ac8ca982d9c589b2c27c9a279699ac884}}\pysiglinewithargsret{{\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1Error:_CPPv4N6nvidia15inferenceserver6client5ErrorE}]{\sphinxcrossref{Error}}} \sphinxbfcode{\sphinxupquote{GetInput}}}{\sphinxbfcode{\sphinxupquote{const}} std::string \&\sphinxstyleemphasis{name}, std::shared\_ptr\textless{}{\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:_CPPv4N6nvidia15inferenceserver6client12InferContext5InputE}]{\sphinxcrossref{Input}}}\textgreater{} *\sphinxstyleemphasis{input}}{ \sphinxbfcode{\sphinxupquote{const}}}%
\pysigstopmultiline
Get a named input. 

\begin{description}
\item[{\sphinxstylestrong{Return}}] \leavevmode
{\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1Error:classnvidia_1_1inferenceserver_1_1client_1_1Error}]{\sphinxcrossref{\DUrole{std,std-ref}{Error}}}} object indicating success or failure. 

\item[{\sphinxstylestrong{Parameters}}] \leavevmode\begin{itemize}
\item {} 
\sphinxcode{\sphinxupquote{name}}: The name of the input. 

\item {} 
\sphinxcode{\sphinxupquote{input}}: Returns the {\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Input:classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Input}]{\sphinxcrossref{\DUrole{std,std-ref}{Input}}}} object for ‘name’. 

\end{itemize}

\end{description}


\end{fulllineitems}

\index{nvidia::inferenceserver::client::InferContext::GetOutput (C++ function)@\spxentry{nvidia::inferenceserver::client::InferContext::GetOutput}\spxextra{C++ function}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:_CPPv4NK6nvidia15inferenceserver6client12InferContext9GetOutputERKNSt6stringEPNSt10shared_ptrI6OutputEE}}%
\pysigstartmultiline
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1a0d3e9e6e55f3162c7148574af7aec251}}\pysiglinewithargsret{{\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1Error:_CPPv4N6nvidia15inferenceserver6client5ErrorE}]{\sphinxcrossref{Error}}} \sphinxbfcode{\sphinxupquote{GetOutput}}}{\sphinxbfcode{\sphinxupquote{const}} std::string \&\sphinxstyleemphasis{name}, std::shared\_ptr\textless{}{\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:_CPPv4N6nvidia15inferenceserver6client12InferContext6OutputE}]{\sphinxcrossref{Output}}}\textgreater{} *\sphinxstyleemphasis{output}}{ \sphinxbfcode{\sphinxupquote{const}}}%
\pysigstopmultiline
Get a named output. 

\begin{description}
\item[{\sphinxstylestrong{Return}}] \leavevmode
{\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1Error:classnvidia_1_1inferenceserver_1_1client_1_1Error}]{\sphinxcrossref{\DUrole{std,std-ref}{Error}}}} object indicating success or failure. 

\item[{\sphinxstylestrong{Parameters}}] \leavevmode\begin{itemize}
\item {} 
\sphinxcode{\sphinxupquote{name}}: The name of the output. 

\item {} 
\sphinxcode{\sphinxupquote{output}}: Returns the {\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Output:classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Output}]{\sphinxcrossref{\DUrole{std,std-ref}{Output}}}} object for ‘name’. 

\end{itemize}

\end{description}


\end{fulllineitems}

\index{nvidia::inferenceserver::client::InferContext::SetRunOptions (C++ function)@\spxentry{nvidia::inferenceserver::client::InferContext::SetRunOptions}\spxextra{C++ function}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:_CPPv4N6nvidia15inferenceserver6client12InferContext13SetRunOptionsERK7Options}}%
\pysigstartmultiline
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1a7cf1746d814561727fd3b07b0f8349f2}}\pysiglinewithargsret{{\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1Error:_CPPv4N6nvidia15inferenceserver6client5ErrorE}]{\sphinxcrossref{Error}}} \sphinxbfcode{\sphinxupquote{SetRunOptions}}}{\sphinxbfcode{\sphinxupquote{const}} {\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:_CPPv4N6nvidia15inferenceserver6client12InferContext7OptionsE}]{\sphinxcrossref{Options}}} \&\sphinxstyleemphasis{options}}{}%
\pysigstopmultiline
Set the options to use for all subsequent {\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1a4cac8002817c9259851305b14c258f36}]{\sphinxcrossref{\DUrole{std,std-ref}{Run()}}}} invocations. 

\begin{description}
\item[{\sphinxstylestrong{Return}}] \leavevmode
{\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1Error:classnvidia_1_1inferenceserver_1_1client_1_1Error}]{\sphinxcrossref{\DUrole{std,std-ref}{Error}}}} object indicating success or failure. 

\item[{\sphinxstylestrong{Parameters}}] \leavevmode\begin{itemize}
\item {} 
\sphinxcode{\sphinxupquote{options}}: The options. 

\end{itemize}

\end{description}


\end{fulllineitems}

\index{nvidia::inferenceserver::client::InferContext::GetStat (C++ function)@\spxentry{nvidia::inferenceserver::client::InferContext::GetStat}\spxextra{C++ function}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:_CPPv4N6nvidia15inferenceserver6client12InferContext7GetStatEP4Stat}}%
\pysigstartmultiline
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1aa083c81909e7a6d10725306d231ea0cb}}\pysiglinewithargsret{{\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1Error:_CPPv4N6nvidia15inferenceserver6client5ErrorE}]{\sphinxcrossref{Error}}} \sphinxbfcode{\sphinxupquote{GetStat}}}{{\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:_CPPv4N6nvidia15inferenceserver6client12InferContext4StatE}]{\sphinxcrossref{Stat}}} *\sphinxstyleemphasis{stat}}{}%
\pysigstopmultiline
Get the current statistics of the {\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:classnvidia_1_1inferenceserver_1_1client_1_1InferContext}]{\sphinxcrossref{\DUrole{std,std-ref}{InferContext}}}}. 

\begin{description}
\item[{\sphinxstylestrong{Return}}] \leavevmode
{\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1Error:classnvidia_1_1inferenceserver_1_1client_1_1Error}]{\sphinxcrossref{\DUrole{std,std-ref}{Error}}}} object indicating success or failure. 

\item[{\sphinxstylestrong{Parameters}}] \leavevmode\begin{itemize}
\item {} 
\sphinxcode{\sphinxupquote{stat}}: Returns the {\hyperref[\detokenize{cpp_api/structnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Stat:structnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Stat}]{\sphinxcrossref{\DUrole{std,std-ref}{Stat}}}} object holding the statistics. 

\end{itemize}

\end{description}


\end{fulllineitems}

\index{nvidia::inferenceserver::client::InferContext::Run (C++ function)@\spxentry{nvidia::inferenceserver::client::InferContext::Run}\spxextra{C++ function}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:_CPPv4N6nvidia15inferenceserver6client12InferContext3RunEP9ResultMap}}%
\pysigstartmultiline
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1a4cac8002817c9259851305b14c258f36}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{virtual}} {\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1Error:_CPPv4N6nvidia15inferenceserver6client5ErrorE}]{\sphinxcrossref{Error}}} \sphinxbfcode{\sphinxupquote{Run}}}{{\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:_CPPv4N6nvidia15inferenceserver6client12InferContext9ResultMapE}]{\sphinxcrossref{ResultMap}}} *\sphinxstyleemphasis{results}}{ = 0}%
\pysigstopmultiline
Send a synchronous request to the inference server to perform an inference to produce results for the outputs specified in the most recent call to {\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1a7cf1746d814561727fd3b07b0f8349f2}]{\sphinxcrossref{\DUrole{std,std-ref}{SetRunOptions()}}}}. 

\begin{description}
\item[{\sphinxstylestrong{Return}}] \leavevmode
{\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1Error:classnvidia_1_1inferenceserver_1_1client_1_1Error}]{\sphinxcrossref{\DUrole{std,std-ref}{Error}}}} object indicating success or failure. 

\item[{\sphinxstylestrong{Parameters}}] \leavevmode\begin{itemize}
\item {} 
\sphinxcode{\sphinxupquote{results}}: Returns {\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result:classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result}]{\sphinxcrossref{\DUrole{std,std-ref}{Result}}}} objects holding inference results as a map from output name to {\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result:classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result}]{\sphinxcrossref{\DUrole{std,std-ref}{Result}}}} object. 

\end{itemize}

\end{description}


\end{fulllineitems}

\index{nvidia::inferenceserver::client::InferContext::AsyncRun (C++ function)@\spxentry{nvidia::inferenceserver::client::InferContext::AsyncRun}\spxextra{C++ function}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:_CPPv4N6nvidia15inferenceserver6client12InferContext8AsyncRunEPNSt10shared_ptrI7RequestEE}}%
\pysigstartmultiline
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1a255a59df29a53d6e45ce86663887615f}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{virtual}} {\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1Error:_CPPv4N6nvidia15inferenceserver6client5ErrorE}]{\sphinxcrossref{Error}}} \sphinxbfcode{\sphinxupquote{AsyncRun}}}{std::shared\_ptr\textless{}{\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:_CPPv4N6nvidia15inferenceserver6client12InferContext7RequestE}]{\sphinxcrossref{Request}}}\textgreater{} *\sphinxstyleemphasis{async\_request}}{ = 0}%
\pysigstopmultiline
Send an asynchronous request to the inference server to perform an inference to produce results for the outputs specified in the most recent call to {\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1a7cf1746d814561727fd3b07b0f8349f2}]{\sphinxcrossref{\DUrole{std,std-ref}{SetRunOptions()}}}}. 

\begin{description}
\item[{\sphinxstylestrong{Return}}] \leavevmode
{\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1Error:classnvidia_1_1inferenceserver_1_1client_1_1Error}]{\sphinxcrossref{\DUrole{std,std-ref}{Error}}}} object indicating success or failure. 

\item[{\sphinxstylestrong{Parameters}}] \leavevmode\begin{itemize}
\item {} 
\sphinxcode{\sphinxupquote{async\_request}}: Returns a {\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Request:classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Request}]{\sphinxcrossref{\DUrole{std,std-ref}{Request}}}} object that can be used to retrieve the inference results for the request. 

\end{itemize}

\end{description}


\end{fulllineitems}

\index{nvidia::inferenceserver::client::InferContext::GetAsyncRunResults (C++ function)@\spxentry{nvidia::inferenceserver::client::InferContext::GetAsyncRunResults}\spxextra{C++ function}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:_CPPv4N6nvidia15inferenceserver6client12InferContext18GetAsyncRunResultsEP9ResultMapRKNSt10shared_ptrI7RequestEEb}}%
\pysigstartmultiline
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1a6a9a45e35031ce4b0b50af8bd0c4ee8c}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{virtual}} {\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1Error:_CPPv4N6nvidia15inferenceserver6client5ErrorE}]{\sphinxcrossref{Error}}} \sphinxbfcode{\sphinxupquote{GetAsyncRunResults}}}{{\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:_CPPv4N6nvidia15inferenceserver6client12InferContext9ResultMapE}]{\sphinxcrossref{ResultMap}}} *\sphinxstyleemphasis{results}, \sphinxbfcode{\sphinxupquote{const}} std::shared\_ptr\textless{}{\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:_CPPv4N6nvidia15inferenceserver6client12InferContext7RequestE}]{\sphinxcrossref{Request}}}\textgreater{} \&\sphinxstyleemphasis{async\_request}, bool \sphinxstyleemphasis{wait}}{ = 0}%
\pysigstopmultiline
Get the results of the asynchronous request referenced by ‘async\_request’. 

\begin{description}
\item[{\sphinxstylestrong{Return}}] \leavevmode
{\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1Error:classnvidia_1_1inferenceserver_1_1client_1_1Error}]{\sphinxcrossref{\DUrole{std,std-ref}{Error}}}} object indicating success or failure. Success will be returned only if the request has been completed succesfully. UNAVAILABLE will be returned if ‘wait’ is false and the request is not ready. 

\item[{\sphinxstylestrong{Parameters}}] \leavevmode\begin{itemize}
\item {} 
\sphinxcode{\sphinxupquote{results}}: Returns {\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result:classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result}]{\sphinxcrossref{\DUrole{std,std-ref}{Result}}}} objects holding inference results as a map from output name to {\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result:classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result}]{\sphinxcrossref{\DUrole{std,std-ref}{Result}}}} object. 

\item {} 
\sphinxcode{\sphinxupquote{async\_request}}: {\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Request:classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Request}]{\sphinxcrossref{\DUrole{std,std-ref}{Request}}}} handle to retrieve results. 

\item {} 
\sphinxcode{\sphinxupquote{wait}}: If true, block until the request completes. Otherwise, return immediately. 

\end{itemize}

\end{description}


\end{fulllineitems}

\index{nvidia::inferenceserver::client::InferContext::GetReadyAsyncRequest (C++ function)@\spxentry{nvidia::inferenceserver::client::InferContext::GetReadyAsyncRequest}\spxextra{C++ function}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:_CPPv4N6nvidia15inferenceserver6client12InferContext20GetReadyAsyncRequestEPNSt10shared_ptrI7RequestEEb}}%
\pysigstartmultiline
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1a27512d1f8f7099d20b2e0eaaac90c3f0}}\pysiglinewithargsret{{\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1Error:_CPPv4N6nvidia15inferenceserver6client5ErrorE}]{\sphinxcrossref{Error}}} \sphinxbfcode{\sphinxupquote{GetReadyAsyncRequest}}}{std::shared\_ptr\textless{}{\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:_CPPv4N6nvidia15inferenceserver6client12InferContext7RequestE}]{\sphinxcrossref{Request}}}\textgreater{} *\sphinxstyleemphasis{async\_request}, bool \sphinxstyleemphasis{wait}}{}%
\pysigstopmultiline
Get any one completed asynchronous request. 

\begin{description}
\item[{\sphinxstylestrong{Return}}] \leavevmode
{\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1Error:classnvidia_1_1inferenceserver_1_1client_1_1Error}]{\sphinxcrossref{\DUrole{std,std-ref}{Error}}}} object indicating success or failure. Success will be returned only if a completed request was returned.. UNAVAILABLE will be returned if ‘wait’ is false and no request is ready. 

\item[{\sphinxstylestrong{Parameters}}] \leavevmode\begin{itemize}
\item {} 
\sphinxcode{\sphinxupquote{async\_request}}: Returns the {\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Request:classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Request}]{\sphinxcrossref{\DUrole{std,std-ref}{Request}}}} object holding the completed request. 

\item {} 
\sphinxcode{\sphinxupquote{wait}}: If true, block until the request completes. Otherwise, return immediately. 

\end{itemize}

\end{description}


\end{fulllineitems}

\subsubsection*{Protected Types}
\index{nvidia::inferenceserver::client::InferContext::AsyncReqMap (C++ type)@\spxentry{nvidia::inferenceserver::client::InferContext::AsyncReqMap}\spxextra{C++ type}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:_CPPv4N6nvidia15inferenceserver6client12InferContext11AsyncReqMapE}}%
\pysigstartmultiline
\pysigline{\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1a6e1d2a28514245af5f71557a18be70c6}}\sphinxbfcode{\sphinxupquote{using }}\sphinxbfcode{\sphinxupquote{AsyncReqMap}} = std::map\textless{}uintptr\_t, std::shared\_ptr\textless{}{\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:_CPPv4N6nvidia15inferenceserver6client12InferContext7RequestE}]{\sphinxcrossref{Request}}}\textgreater{}\textgreater{}}%
\pysigstopmultiline
\end{fulllineitems}

\subsubsection*{Protected Functions}
\index{nvidia::inferenceserver::client::InferContext::InferContext (C++ function)@\spxentry{nvidia::inferenceserver::client::InferContext::InferContext}\spxextra{C++ function}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:_CPPv4N6nvidia15inferenceserver6client12InferContext12InferContextERKNSt6stringE7int64_t13CorrelationIDb}}%
\pysigstartmultiline
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1a1ad0ced372d23d1ed5a0af511ab7aa21}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{InferContext}}}{\sphinxbfcode{\sphinxupquote{const}} std::string\&, int64\_t, CorrelationID, bool}{}%
\pysigstopmultiline
\end{fulllineitems}

\index{nvidia::inferenceserver::client::InferContext::AsyncTransfer (C++ function)@\spxentry{nvidia::inferenceserver::client::InferContext::AsyncTransfer}\spxextra{C++ function}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:_CPPv4N6nvidia15inferenceserver6client12InferContext13AsyncTransferEv}}%
\pysigstartmultiline
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1ae32e8e83bf498b4e91452703dd12cca0}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{virtual}} void \sphinxbfcode{\sphinxupquote{AsyncTransfer}}}{}{ = 0}%
\pysigstopmultiline
\end{fulllineitems}

\index{nvidia::inferenceserver::client::InferContext::PreRunProcessing (C++ function)@\spxentry{nvidia::inferenceserver::client::InferContext::PreRunProcessing}\spxextra{C++ function}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:_CPPv4N6nvidia15inferenceserver6client12InferContext16PreRunProcessingERNSt10shared_ptrI7RequestEE}}%
\pysigstartmultiline
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1ad2c7af868ffbb1a2e24f47d9ae0093a5}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{virtual}} {\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1Error:_CPPv4N6nvidia15inferenceserver6client5ErrorE}]{\sphinxcrossref{Error}}} \sphinxbfcode{\sphinxupquote{PreRunProcessing}}}{std::shared\_ptr\textless{}{\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:_CPPv4N6nvidia15inferenceserver6client12InferContext7RequestE}]{\sphinxcrossref{Request}}}\textgreater{} \&\sphinxstyleemphasis{request}}{ = 0}%
\pysigstopmultiline
\end{fulllineitems}

\index{nvidia::inferenceserver::client::InferContext::IsRequestReady (C++ function)@\spxentry{nvidia::inferenceserver::client::InferContext::IsRequestReady}\spxextra{C++ function}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:_CPPv4N6nvidia15inferenceserver6client12InferContext14IsRequestReadyERKNSt10shared_ptrI7RequestEEb}}%
\pysigstartmultiline
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1ab7437e029e942557e2d11f8c22ae6d6b}}\pysiglinewithargsret{{\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1Error:_CPPv4N6nvidia15inferenceserver6client5ErrorE}]{\sphinxcrossref{Error}}} \sphinxbfcode{\sphinxupquote{IsRequestReady}}}{\sphinxbfcode{\sphinxupquote{const}} std::shared\_ptr\textless{}{\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:_CPPv4N6nvidia15inferenceserver6client12InferContext7RequestE}]{\sphinxcrossref{Request}}}\textgreater{} \&\sphinxstyleemphasis{async\_request}, bool \sphinxstyleemphasis{wait}}{}%
\pysigstopmultiline
\end{fulllineitems}

\index{nvidia::inferenceserver::client::InferContext::UpdateStat (C++ function)@\spxentry{nvidia::inferenceserver::client::InferContext::UpdateStat}\spxextra{C++ function}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:_CPPv4N6nvidia15inferenceserver6client12InferContext10UpdateStatERK13RequestTimers}}%
\pysigstartmultiline
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1a11eff835f49b4d9126e1606a3b2c8615}}\pysiglinewithargsret{{\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1Error:_CPPv4N6nvidia15inferenceserver6client5ErrorE}]{\sphinxcrossref{Error}}} \sphinxbfcode{\sphinxupquote{UpdateStat}}}{\sphinxbfcode{\sphinxupquote{const}} {\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:_CPPv4N6nvidia15inferenceserver6client12InferContext13RequestTimersE}]{\sphinxcrossref{RequestTimers}}} \&\sphinxstyleemphasis{timer}}{}%
\pysigstopmultiline
\end{fulllineitems}

\subsubsection*{Protected Attributes}
\index{nvidia::inferenceserver::client::InferContext::ongoing\_async\_requests\_ (C++ member)@\spxentry{nvidia::inferenceserver::client::InferContext::ongoing\_async\_requests\_}\spxextra{C++ member}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:_CPPv4N6nvidia15inferenceserver6client12InferContext23ongoing_async_requests_E}}%
\pysigstartmultiline
\pysigline{\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1aa2dda0b94a39e54ce2d07a3b5b6988b2}}{\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:_CPPv4N6nvidia15inferenceserver6client12InferContext11AsyncReqMapE}]{\sphinxcrossref{AsyncReqMap}}} \sphinxbfcode{\sphinxupquote{ongoing\_async\_requests\_}}}%
\pysigstopmultiline
\end{fulllineitems}

\index{nvidia::inferenceserver::client::InferContext::model\_name\_ (C++ member)@\spxentry{nvidia::inferenceserver::client::InferContext::model\_name\_}\spxextra{C++ member}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:_CPPv4N6nvidia15inferenceserver6client12InferContext11model_name_E}}%
\pysigstartmultiline
\pysigline{\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1aca65f4b17c14e3c25350c8af3571f86c}}\sphinxbfcode{\sphinxupquote{const}} std::string \sphinxbfcode{\sphinxupquote{model\_name\_}}}%
\pysigstopmultiline
\end{fulllineitems}

\index{nvidia::inferenceserver::client::InferContext::model\_version\_ (C++ member)@\spxentry{nvidia::inferenceserver::client::InferContext::model\_version\_}\spxextra{C++ member}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:_CPPv4N6nvidia15inferenceserver6client12InferContext14model_version_E}}%
\pysigstartmultiline
\pysigline{\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1aea092ac954441e636316644bd94b5979}}\sphinxbfcode{\sphinxupquote{const}} int64\_t \sphinxbfcode{\sphinxupquote{model\_version\_}}}%
\pysigstopmultiline
\end{fulllineitems}

\index{nvidia::inferenceserver::client::InferContext::correlation\_id\_ (C++ member)@\spxentry{nvidia::inferenceserver::client::InferContext::correlation\_id\_}\spxextra{C++ member}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:_CPPv4N6nvidia15inferenceserver6client12InferContext15correlation_id_E}}%
\pysigstartmultiline
\pysigline{\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1a1836d4a88fbfa609def01ee59a675f4c}}\sphinxbfcode{\sphinxupquote{const}} CorrelationID \sphinxbfcode{\sphinxupquote{correlation\_id\_}}}%
\pysigstopmultiline
\end{fulllineitems}

\index{nvidia::inferenceserver::client::InferContext::verbose\_ (C++ member)@\spxentry{nvidia::inferenceserver::client::InferContext::verbose\_}\spxextra{C++ member}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:_CPPv4N6nvidia15inferenceserver6client12InferContext8verbose_E}}%
\pysigstartmultiline
\pysigline{\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1a69b72d3b61b07468f5a13c97f36c5e6a}}\sphinxbfcode{\sphinxupquote{const}} bool \sphinxbfcode{\sphinxupquote{verbose\_}}}%
\pysigstopmultiline
\end{fulllineitems}

\index{nvidia::inferenceserver::client::InferContext::max\_batch\_size\_ (C++ member)@\spxentry{nvidia::inferenceserver::client::InferContext::max\_batch\_size\_}\spxextra{C++ member}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:_CPPv4N6nvidia15inferenceserver6client12InferContext15max_batch_size_E}}%
\pysigstartmultiline
\pysigline{\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1abfc00d0d8175ac030d881365b7030efd}}uint64\_t \sphinxbfcode{\sphinxupquote{max\_batch\_size\_}}}%
\pysigstopmultiline
\end{fulllineitems}

\index{nvidia::inferenceserver::client::InferContext::batch\_size\_ (C++ member)@\spxentry{nvidia::inferenceserver::client::InferContext::batch\_size\_}\spxextra{C++ member}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:_CPPv4N6nvidia15inferenceserver6client12InferContext11batch_size_E}}%
\pysigstartmultiline
\pysigline{\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1a12cc5d7a5db6437fe9fdc4ccc60874a9}}uint64\_t \sphinxbfcode{\sphinxupquote{batch\_size\_}}}%
\pysigstopmultiline
\end{fulllineitems}

\index{nvidia::inferenceserver::client::InferContext::async\_request\_id\_ (C++ member)@\spxentry{nvidia::inferenceserver::client::InferContext::async\_request\_id\_}\spxextra{C++ member}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:_CPPv4N6nvidia15inferenceserver6client12InferContext17async_request_id_E}}%
\pysigstartmultiline
\pysigline{\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1aeb3741fc43d18b9cd7abab193f7ffca0}}uint64\_t \sphinxbfcode{\sphinxupquote{async\_request\_id\_}}}%
\pysigstopmultiline
\end{fulllineitems}

\index{nvidia::inferenceserver::client::InferContext::inputs\_ (C++ member)@\spxentry{nvidia::inferenceserver::client::InferContext::inputs\_}\spxextra{C++ member}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:_CPPv4N6nvidia15inferenceserver6client12InferContext7inputs_E}}%
\pysigstartmultiline
\pysigline{\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1a2959c35ac38d06693ee9e3482f4b344b}}std::vector\textless{}std::shared\_ptr\textless{}{\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:_CPPv4N6nvidia15inferenceserver6client12InferContext5InputE}]{\sphinxcrossref{Input}}}\textgreater{}\textgreater{} \sphinxbfcode{\sphinxupquote{inputs\_}}}%
\pysigstopmultiline
\end{fulllineitems}

\index{nvidia::inferenceserver::client::InferContext::outputs\_ (C++ member)@\spxentry{nvidia::inferenceserver::client::InferContext::outputs\_}\spxextra{C++ member}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:_CPPv4N6nvidia15inferenceserver6client12InferContext8outputs_E}}%
\pysigstartmultiline
\pysigline{\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1a21c9f59353ef1d4215e033f5a4b5d86a}}std::vector\textless{}std::shared\_ptr\textless{}{\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:_CPPv4N6nvidia15inferenceserver6client12InferContext6OutputE}]{\sphinxcrossref{Output}}}\textgreater{}\textgreater{} \sphinxbfcode{\sphinxupquote{outputs\_}}}%
\pysigstopmultiline
\end{fulllineitems}

\index{nvidia::inferenceserver::client::InferContext::infer\_request\_ (C++ member)@\spxentry{nvidia::inferenceserver::client::InferContext::infer\_request\_}\spxextra{C++ member}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:_CPPv4N6nvidia15inferenceserver6client12InferContext14infer_request_E}}%
\pysigstartmultiline
\pysigline{\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1ad65a99fa3c6afcf961de531f4421a73d}}InferRequestHeader \sphinxbfcode{\sphinxupquote{infer\_request\_}}}%
\pysigstopmultiline
\end{fulllineitems}

\index{nvidia::inferenceserver::client::InferContext::sync\_request\_ (C++ member)@\spxentry{nvidia::inferenceserver::client::InferContext::sync\_request\_}\spxextra{C++ member}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:_CPPv4N6nvidia15inferenceserver6client12InferContext13sync_request_E}}%
\pysigstartmultiline
\pysigline{\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1a6b2d7784bcf85ee225b574f73d6059ca}}std::shared\_ptr\textless{}{\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:_CPPv4N6nvidia15inferenceserver6client12InferContext7RequestE}]{\sphinxcrossref{Request}}}\textgreater{} \sphinxbfcode{\sphinxupquote{sync\_request\_}}}%
\pysigstopmultiline
\end{fulllineitems}

\index{nvidia::inferenceserver::client::InferContext::context\_stat\_ (C++ member)@\spxentry{nvidia::inferenceserver::client::InferContext::context\_stat\_}\spxextra{C++ member}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:_CPPv4N6nvidia15inferenceserver6client12InferContext13context_stat_E}}%
\pysigstartmultiline
\pysigline{\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1a5fde4cd588ca640d2b17e1768406eb1c}}{\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:_CPPv4N6nvidia15inferenceserver6client12InferContext4StatE}]{\sphinxcrossref{Stat}}} \sphinxbfcode{\sphinxupquote{context\_stat\_}}}%
\pysigstopmultiline
\end{fulllineitems}

\index{nvidia::inferenceserver::client::InferContext::worker\_ (C++ member)@\spxentry{nvidia::inferenceserver::client::InferContext::worker\_}\spxextra{C++ member}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:_CPPv4N6nvidia15inferenceserver6client12InferContext7worker_E}}%
\pysigstartmultiline
\pysigline{\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1a39fdbf2a953b23d0005aef8e2618fcaf}}std::thread \sphinxbfcode{\sphinxupquote{worker\_}}}%
\pysigstopmultiline
\end{fulllineitems}

\index{nvidia::inferenceserver::client::InferContext::mutex\_ (C++ member)@\spxentry{nvidia::inferenceserver::client::InferContext::mutex\_}\spxextra{C++ member}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:_CPPv4N6nvidia15inferenceserver6client12InferContext6mutex_E}}%
\pysigstartmultiline
\pysigline{\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1a1baafc5376df1a6e16de58633c93b4cc}}std::mutex \sphinxbfcode{\sphinxupquote{mutex\_}}}%
\pysigstopmultiline
\end{fulllineitems}

\index{nvidia::inferenceserver::client::InferContext::cv\_ (C++ member)@\spxentry{nvidia::inferenceserver::client::InferContext::cv\_}\spxextra{C++ member}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:_CPPv4N6nvidia15inferenceserver6client12InferContext3cv_E}}%
\pysigstartmultiline
\pysigline{\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1a685dfb2b9634c050b2d552f1613d0bc0}}std::condition\_variable \sphinxbfcode{\sphinxupquote{cv\_}}}%
\pysigstopmultiline
\end{fulllineitems}

\index{nvidia::inferenceserver::client::InferContext::exiting\_ (C++ member)@\spxentry{nvidia::inferenceserver::client::InferContext::exiting\_}\spxextra{C++ member}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:_CPPv4N6nvidia15inferenceserver6client12InferContext8exiting_E}}%
\pysigstartmultiline
\pysigline{\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1a5f6d75086a6155fba595351c4e7abdd6}}bool \sphinxbfcode{\sphinxupquote{exiting\_}}}%
\pysigstopmultiline
\end{fulllineitems}

\index{nvidia::inferenceserver::client::InferContext::Input (C++ class)@\spxentry{nvidia::inferenceserver::client::InferContext::Input}\spxextra{C++ class}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:_CPPv4N6nvidia15inferenceserver6client12InferContext5InputE}}%
\pysigstartmultiline
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Input}}\pysigline{\sphinxbfcode{\sphinxupquote{class }}\sphinxbfcode{\sphinxupquote{Input}}}%
\pysigstopmultiline
An input to the model. 
\subsubsection*{Public Functions}
\index{nvidia::inferenceserver::client::InferContext::Input::\textasciitilde{}Input (C++ function)@\spxentry{nvidia::inferenceserver::client::InferContext::Input::\textasciitilde{}Input}\spxextra{C++ function}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:_CPPv4N6nvidia15inferenceserver6client12InferContext5InputD0Ev}}%
\pysigstartmultiline
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Input_1aeb32510fa7e86063d3341302ff5cc322}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{virtual}} \sphinxbfcode{\sphinxupquote{\textasciitilde{}Input}}}{}{}%
\pysigstopmultiline
Destroy the input. 

\end{fulllineitems}

\index{nvidia::inferenceserver::client::InferContext::Input::Name (C++ function)@\spxentry{nvidia::inferenceserver::client::InferContext::Input::Name}\spxextra{C++ function}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:_CPPv4NK6nvidia15inferenceserver6client12InferContext5Input4NameEv}}%
\pysigstartmultiline
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Input_1a66b20a9167b8eeb9ddde10430b37fd01}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{virtual}} \sphinxbfcode{\sphinxupquote{const}} std::string \&\sphinxbfcode{\sphinxupquote{Name}}}{}{ \sphinxbfcode{\sphinxupquote{const}} = 0}%
\pysigstopmultiline
\begin{description}
\item[{\sphinxstylestrong{Return}}] \leavevmode
The name of the input. 

\end{description}


\end{fulllineitems}

\index{nvidia::inferenceserver::client::InferContext::Input::ByteSize (C++ function)@\spxentry{nvidia::inferenceserver::client::InferContext::Input::ByteSize}\spxextra{C++ function}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:_CPPv4NK6nvidia15inferenceserver6client12InferContext5Input8ByteSizeEv}}%
\pysigstartmultiline
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Input_1a34951a4f1c906bf3c147ee84549d8667}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{virtual}} int64\_t \sphinxbfcode{\sphinxupquote{ByteSize}}}{}{ \sphinxbfcode{\sphinxupquote{const}} = 0}%
\pysigstopmultiline
\begin{description}
\item[{\sphinxstylestrong{Return}}] \leavevmode
The size in bytes of this input. This is the size for one instance of the input, not the entire size of a batched input. When the byte-size is not known, for example for non-fixed-sized types like TYPE\_STRING or for inputs with variable-size dimensions, this will return -1. 

\end{description}


\end{fulllineitems}

\index{nvidia::inferenceserver::client::InferContext::Input::TotalByteSize (C++ function)@\spxentry{nvidia::inferenceserver::client::InferContext::Input::TotalByteSize}\spxextra{C++ function}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:_CPPv4NK6nvidia15inferenceserver6client12InferContext5Input13TotalByteSizeEv}}%
\pysigstartmultiline
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Input_1a07c715257ed5ed2feab0982d9c6fd941}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{virtual}} size\_t \sphinxbfcode{\sphinxupquote{TotalByteSize}}}{}{ \sphinxbfcode{\sphinxupquote{const}} = 0}%
\pysigstopmultiline
\begin{description}
\item[{\sphinxstylestrong{Return}}] \leavevmode
The size in bytes of entire batch of this input. For fixed-sized types this is just {\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Input:classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Input_1a34951a4f1c906bf3c147ee84549d8667}]{\sphinxcrossref{\DUrole{std,std-ref}{ByteSize()}}}} * batch-size, but for non-fixed-sized types like TYPE\_STRING it is the only way to get the entire input size. 

\end{description}


\end{fulllineitems}

\index{nvidia::inferenceserver::client::InferContext::Input::DType (C++ function)@\spxentry{nvidia::inferenceserver::client::InferContext::Input::DType}\spxextra{C++ function}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:_CPPv4NK6nvidia15inferenceserver6client12InferContext5Input5DTypeEv}}%
\pysigstartmultiline
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Input_1ab90380c6a46ea973a08f86c5a1bb3ea8}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{virtual}} DataType \sphinxbfcode{\sphinxupquote{DType}}}{}{ \sphinxbfcode{\sphinxupquote{const}} = 0}%
\pysigstopmultiline
\begin{description}
\item[{\sphinxstylestrong{Return}}] \leavevmode
The data-type of the input. 

\end{description}


\end{fulllineitems}

\index{nvidia::inferenceserver::client::InferContext::Input::Format (C++ function)@\spxentry{nvidia::inferenceserver::client::InferContext::Input::Format}\spxextra{C++ function}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:_CPPv4NK6nvidia15inferenceserver6client12InferContext5Input6FormatEv}}%
\pysigstartmultiline
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Input_1a9ab3e04fbb0ad181fd74bf7675249fef}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{virtual}} ModelInput::Format \sphinxbfcode{\sphinxupquote{Format}}}{}{ \sphinxbfcode{\sphinxupquote{const}} = 0}%
\pysigstopmultiline
\begin{description}
\item[{\sphinxstylestrong{Return}}] \leavevmode
The format of the input. 

\end{description}


\end{fulllineitems}

\index{nvidia::inferenceserver::client::InferContext::Input::Dims (C++ function)@\spxentry{nvidia::inferenceserver::client::InferContext::Input::Dims}\spxextra{C++ function}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:_CPPv4NK6nvidia15inferenceserver6client12InferContext5Input4DimsEv}}%
\pysigstartmultiline
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Input_1a500ee9536afa9d6286cd23ee1308ebda}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{virtual}} \sphinxbfcode{\sphinxupquote{const}} DimsList \&\sphinxbfcode{\sphinxupquote{Dims}}}{}{ \sphinxbfcode{\sphinxupquote{const}} = 0}%
\pysigstopmultiline
\begin{description}
\item[{\sphinxstylestrong{Return}}] \leavevmode
The dimensions/shape of the input specified in the model configuration. Variable-size dimensions are reported as -1. 

\end{description}


\end{fulllineitems}

\index{nvidia::inferenceserver::client::InferContext::Input::Reset (C++ function)@\spxentry{nvidia::inferenceserver::client::InferContext::Input::Reset}\spxextra{C++ function}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:_CPPv4N6nvidia15inferenceserver6client12InferContext5Input5ResetEv}}%
\pysigstartmultiline
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Input_1a3dee261f7bed6cb3840db299c6b1d246}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{virtual}} {\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1Error:_CPPv4N6nvidia15inferenceserver6client5ErrorE}]{\sphinxcrossref{Error}}} \sphinxbfcode{\sphinxupquote{Reset}}}{}{ = 0}%
\pysigstopmultiline
Prepare this input to receive new tensor values. 

Forget any existing values that were set by previous calls to {\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Input:classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Input_1a414fe9afbb60a48f162bac5439beee4d}]{\sphinxcrossref{\DUrole{std,std-ref}{SetRaw()}}}}. \begin{description}
\item[{\sphinxstylestrong{Return}}] \leavevmode
{\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1Error:classnvidia_1_1inferenceserver_1_1client_1_1Error}]{\sphinxcrossref{\DUrole{std,std-ref}{Error}}}} object indicating success or failure. 

\end{description}


\end{fulllineitems}

\index{nvidia::inferenceserver::client::InferContext::Input::Shape (C++ function)@\spxentry{nvidia::inferenceserver::client::InferContext::Input::Shape}\spxextra{C++ function}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:_CPPv4NK6nvidia15inferenceserver6client12InferContext5Input5ShapeEv}}%
\pysigstartmultiline
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Input_1aedd9cacfe4e87008521ed63c1eb8e035}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{virtual}} \sphinxbfcode{\sphinxupquote{const}} std::vector\textless{}int64\_t\textgreater{} \&\sphinxbfcode{\sphinxupquote{Shape}}}{}{ \sphinxbfcode{\sphinxupquote{const}} = 0}%
\pysigstopmultiline
Get the shape for this input that was most recently set by SetShape. 

\begin{description}
\item[{\sphinxstylestrong{Return}}] \leavevmode
The shape, or empty vector if SetShape has not been called. 

\end{description}


\end{fulllineitems}

\index{nvidia::inferenceserver::client::InferContext::Input::SetShape (C++ function)@\spxentry{nvidia::inferenceserver::client::InferContext::Input::SetShape}\spxextra{C++ function}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:_CPPv4N6nvidia15inferenceserver6client12InferContext5Input8SetShapeERKNSt6vectorI7int64_tEE}}%
\pysigstartmultiline
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Input_1a0e2c4e3813ebfda6ecef7e23a7f00de5}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{virtual}} {\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1Error:_CPPv4N6nvidia15inferenceserver6client5ErrorE}]{\sphinxcrossref{Error}}} \sphinxbfcode{\sphinxupquote{SetShape}}}{\sphinxbfcode{\sphinxupquote{const}} std::vector\textless{}int64\_t\textgreater{} \&\sphinxstyleemphasis{dims}}{ = 0}%
\pysigstopmultiline
Set the shape for this input. 

The shape must be set for inputs that have variable-size dimensions and is optional for other inputs. The shape must be set before calling SetRaw or SetFromString. \begin{description}
\item[{\sphinxstylestrong{Return}}] \leavevmode
{\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1Error:classnvidia_1_1inferenceserver_1_1client_1_1Error}]{\sphinxcrossref{\DUrole{std,std-ref}{Error}}}} object indicating success or failure. 

\item[{\sphinxstylestrong{Parameters}}] \leavevmode\begin{itemize}
\item {} 
\sphinxcode{\sphinxupquote{dims}}: The dimensions of the shape. 

\end{itemize}

\end{description}


\end{fulllineitems}

\index{nvidia::inferenceserver::client::InferContext::Input::SetRaw (C++ function)@\spxentry{nvidia::inferenceserver::client::InferContext::Input::SetRaw}\spxextra{C++ function}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:_CPPv4N6nvidia15inferenceserver6client12InferContext5Input6SetRawEPK7uint8_t6size_t}}%
\pysigstartmultiline
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Input_1a414fe9afbb60a48f162bac5439beee4d}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{virtual}} {\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1Error:_CPPv4N6nvidia15inferenceserver6client5ErrorE}]{\sphinxcrossref{Error}}} \sphinxbfcode{\sphinxupquote{SetRaw}}}{\sphinxbfcode{\sphinxupquote{const}} uint8\_t *\sphinxstyleemphasis{input}, size\_t \sphinxstyleemphasis{input\_byte\_size}}{ = 0}%
\pysigstopmultiline
Set tensor values for this input from a byte array. 

The array is not copied and so it must not be modified or destroyed until this input is no longer needed (that is until the {\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1a4cac8002817c9259851305b14c258f36}]{\sphinxcrossref{\DUrole{std,std-ref}{Run()}}}} call(s) that use the input have completed). For batched inputs this function must be called batch-size times to provide all tensor values for a batch of this input. \begin{description}
\item[{\sphinxstylestrong{Return}}] \leavevmode
{\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1Error:classnvidia_1_1inferenceserver_1_1client_1_1Error}]{\sphinxcrossref{\DUrole{std,std-ref}{Error}}}} object indicating success or failure. 

\item[{\sphinxstylestrong{Parameters}}] \leavevmode\begin{itemize}
\item {} 
\sphinxcode{\sphinxupquote{input}}: The pointer to the array holding the tensor value. 

\item {} 
\sphinxcode{\sphinxupquote{input\_byte\_size}}: The size of the array in bytes, must match the size expected by the input. 

\end{itemize}

\end{description}


\end{fulllineitems}

\index{nvidia::inferenceserver::client::InferContext::Input::SetRaw (C++ function)@\spxentry{nvidia::inferenceserver::client::InferContext::Input::SetRaw}\spxextra{C++ function}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:_CPPv4N6nvidia15inferenceserver6client12InferContext5Input6SetRawERKNSt6vectorI7uint8_tEE}}%
\pysigstartmultiline
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Input_1a094ec9eece1e10d877db9af69867090e}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{virtual}} {\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1Error:_CPPv4N6nvidia15inferenceserver6client5ErrorE}]{\sphinxcrossref{Error}}} \sphinxbfcode{\sphinxupquote{SetRaw}}}{\sphinxbfcode{\sphinxupquote{const}} std::vector\textless{}uint8\_t\textgreater{} \&\sphinxstyleemphasis{input}}{ = 0}%
\pysigstopmultiline
Set tensor values for this input from a byte vector. 

The vector is not copied and so it must not be modified or destroyed until this input is no longer needed (that is until the {\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1a4cac8002817c9259851305b14c258f36}]{\sphinxcrossref{\DUrole{std,std-ref}{Run()}}}} call(s) that use the input have completed). For batched inputs this function must be called batch-size times to provide all tensor values for a batch of this input. \begin{description}
\item[{\sphinxstylestrong{Return}}] \leavevmode
{\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1Error:classnvidia_1_1inferenceserver_1_1client_1_1Error}]{\sphinxcrossref{\DUrole{std,std-ref}{Error}}}} object indicating success or failure. 

\item[{\sphinxstylestrong{Parameters}}] \leavevmode\begin{itemize}
\item {} 
\sphinxcode{\sphinxupquote{input}}: The vector holding tensor values. 

\end{itemize}

\end{description}


\end{fulllineitems}

\index{nvidia::inferenceserver::client::InferContext::Input::SetFromString (C++ function)@\spxentry{nvidia::inferenceserver::client::InferContext::Input::SetFromString}\spxextra{C++ function}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:_CPPv4N6nvidia15inferenceserver6client12InferContext5Input13SetFromStringERKNSt6vectorINSt6stringEEE}}%
\pysigstartmultiline
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Input_1a73495802b757c43b67edd334fec341d7}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{virtual}} {\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1Error:_CPPv4N6nvidia15inferenceserver6client5ErrorE}]{\sphinxcrossref{Error}}} \sphinxbfcode{\sphinxupquote{SetFromString}}}{\sphinxbfcode{\sphinxupquote{const}} std::vector\textless{}std::string\textgreater{} \&\sphinxstyleemphasis{input}}{ = 0}%
\pysigstopmultiline
Set tensor values for this input from a vector or strings. 

This method can only be used for tensors with STRING data-type. The strings are assigned in row-major order to the elements of the tensor. The strings are copied and so the ‘input’ does not need to be preserved as with {\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Input:classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Input_1a414fe9afbb60a48f162bac5439beee4d}]{\sphinxcrossref{\DUrole{std,std-ref}{SetRaw()}}}}. For batched inputs this function must be called batch-size times to provide all tensor values for a batch of this input. \begin{description}
\item[{\sphinxstylestrong{Return}}] \leavevmode
{\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1Error:classnvidia_1_1inferenceserver_1_1client_1_1Error}]{\sphinxcrossref{\DUrole{std,std-ref}{Error}}}} object indicating success or failure. 

\item[{\sphinxstylestrong{Parameters}}] \leavevmode\begin{itemize}
\item {} 
\sphinxcode{\sphinxupquote{input}}: The vector holding tensor string values. 

\end{itemize}

\end{description}


\end{fulllineitems}


\end{fulllineitems}

\index{nvidia::inferenceserver::client::InferContext::Options (C++ class)@\spxentry{nvidia::inferenceserver::client::InferContext::Options}\spxextra{C++ class}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:_CPPv4N6nvidia15inferenceserver6client12InferContext7OptionsE}}%
\pysigstartmultiline
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Options}}\pysigline{\sphinxbfcode{\sphinxupquote{class }}\sphinxbfcode{\sphinxupquote{Options}}}%
\pysigstopmultiline
Run options to be applied to all subsequent {\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1a4cac8002817c9259851305b14c258f36}]{\sphinxcrossref{\DUrole{std,std-ref}{Run()}}}} invocations. 
\subsubsection*{Public Functions}
\index{nvidia::inferenceserver::client::InferContext::Options::\textasciitilde{}Options (C++ function)@\spxentry{nvidia::inferenceserver::client::InferContext::Options::\textasciitilde{}Options}\spxextra{C++ function}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:_CPPv4N6nvidia15inferenceserver6client12InferContext7OptionsD0Ev}}%
\pysigstartmultiline
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Options_1abcaa16202fc4751a7657808cc4d71884}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{virtual}} \sphinxbfcode{\sphinxupquote{\textasciitilde{}Options}}}{}{}%
\pysigstopmultiline
\end{fulllineitems}

\index{nvidia::inferenceserver::client::InferContext::Options::Flag (C++ function)@\spxentry{nvidia::inferenceserver::client::InferContext::Options::Flag}\spxextra{C++ function}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:_CPPv4NK6nvidia15inferenceserver6client12InferContext7Options4FlagEN18InferRequestHeader4FlagE}}%
\pysigstartmultiline
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Options_1a23b1ac6a7393c1bad21fc8e7a3373ead}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{virtual}} bool \sphinxbfcode{\sphinxupquote{Flag}}}{InferRequestHeader::Flag \sphinxstyleemphasis{flag}}{ \sphinxbfcode{\sphinxupquote{const}} = 0}%
\pysigstopmultiline
Get the value of a request flag being used for all subsequent inferences. 

Cannot be used with FLAG\_NONE. \begin{description}
\item[{\sphinxstylestrong{Return}}] \leavevmode
The true/false value currently set for the flag. If ‘flag’ is FLAG\_NONE then return false. 

\item[{\sphinxstylestrong{Parameters}}] \leavevmode\begin{itemize}
\item {} 
\sphinxcode{\sphinxupquote{flag}}: The flag to get the value for. 

\end{itemize}

\end{description}


\end{fulllineitems}

\index{nvidia::inferenceserver::client::InferContext::Options::SetFlag (C++ function)@\spxentry{nvidia::inferenceserver::client::InferContext::Options::SetFlag}\spxextra{C++ function}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:_CPPv4N6nvidia15inferenceserver6client12InferContext7Options7SetFlagEN18InferRequestHeader4FlagEb}}%
\pysigstartmultiline
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Options_1ae12fc02abb1e1f42d1de0e0d4f88c03d}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{virtual}} void \sphinxbfcode{\sphinxupquote{SetFlag}}}{InferRequestHeader::Flag \sphinxstyleemphasis{flag}, bool \sphinxstyleemphasis{value}}{ = 0}%
\pysigstopmultiline
Set a request flag to be used for all subsequent inferences. 

\begin{description}
\item[{\sphinxstylestrong{Parameters}}] \leavevmode\begin{itemize}
\item {} 
\sphinxcode{\sphinxupquote{flag}}: The flag to set. Cannot be used with FLAG\_NONE. 

\item {} 
\sphinxcode{\sphinxupquote{value}}: The true/false value to set for the flag. If ‘flag’ is FLAG\_NONE then do nothing. 

\end{itemize}

\end{description}


\end{fulllineitems}

\index{nvidia::inferenceserver::client::InferContext::Options::Flags (C++ function)@\spxentry{nvidia::inferenceserver::client::InferContext::Options::Flags}\spxextra{C++ function}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:_CPPv4NK6nvidia15inferenceserver6client12InferContext7Options5FlagsEv}}%
\pysigstartmultiline
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Options_1a51f46889d0f436185379def5a99b67fc}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{virtual}} uint32\_t \sphinxbfcode{\sphinxupquote{Flags}}}{}{ \sphinxbfcode{\sphinxupquote{const}} = 0}%
\pysigstopmultiline
Get the value of all request flags being used for all subsequent inferences. 

\begin{description}
\item[{\sphinxstylestrong{Return}}] \leavevmode
The bitwise-or of flag values as a single uint32\_t value. 

\end{description}


\end{fulllineitems}

\index{nvidia::inferenceserver::client::InferContext::Options::SetFlags (C++ function)@\spxentry{nvidia::inferenceserver::client::InferContext::Options::SetFlags}\spxextra{C++ function}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:_CPPv4N6nvidia15inferenceserver6client12InferContext7Options8SetFlagsE8uint32_t}}%
\pysigstartmultiline
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Options_1a06defa0f160399a6e81d31bde89cbcc0}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{virtual}} void \sphinxbfcode{\sphinxupquote{SetFlags}}}{uint32\_t \sphinxstyleemphasis{flags}}{ = 0}%
\pysigstopmultiline
Set all request flags to be used for all subsequent inferences. 

\begin{description}
\item[{\sphinxstylestrong{Parameters}}] \leavevmode\begin{itemize}
\item {} 
\sphinxcode{\sphinxupquote{flags}}: The bitwise-or of flag values to set. 

\end{itemize}

\end{description}


\end{fulllineitems}

\index{nvidia::inferenceserver::client::InferContext::Options::BatchSize (C++ function)@\spxentry{nvidia::inferenceserver::client::InferContext::Options::BatchSize}\spxextra{C++ function}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:_CPPv4NK6nvidia15inferenceserver6client12InferContext7Options9BatchSizeEv}}%
\pysigstartmultiline
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Options_1a6546eb56589394cc4222272afcc23d89}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{virtual}} size\_t \sphinxbfcode{\sphinxupquote{BatchSize}}}{}{ \sphinxbfcode{\sphinxupquote{const}} = 0}%
\pysigstopmultiline
\begin{description}
\item[{\sphinxstylestrong{Return}}] \leavevmode
The batch size to use for all subsequent inferences. 

\end{description}


\end{fulllineitems}

\index{nvidia::inferenceserver::client::InferContext::Options::SetBatchSize (C++ function)@\spxentry{nvidia::inferenceserver::client::InferContext::Options::SetBatchSize}\spxextra{C++ function}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:_CPPv4N6nvidia15inferenceserver6client12InferContext7Options12SetBatchSizeE6size_t}}%
\pysigstartmultiline
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Options_1a6f390b31a0a8a265c8e15a77f6f9e7c1}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{virtual}} void \sphinxbfcode{\sphinxupquote{SetBatchSize}}}{size\_t \sphinxstyleemphasis{batch\_size}}{ = 0}%
\pysigstopmultiline
Set the batch size to use for all subsequent inferences. 

\begin{description}
\item[{\sphinxstylestrong{Parameters}}] \leavevmode\begin{itemize}
\item {} 
\sphinxcode{\sphinxupquote{batch\_size}}: The batch size. 

\end{itemize}

\end{description}


\end{fulllineitems}

\index{nvidia::inferenceserver::client::InferContext::Options::AddRawResult (C++ function)@\spxentry{nvidia::inferenceserver::client::InferContext::Options::AddRawResult}\spxextra{C++ function}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:_CPPv4N6nvidia15inferenceserver6client12InferContext7Options12AddRawResultERKNSt10shared_ptrIN12InferContext6OutputEEE}}%
\pysigstartmultiline
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Options_1abb0dceb3499c833684e0dfbe608b3360}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{virtual}} {\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1Error:_CPPv4N6nvidia15inferenceserver6client5ErrorE}]{\sphinxcrossref{Error}}} \sphinxbfcode{\sphinxupquote{AddRawResult}}}{\sphinxbfcode{\sphinxupquote{const}} std::shared\_ptr\textless{}{\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:_CPPv4N6nvidia15inferenceserver6client12InferContextE}]{\sphinxcrossref{InferContext}}}::{\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:_CPPv4N6nvidia15inferenceserver6client12InferContext6OutputE}]{\sphinxcrossref{Output}}}\textgreater{} \&\sphinxstyleemphasis{output}}{ = 0}%
\pysigstopmultiline
Add ‘output’ to the list of requested RAW results. 

{\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1a4cac8002817c9259851305b14c258f36}]{\sphinxcrossref{\DUrole{std,std-ref}{Run()}}}} will return the output’s full tensor as a result. \begin{description}
\item[{\sphinxstylestrong{Return}}] \leavevmode
{\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1Error:classnvidia_1_1inferenceserver_1_1client_1_1Error}]{\sphinxcrossref{\DUrole{std,std-ref}{Error}}}} object indicating success or failure. 

\item[{\sphinxstylestrong{Parameters}}] \leavevmode\begin{itemize}
\item {} 
\sphinxcode{\sphinxupquote{output}}: The output. 

\end{itemize}

\end{description}


\end{fulllineitems}

\index{nvidia::inferenceserver::client::InferContext::Options::AddClassResult (C++ function)@\spxentry{nvidia::inferenceserver::client::InferContext::Options::AddClassResult}\spxextra{C++ function}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:_CPPv4N6nvidia15inferenceserver6client12InferContext7Options14AddClassResultERKNSt10shared_ptrIN12InferContext6OutputEEE8uint64_t}}%
\pysigstartmultiline
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Options_1a5b584e6fa3d2867d1ec4ce7a0bfee809}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{virtual}} {\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1Error:_CPPv4N6nvidia15inferenceserver6client5ErrorE}]{\sphinxcrossref{Error}}} \sphinxbfcode{\sphinxupquote{AddClassResult}}}{\sphinxbfcode{\sphinxupquote{const}} std::shared\_ptr\textless{}{\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:_CPPv4N6nvidia15inferenceserver6client12InferContextE}]{\sphinxcrossref{InferContext}}}::{\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:_CPPv4N6nvidia15inferenceserver6client12InferContext6OutputE}]{\sphinxcrossref{Output}}}\textgreater{} \&\sphinxstyleemphasis{output}, uint64\_t \sphinxstyleemphasis{k}}{ = 0}%
\pysigstopmultiline
Add ‘output’ to the list of requested CLASS results. 

{\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1a4cac8002817c9259851305b14c258f36}]{\sphinxcrossref{\DUrole{std,std-ref}{Run()}}}} will return the highest ‘k’ values of ‘output’ as a result. \begin{description}
\item[{\sphinxstylestrong{Return}}] \leavevmode
{\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1Error:classnvidia_1_1inferenceserver_1_1client_1_1Error}]{\sphinxcrossref{\DUrole{std,std-ref}{Error}}}} object indicating success or failure. 

\item[{\sphinxstylestrong{Parameters}}] \leavevmode\begin{itemize}
\item {} 
\sphinxcode{\sphinxupquote{output}}: The output. 

\item {} 
\sphinxcode{\sphinxupquote{k}}: Set how many class results to return for the output. 

\end{itemize}

\end{description}


\end{fulllineitems}

\subsubsection*{Public Static Functions}
\index{nvidia::inferenceserver::client::InferContext::Options::Create (C++ function)@\spxentry{nvidia::inferenceserver::client::InferContext::Options::Create}\spxextra{C++ function}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:_CPPv4N6nvidia15inferenceserver6client12InferContext7Options6CreateEPNSt10unique_ptrI7OptionsEE}}%
\pysigstartmultiline
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Options_1a63d97ebc1993c0034ea6822c1d7600b1}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{static}} {\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1Error:_CPPv4N6nvidia15inferenceserver6client5ErrorE}]{\sphinxcrossref{Error}}} \sphinxbfcode{\sphinxupquote{Create}}}{std::unique\_ptr\textless{}{\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:_CPPv4N6nvidia15inferenceserver6client12InferContext7OptionsE}]{\sphinxcrossref{Options}}}\textgreater{} *\sphinxstyleemphasis{options}}{}%
\pysigstopmultiline
Create a new {\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Options:classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Options}]{\sphinxcrossref{\DUrole{std,std-ref}{Options}}}} object with default values. 

\begin{description}
\item[{\sphinxstylestrong{Return}}] \leavevmode
{\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1Error:classnvidia_1_1inferenceserver_1_1client_1_1Error}]{\sphinxcrossref{\DUrole{std,std-ref}{Error}}}} object indicating success or failure. 

\end{description}


\end{fulllineitems}


\end{fulllineitems}

\index{nvidia::inferenceserver::client::InferContext::Output (C++ class)@\spxentry{nvidia::inferenceserver::client::InferContext::Output}\spxextra{C++ class}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:_CPPv4N6nvidia15inferenceserver6client12InferContext6OutputE}}%
\pysigstartmultiline
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Output}}\pysigline{\sphinxbfcode{\sphinxupquote{class }}\sphinxbfcode{\sphinxupquote{Output}}}%
\pysigstopmultiline
An output from the model. 
\subsubsection*{Public Functions}
\index{nvidia::inferenceserver::client::InferContext::Output::\textasciitilde{}Output (C++ function)@\spxentry{nvidia::inferenceserver::client::InferContext::Output::\textasciitilde{}Output}\spxextra{C++ function}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:_CPPv4N6nvidia15inferenceserver6client12InferContext6OutputD0Ev}}%
\pysigstartmultiline
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Output_1a8a685059d16e4a77fb8692dd82de0ba0}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{virtual}} \sphinxbfcode{\sphinxupquote{\textasciitilde{}Output}}}{}{}%
\pysigstopmultiline
Destroy the output. 

\end{fulllineitems}

\index{nvidia::inferenceserver::client::InferContext::Output::Name (C++ function)@\spxentry{nvidia::inferenceserver::client::InferContext::Output::Name}\spxextra{C++ function}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:_CPPv4NK6nvidia15inferenceserver6client12InferContext6Output4NameEv}}%
\pysigstartmultiline
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Output_1a72fd5ea270c62500d6c182c4f2a035ab}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{virtual}} \sphinxbfcode{\sphinxupquote{const}} std::string \&\sphinxbfcode{\sphinxupquote{Name}}}{}{ \sphinxbfcode{\sphinxupquote{const}} = 0}%
\pysigstopmultiline
\begin{description}
\item[{\sphinxstylestrong{Return}}] \leavevmode
The name of the output. 

\end{description}


\end{fulllineitems}

\index{nvidia::inferenceserver::client::InferContext::Output::DType (C++ function)@\spxentry{nvidia::inferenceserver::client::InferContext::Output::DType}\spxextra{C++ function}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:_CPPv4NK6nvidia15inferenceserver6client12InferContext6Output5DTypeEv}}%
\pysigstartmultiline
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Output_1a66c02cfce60941e575c6583ad160896c}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{virtual}} DataType \sphinxbfcode{\sphinxupquote{DType}}}{}{ \sphinxbfcode{\sphinxupquote{const}} = 0}%
\pysigstopmultiline
\begin{description}
\item[{\sphinxstylestrong{Return}}] \leavevmode
The data-type of the output. 

\end{description}


\end{fulllineitems}

\index{nvidia::inferenceserver::client::InferContext::Output::Dims (C++ function)@\spxentry{nvidia::inferenceserver::client::InferContext::Output::Dims}\spxextra{C++ function}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:_CPPv4NK6nvidia15inferenceserver6client12InferContext6Output4DimsEv}}%
\pysigstartmultiline
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Output_1a21f73c9cf17d65831cdbecf904993d05}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{virtual}} \sphinxbfcode{\sphinxupquote{const}} DimsList \&\sphinxbfcode{\sphinxupquote{Dims}}}{}{ \sphinxbfcode{\sphinxupquote{const}} = 0}%
\pysigstopmultiline
\begin{description}
\item[{\sphinxstylestrong{Return}}] \leavevmode
The dimensions/shape of the output specified in the model configuration. Variable-size dimensions are reported as -1. 

\end{description}


\end{fulllineitems}


\end{fulllineitems}

\index{nvidia::inferenceserver::client::InferContext::Request (C++ class)@\spxentry{nvidia::inferenceserver::client::InferContext::Request}\spxextra{C++ class}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:_CPPv4N6nvidia15inferenceserver6client12InferContext7RequestE}}%
\pysigstartmultiline
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Request}}\pysigline{\sphinxbfcode{\sphinxupquote{class }}\sphinxbfcode{\sphinxupquote{Request}}}%
\pysigstopmultiline
Handle to a inference request. 

The request handle is used to get request results if the request is sent by {\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1a255a59df29a53d6e45ce86663887615f}]{\sphinxcrossref{\DUrole{std,std-ref}{AsyncRun()}}}}. 
\subsubsection*{Public Functions}
\index{nvidia::inferenceserver::client::InferContext::Request::\textasciitilde{}Request (C++ function)@\spxentry{nvidia::inferenceserver::client::InferContext::Request::\textasciitilde{}Request}\spxextra{C++ function}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:_CPPv4N6nvidia15inferenceserver6client12InferContext7RequestD0Ev}}%
\pysigstartmultiline
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Request_1a3146daca9aed389d56fcbd71123f7206}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{virtual}} \sphinxbfcode{\sphinxupquote{\textasciitilde{}Request}}}{}{}%
\pysigstopmultiline
Destroy the request handle. 

\end{fulllineitems}

\index{nvidia::inferenceserver::client::InferContext::Request::Id (C++ function)@\spxentry{nvidia::inferenceserver::client::InferContext::Request::Id}\spxextra{C++ function}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:_CPPv4NK6nvidia15inferenceserver6client12InferContext7Request2IdEv}}%
\pysigstartmultiline
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Request_1a796b6c3cac55f00c822f3c1ced007509}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{virtual}} uint64\_t \sphinxbfcode{\sphinxupquote{Id}}}{}{ \sphinxbfcode{\sphinxupquote{const}} = 0}%
\pysigstopmultiline
\begin{description}
\item[{\sphinxstylestrong{Return}}] \leavevmode
The unique identifier of the request. 

\end{description}


\end{fulllineitems}


\end{fulllineitems}

\index{nvidia::inferenceserver::client::InferContext::RequestTimers (C++ class)@\spxentry{nvidia::inferenceserver::client::InferContext::RequestTimers}\spxextra{C++ class}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:_CPPv4N6nvidia15inferenceserver6client12InferContext13RequestTimersE}}%
\pysigstartmultiline
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1RequestTimers}}\pysigline{\sphinxbfcode{\sphinxupquote{class }}\sphinxbfcode{\sphinxupquote{RequestTimers}}}%
\pysigstopmultiline
Timer to record the timestamp for different stages of request handling. 
\subsubsection*{Public Types}
\index{nvidia::inferenceserver::client::InferContext::Kind (C++ type)@\spxentry{nvidia::inferenceserver::client::InferContext::Kind}\spxextra{C++ type}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:_CPPv4N6nvidia15inferenceserver6client12InferContext4KindE}}%
\pysigstartmultiline
\pysigline{\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1RequestTimers_1ad3d72a1b49c1e9ebb2bbeb15e04401f8}}\sphinxbfcode{\sphinxupquote{enum }}\sphinxbfcode{\sphinxupquote{Kind}}}%
\pysigstopmultiline
The kind of the timer. 

\sphinxstyleemphasis{Values:}
\index{nvidia::inferenceserver::client::InferContext::REQUEST\_START (C++ enumerator)@\spxentry{nvidia::inferenceserver::client::InferContext::REQUEST\_START}\spxextra{C++ enumerator}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:_CPPv4N6nvidia15inferenceserver6client12InferContext13REQUEST_STARTE}}%
\pysigstartmultiline
\pysigline{\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1RequestTimers_1ad3d72a1b49c1e9ebb2bbeb15e04401f8a2ca0e66d9f91476f9f00f61c5a834cbb}}\sphinxbfcode{\sphinxupquote{REQUEST\_START}}}%
\pysigstopmultiline
The start of request handling. 

\end{fulllineitems}

\index{nvidia::inferenceserver::client::InferContext::REQUEST\_END (C++ enumerator)@\spxentry{nvidia::inferenceserver::client::InferContext::REQUEST\_END}\spxextra{C++ enumerator}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:_CPPv4N6nvidia15inferenceserver6client12InferContext11REQUEST_ENDE}}%
\pysigstartmultiline
\pysigline{\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1RequestTimers_1ad3d72a1b49c1e9ebb2bbeb15e04401f8ad793ca80c1d0010ecc2f269b6a4b621a}}\sphinxbfcode{\sphinxupquote{REQUEST\_END}}}%
\pysigstopmultiline
The end of request handling. 

\end{fulllineitems}

\index{nvidia::inferenceserver::client::InferContext::SEND\_START (C++ enumerator)@\spxentry{nvidia::inferenceserver::client::InferContext::SEND\_START}\spxextra{C++ enumerator}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:_CPPv4N6nvidia15inferenceserver6client12InferContext10SEND_STARTE}}%
\pysigstartmultiline
\pysigline{\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1RequestTimers_1ad3d72a1b49c1e9ebb2bbeb15e04401f8a4cb619f22003a3f9334144b07c71d1f3}}\sphinxbfcode{\sphinxupquote{SEND\_START}}}%
\pysigstopmultiline
The start of sending request bytes to the server (i.e. first byte). 

\end{fulllineitems}

\index{nvidia::inferenceserver::client::InferContext::SEND\_END (C++ enumerator)@\spxentry{nvidia::inferenceserver::client::InferContext::SEND\_END}\spxextra{C++ enumerator}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:_CPPv4N6nvidia15inferenceserver6client12InferContext8SEND_ENDE}}%
\pysigstartmultiline
\pysigline{\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1RequestTimers_1ad3d72a1b49c1e9ebb2bbeb15e04401f8a8330cc174bf610d2c3afaa2d68e535b2}}\sphinxbfcode{\sphinxupquote{SEND\_END}}}%
\pysigstopmultiline
The end of sending request bytes to the server (i.e. last byte). 

\end{fulllineitems}

\index{nvidia::inferenceserver::client::InferContext::RECEIVE\_START (C++ enumerator)@\spxentry{nvidia::inferenceserver::client::InferContext::RECEIVE\_START}\spxextra{C++ enumerator}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:_CPPv4N6nvidia15inferenceserver6client12InferContext13RECEIVE_STARTE}}%
\pysigstartmultiline
\pysigline{\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1RequestTimers_1ad3d72a1b49c1e9ebb2bbeb15e04401f8aec6eb4b12033d2b794c2a0f3d31c2930}}\sphinxbfcode{\sphinxupquote{RECEIVE\_START}}}%
\pysigstopmultiline
The start of receiving response bytes from the server (i.e. 

first byte). 

\end{fulllineitems}

\index{nvidia::inferenceserver::client::InferContext::RECEIVE\_END (C++ enumerator)@\spxentry{nvidia::inferenceserver::client::InferContext::RECEIVE\_END}\spxextra{C++ enumerator}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:_CPPv4N6nvidia15inferenceserver6client12InferContext11RECEIVE_ENDE}}%
\pysigstartmultiline
\pysigline{\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1RequestTimers_1ad3d72a1b49c1e9ebb2bbeb15e04401f8a66feb30d6ad2348be0c107324aceb3b7}}\sphinxbfcode{\sphinxupquote{RECEIVE\_END}}}%
\pysigstopmultiline
The end of receiving response bytes from the server (i.e. 

last byte). 

\end{fulllineitems}


\end{fulllineitems}

\subsubsection*{Public Functions}
\index{nvidia::inferenceserver::client::InferContext::RequestTimers::RequestTimers (C++ function)@\spxentry{nvidia::inferenceserver::client::InferContext::RequestTimers::RequestTimers}\spxextra{C++ function}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:_CPPv4N6nvidia15inferenceserver6client12InferContext13RequestTimers13RequestTimersEv}}%
\pysigstartmultiline
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1RequestTimers_1ac2935983f1de4bd33820357823c36e2f}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{RequestTimers}}}{}{}%
\pysigstopmultiline
Construct a timer with zero-ed timestamps. 

\end{fulllineitems}

\index{nvidia::inferenceserver::client::InferContext::RequestTimers::Reset (C++ function)@\spxentry{nvidia::inferenceserver::client::InferContext::RequestTimers::Reset}\spxextra{C++ function}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:_CPPv4N6nvidia15inferenceserver6client12InferContext13RequestTimers5ResetEv}}%
\pysigstartmultiline
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1RequestTimers_1aa5ace810b5d34b9f6c3a9ae9a3d21dda}}\pysiglinewithargsret{{\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1Error:_CPPv4N6nvidia15inferenceserver6client5ErrorE}]{\sphinxcrossref{Error}}} \sphinxbfcode{\sphinxupquote{Reset}}}{}{}%
\pysigstopmultiline
Reset all timestamp values to zero. 

Must be called before re-using the timer. \begin{description}
\item[{\sphinxstylestrong{Return}}] \leavevmode
{\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1Error:classnvidia_1_1inferenceserver_1_1client_1_1Error}]{\sphinxcrossref{\DUrole{std,std-ref}{Error}}}} object indicating success or failure. 

\end{description}


\end{fulllineitems}

\index{nvidia::inferenceserver::client::InferContext::RequestTimers::Record (C++ function)@\spxentry{nvidia::inferenceserver::client::InferContext::RequestTimers::Record}\spxextra{C++ function}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:_CPPv4N6nvidia15inferenceserver6client12InferContext13RequestTimers6RecordE4Kind}}%
\pysigstartmultiline
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1RequestTimers_1ab15f971203dcbd5ed114709318f2232b}}\pysiglinewithargsret{{\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1Error:_CPPv4N6nvidia15inferenceserver6client5ErrorE}]{\sphinxcrossref{Error}}} \sphinxbfcode{\sphinxupquote{Record}}}{{\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1RequestTimers:_CPPv4N6nvidia15inferenceserver6client12InferContext13RequestTimers4KindE}]{\sphinxcrossref{Kind}}} \sphinxstyleemphasis{kind}}{}%
\pysigstopmultiline
Record the current timestamp for a request stage. 

\begin{description}
\item[{\sphinxstylestrong{Return}}] \leavevmode
{\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1Error:classnvidia_1_1inferenceserver_1_1client_1_1Error}]{\sphinxcrossref{\DUrole{std,std-ref}{Error}}}} object indicating success or failure. 

\item[{\sphinxstylestrong{Parameters}}] \leavevmode\begin{itemize}
\item {} 
\sphinxcode{\sphinxupquote{kind}}: The Kind of the timestamp. 

\end{itemize}

\end{description}


\end{fulllineitems}


\end{fulllineitems}

\index{nvidia::inferenceserver::client::InferContext::Result (C++ class)@\spxentry{nvidia::inferenceserver::client::InferContext::Result}\spxextra{C++ class}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:_CPPv4N6nvidia15inferenceserver6client12InferContext6ResultE}}%
\pysigstartmultiline
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result}}\pysigline{\sphinxbfcode{\sphinxupquote{class }}\sphinxbfcode{\sphinxupquote{Result}}}%
\pysigstopmultiline
An inference result corresponding to an output. 
\subsubsection*{Public Types}
\index{nvidia::inferenceserver::client::InferContext::ResultFormat (C++ type)@\spxentry{nvidia::inferenceserver::client::InferContext::ResultFormat}\spxextra{C++ type}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:_CPPv4N6nvidia15inferenceserver6client12InferContext12ResultFormatE}}%
\pysigstartmultiline
\pysigline{\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result_1a8bb9d36d1808a030dda919af87746dad}}\sphinxbfcode{\sphinxupquote{enum }}\sphinxbfcode{\sphinxupquote{ResultFormat}}}%
\pysigstopmultiline
Format in which result is returned. 

\sphinxstyleemphasis{Values:}
\index{nvidia::inferenceserver::client::InferContext::RAW (C++ enumerator)@\spxentry{nvidia::inferenceserver::client::InferContext::RAW}\spxextra{C++ enumerator}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:_CPPv4N6nvidia15inferenceserver6client12InferContext3RAWE}}%
\pysigstartmultiline
\pysigline{\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result_1a8bb9d36d1808a030dda919af87746dadada3b38eab85f839509454eb2f3df8ea6}}\sphinxbfcode{\sphinxupquote{RAW}} = 0}%
\pysigstopmultiline
RAW format is the entire result tensor of values. 

\end{fulllineitems}

\index{nvidia::inferenceserver::client::InferContext::CLASS (C++ enumerator)@\spxentry{nvidia::inferenceserver::client::InferContext::CLASS}\spxextra{C++ enumerator}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:_CPPv4N6nvidia15inferenceserver6client12InferContext5CLASSE}}%
\pysigstartmultiline
\pysigline{\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result_1a8bb9d36d1808a030dda919af87746dada9f91171ba1a8902be2821d6ba488dbd4}}\sphinxbfcode{\sphinxupquote{CLASS}} = 1}%
\pysigstopmultiline
CLASS format is the top-k highest probability values of the result and the associated class label (if provided by the model). 

\end{fulllineitems}


\end{fulllineitems}

\subsubsection*{Public Functions}
\index{nvidia::inferenceserver::client::InferContext::Result::\textasciitilde{}Result (C++ function)@\spxentry{nvidia::inferenceserver::client::InferContext::Result::\textasciitilde{}Result}\spxextra{C++ function}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:_CPPv4N6nvidia15inferenceserver6client12InferContext6ResultD0Ev}}%
\pysigstartmultiline
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result_1aa87dddd7be7a18f787946a6548398d3b}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{virtual}} \sphinxbfcode{\sphinxupquote{\textasciitilde{}Result}}}{}{}%
\pysigstopmultiline
Destroy the result. 

\end{fulllineitems}

\index{nvidia::inferenceserver::client::InferContext::Result::ModelName (C++ function)@\spxentry{nvidia::inferenceserver::client::InferContext::Result::ModelName}\spxextra{C++ function}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:_CPPv4NK6nvidia15inferenceserver6client12InferContext6Result9ModelNameEv}}%
\pysigstartmultiline
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result_1a78206cf92579339875ad3a44318a28f7}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{virtual}} \sphinxbfcode{\sphinxupquote{const}} std::string \&\sphinxbfcode{\sphinxupquote{ModelName}}}{}{ \sphinxbfcode{\sphinxupquote{const}} = 0}%
\pysigstopmultiline
\begin{description}
\item[{\sphinxstylestrong{Return}}] \leavevmode
The name of the model that produced this result. 

\end{description}


\end{fulllineitems}

\index{nvidia::inferenceserver::client::InferContext::Result::ModelVersion (C++ function)@\spxentry{nvidia::inferenceserver::client::InferContext::Result::ModelVersion}\spxextra{C++ function}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:_CPPv4NK6nvidia15inferenceserver6client12InferContext6Result12ModelVersionEv}}%
\pysigstartmultiline
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result_1a9a81d8a1588920dc784fcb4b2520f566}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{virtual}} int64\_t \sphinxbfcode{\sphinxupquote{ModelVersion}}}{}{ \sphinxbfcode{\sphinxupquote{const}} = 0}%
\pysigstopmultiline
\begin{description}
\item[{\sphinxstylestrong{Return}}] \leavevmode
The version of the model that produced this result. 

\end{description}


\end{fulllineitems}

\index{nvidia::inferenceserver::client::InferContext::Result::GetOutput (C++ function)@\spxentry{nvidia::inferenceserver::client::InferContext::Result::GetOutput}\spxextra{C++ function}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:_CPPv4NK6nvidia15inferenceserver6client12InferContext6Result9GetOutputEv}}%
\pysigstartmultiline
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result_1a7687d7d07c181ba27f32654341390a92}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{virtual}} \sphinxbfcode{\sphinxupquote{const}} std::shared\_ptr\textless{}{\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:_CPPv4N6nvidia15inferenceserver6client12InferContext6OutputE}]{\sphinxcrossref{Output}}}\textgreater{} \sphinxbfcode{\sphinxupquote{GetOutput}}}{}{ \sphinxbfcode{\sphinxupquote{const}} = 0}%
\pysigstopmultiline
\begin{description}
\item[{\sphinxstylestrong{Return}}] \leavevmode
The {\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Output:classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Output}]{\sphinxcrossref{\DUrole{std,std-ref}{Output}}}} object corresponding to this result. 

\end{description}


\end{fulllineitems}

\index{nvidia::inferenceserver::client::InferContext::Result::GetRawShape (C++ function)@\spxentry{nvidia::inferenceserver::client::InferContext::Result::GetRawShape}\spxextra{C++ function}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:_CPPv4NK6nvidia15inferenceserver6client12InferContext6Result11GetRawShapeEPNSt6vectorI7int64_tEE}}%
\pysigstartmultiline
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result_1aeeec1b3448cf681e3ec8ac3c3c589374}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{virtual}} {\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1Error:_CPPv4N6nvidia15inferenceserver6client5ErrorE}]{\sphinxcrossref{Error}}} \sphinxbfcode{\sphinxupquote{GetRawShape}}}{std::vector\textless{}int64\_t\textgreater{} *\sphinxstyleemphasis{shape}}{ \sphinxbfcode{\sphinxupquote{const}} = 0}%
\pysigstopmultiline
Get the shape of a raw result. 

The shape does not include the batch dimension. \begin{description}
\item[{\sphinxstylestrong{Return}}] \leavevmode
{\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1Error:classnvidia_1_1inferenceserver_1_1client_1_1Error}]{\sphinxcrossref{\DUrole{std,std-ref}{Error}}}} object indicating success or failure. 

\item[{\sphinxstylestrong{Parameters}}] \leavevmode\begin{itemize}
\item {} 
\sphinxcode{\sphinxupquote{shape}}: Returns the shape. 

\end{itemize}

\end{description}


\end{fulllineitems}

\index{nvidia::inferenceserver::client::InferContext::Result::GetRaw (C++ function)@\spxentry{nvidia::inferenceserver::client::InferContext::Result::GetRaw}\spxextra{C++ function}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:_CPPv4NK6nvidia15inferenceserver6client12InferContext6Result6GetRawE6size_tPPKNSt6vectorI7uint8_tEE}}%
\pysigstartmultiline
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result_1af97122884c92553992261c501dbe3095}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{virtual}} {\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1Error:_CPPv4N6nvidia15inferenceserver6client5ErrorE}]{\sphinxcrossref{Error}}} \sphinxbfcode{\sphinxupquote{GetRaw}}}{size\_t \sphinxstyleemphasis{batch\_idx}, \sphinxbfcode{\sphinxupquote{const}} std::vector\textless{}uint8\_t\textgreater{} **\sphinxstyleemphasis{buf}}{ \sphinxbfcode{\sphinxupquote{const}} = 0}%
\pysigstopmultiline
Get a reference to entire raw result data for a specific batch entry. 

Returns error if this result is not RAW format. \begin{description}
\item[{\sphinxstylestrong{Return}}] \leavevmode
{\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1Error:classnvidia_1_1inferenceserver_1_1client_1_1Error}]{\sphinxcrossref{\DUrole{std,std-ref}{Error}}}} object indicating success or failure. 

\item[{\sphinxstylestrong{Parameters}}] \leavevmode\begin{itemize}
\item {} 
\sphinxcode{\sphinxupquote{batch\_idx}}: Returns the results for this entry of the batch. 

\item {} 
\sphinxcode{\sphinxupquote{buf}}: Returns the vector of result bytes. 

\end{itemize}

\end{description}


\end{fulllineitems}

\index{nvidia::inferenceserver::client::InferContext::Result::GetRawAtCursor (C++ function)@\spxentry{nvidia::inferenceserver::client::InferContext::Result::GetRawAtCursor}\spxextra{C++ function}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:_CPPv4N6nvidia15inferenceserver6client12InferContext6Result14GetRawAtCursorE6size_tPPK7uint8_t6size_t}}%
\pysigstartmultiline
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result_1a2a13887812b025896d569f041db7e2c8}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{virtual}} {\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1Error:_CPPv4N6nvidia15inferenceserver6client5ErrorE}]{\sphinxcrossref{Error}}} \sphinxbfcode{\sphinxupquote{GetRawAtCursor}}}{size\_t \sphinxstyleemphasis{batch\_idx}, \sphinxbfcode{\sphinxupquote{const}} uint8\_t **\sphinxstyleemphasis{buf}, size\_t \sphinxstyleemphasis{adv\_byte\_size}}{ = 0}%
\pysigstopmultiline
Get a reference to raw result data for a specific batch entry at the current “cursor” and advance the cursor by the specified number of bytes. 

More typically use {\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result:classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result_1a2a13887812b025896d569f041db7e2c8}]{\sphinxcrossref{\DUrole{std,std-ref}{GetRawAtCursor\textless{}T\textgreater{}()}}}} method to return the data as a specific type T. Use {\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result:classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result_1adb27437d415b569a3a4b7700c56002bf}]{\sphinxcrossref{\DUrole{std,std-ref}{ResetCursor()}}}} to reset the cursor to the beginning of the result. Returns error if this result is not RAW format. \begin{description}
\item[{\sphinxstylestrong{Return}}] \leavevmode
{\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1Error:classnvidia_1_1inferenceserver_1_1client_1_1Error}]{\sphinxcrossref{\DUrole{std,std-ref}{Error}}}} object indicating success or failure. 

\item[{\sphinxstylestrong{Parameters}}] \leavevmode\begin{itemize}
\item {} 
\sphinxcode{\sphinxupquote{batch\_idx}}: Returns results for this entry of the batch. 

\item {} 
\sphinxcode{\sphinxupquote{buf}}: Returns pointer to ‘adv\_byte\_size’ bytes of data. 

\item {} 
\sphinxcode{\sphinxupquote{adv\_byte\_size}}: The number of bytes of data to get a reference to. 

\end{itemize}

\end{description}


\end{fulllineitems}

\index{nvidia::inferenceserver::client::InferContext::Result::GetRawAtCursor (C++ function)@\spxentry{nvidia::inferenceserver::client::InferContext::Result::GetRawAtCursor}\spxextra{C++ function}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:_CPPv4N6nvidia15inferenceserver6client12InferContext6Result14GetRawAtCursorE6size_tP1T}}%
\pysigstartmultiline
\pysigline{\sphinxbfcode{\sphinxupquote{template }}\textless{}\sphinxbfcode{\sphinxupquote{typename}} T\textgreater{}}\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result_1a64d951fd64068b9d2e3da103f47390fe}}\pysiglinewithargsret{{\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1Error:_CPPv4N6nvidia15inferenceserver6client5ErrorE}]{\sphinxcrossref{Error}}} \sphinxbfcode{\sphinxupquote{GetRawAtCursor}}}{size\_t \sphinxstyleemphasis{batch\_idx}, T *\sphinxstyleemphasis{out}}{}%
\pysigstopmultiline
Read a value for a specific batch entry at the current “cursor” from the result tensor as the specified type T and advance the cursor. 

Use {\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result:classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result_1adb27437d415b569a3a4b7700c56002bf}]{\sphinxcrossref{\DUrole{std,std-ref}{ResetCursor()}}}} to reset the cursor to the beginning of the result. Returns error if this result is not RAW format. \begin{description}
\item[{\sphinxstylestrong{Return}}] \leavevmode
{\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1Error:classnvidia_1_1inferenceserver_1_1client_1_1Error}]{\sphinxcrossref{\DUrole{std,std-ref}{Error}}}} object indicating success or failure. 

\item[{\sphinxstylestrong{Parameters}}] \leavevmode\begin{itemize}
\item {} 
\sphinxcode{\sphinxupquote{batch\_idx}}: Returns results for this entry of the batch. 

\item {} 
\sphinxcode{\sphinxupquote{out}}: Returns the value at the cursor. 

\end{itemize}

\end{description}


\end{fulllineitems}

\index{nvidia::inferenceserver::client::InferContext::Result::GetClassCount (C++ function)@\spxentry{nvidia::inferenceserver::client::InferContext::Result::GetClassCount}\spxextra{C++ function}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:_CPPv4NK6nvidia15inferenceserver6client12InferContext6Result13GetClassCountE6size_tP6size_t}}%
\pysigstartmultiline
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result_1ac50ae04862c58a004547cac5debc7c98}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{virtual}} {\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1Error:_CPPv4N6nvidia15inferenceserver6client5ErrorE}]{\sphinxcrossref{Error}}} \sphinxbfcode{\sphinxupquote{GetClassCount}}}{size\_t \sphinxstyleemphasis{batch\_idx}, size\_t *\sphinxstyleemphasis{cnt}}{ \sphinxbfcode{\sphinxupquote{const}} = 0}%
\pysigstopmultiline
Get the number of class results for a batch. 

Returns error if this result is not CLASS format. \begin{description}
\item[{\sphinxstylestrong{Return}}] \leavevmode
{\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1Error:classnvidia_1_1inferenceserver_1_1client_1_1Error}]{\sphinxcrossref{\DUrole{std,std-ref}{Error}}}} object indicating success or failure. 

\item[{\sphinxstylestrong{Parameters}}] \leavevmode\begin{itemize}
\item {} 
\sphinxcode{\sphinxupquote{batch\_idx}}: The index in the batch. 

\item {} 
\sphinxcode{\sphinxupquote{cnt}}: Returns the number of {\hyperref[\detokenize{cpp_api/structnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result_1_1ClassResult:structnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result_1_1ClassResult}]{\sphinxcrossref{\DUrole{std,std-ref}{ClassResult}}}} entries for the batch entry. 

\end{itemize}

\end{description}


\end{fulllineitems}

\index{nvidia::inferenceserver::client::InferContext::Result::GetClassAtCursor (C++ function)@\spxentry{nvidia::inferenceserver::client::InferContext::Result::GetClassAtCursor}\spxextra{C++ function}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:_CPPv4N6nvidia15inferenceserver6client12InferContext6Result16GetClassAtCursorE6size_tP11ClassResult}}%
\pysigstartmultiline
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result_1a0b78337cb9b55608b310f226dd6e8287}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{virtual}} {\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1Error:_CPPv4N6nvidia15inferenceserver6client5ErrorE}]{\sphinxcrossref{Error}}} \sphinxbfcode{\sphinxupquote{GetClassAtCursor}}}{size\_t \sphinxstyleemphasis{batch\_idx}, {\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:_CPPv4N6nvidia15inferenceserver6client12InferContext6Result11ClassResultE}]{\sphinxcrossref{ClassResult}}} *\sphinxstyleemphasis{result}}{ = 0}%
\pysigstopmultiline
Get the {\hyperref[\detokenize{cpp_api/structnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result_1_1ClassResult:structnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result_1_1ClassResult}]{\sphinxcrossref{\DUrole{std,std-ref}{ClassResult}}}} result for a specific batch entry at the current cursor. 

Use {\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result:classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result_1adb27437d415b569a3a4b7700c56002bf}]{\sphinxcrossref{\DUrole{std,std-ref}{ResetCursor()}}}} to reset the cursor to the beginning of the result. Returns error if this result is not CLASS format. \begin{description}
\item[{\sphinxstylestrong{Return}}] \leavevmode
{\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1Error:classnvidia_1_1inferenceserver_1_1client_1_1Error}]{\sphinxcrossref{\DUrole{std,std-ref}{Error}}}} object indicating success or failure. 

\item[{\sphinxstylestrong{Parameters}}] \leavevmode\begin{itemize}
\item {} 
\sphinxcode{\sphinxupquote{batch\_idx}}: The index in the batch. 

\item {} 
\sphinxcode{\sphinxupquote{result}}: Returns the {\hyperref[\detokenize{cpp_api/structnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result_1_1ClassResult:structnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result_1_1ClassResult}]{\sphinxcrossref{\DUrole{std,std-ref}{ClassResult}}}} value for the batch at the cursor. 

\end{itemize}

\end{description}


\end{fulllineitems}

\index{nvidia::inferenceserver::client::InferContext::Result::ResetCursors (C++ function)@\spxentry{nvidia::inferenceserver::client::InferContext::Result::ResetCursors}\spxextra{C++ function}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:_CPPv4N6nvidia15inferenceserver6client12InferContext6Result12ResetCursorsEv}}%
\pysigstartmultiline
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result_1a304de867ffb5121c21874e11a4e9d34a}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{virtual}} {\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1Error:_CPPv4N6nvidia15inferenceserver6client5ErrorE}]{\sphinxcrossref{Error}}} \sphinxbfcode{\sphinxupquote{ResetCursors}}}{}{ = 0}%
\pysigstopmultiline
Reset cursor to beginning of result for all batch entries. 

\begin{description}
\item[{\sphinxstylestrong{Return}}] \leavevmode
{\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1Error:classnvidia_1_1inferenceserver_1_1client_1_1Error}]{\sphinxcrossref{\DUrole{std,std-ref}{Error}}}} object indicating success or failure. 

\end{description}


\end{fulllineitems}

\index{nvidia::inferenceserver::client::InferContext::Result::ResetCursor (C++ function)@\spxentry{nvidia::inferenceserver::client::InferContext::Result::ResetCursor}\spxextra{C++ function}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:_CPPv4N6nvidia15inferenceserver6client12InferContext6Result11ResetCursorE6size_t}}%
\pysigstartmultiline
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result_1adb27437d415b569a3a4b7700c56002bf}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{virtual}} {\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1Error:_CPPv4N6nvidia15inferenceserver6client5ErrorE}]{\sphinxcrossref{Error}}} \sphinxbfcode{\sphinxupquote{ResetCursor}}}{size\_t \sphinxstyleemphasis{batch\_idx}}{ = 0}%
\pysigstopmultiline
Reset cursor to beginning of result for specified batch entry. 

\begin{description}
\item[{\sphinxstylestrong{Return}}] \leavevmode
{\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1Error:classnvidia_1_1inferenceserver_1_1client_1_1Error}]{\sphinxcrossref{\DUrole{std,std-ref}{Error}}}} object indicating success or failure. 

\item[{\sphinxstylestrong{Parameters}}] \leavevmode\begin{itemize}
\item {} 
\sphinxcode{\sphinxupquote{batch\_idx}}: The index in the batch. 

\end{itemize}

\end{description}


\end{fulllineitems}

\index{nvidia::inferenceserver::client::InferContext::Result::GetRawAtCursor (C++ function)@\spxentry{nvidia::inferenceserver::client::InferContext::Result::GetRawAtCursor}\spxextra{C++ function}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:_CPPv4N6nvidia15inferenceserver6client12InferContext6Result14GetRawAtCursorE6size_tPNSt6stringE}}%
\pysigstartmultiline
\pysigline{\sphinxbfcode{\sphinxupquote{template }}\textless{}\textgreater{}}\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result_1a742f381607b9915cb2fc558511f491c1}}\pysiglinewithargsret{{\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1Error:_CPPv4N6nvidia15inferenceserver6client5ErrorE}]{\sphinxcrossref{Error}}} \sphinxbfcode{\sphinxupquote{GetRawAtCursor}}}{size\_t \sphinxstyleemphasis{batch\_idx}, std::string *\sphinxstyleemphasis{out}}{}%
\pysigstopmultiline
\end{fulllineitems}

\index{nvidia::inferenceserver::client::InferContext::Result::ClassResult (C++ class)@\spxentry{nvidia::inferenceserver::client::InferContext::Result::ClassResult}\spxextra{C++ class}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:_CPPv4N6nvidia15inferenceserver6client12InferContext6Result11ClassResultE}}%
\pysigstartmultiline
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:structnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result_1_1ClassResult}}\pysigline{\sphinxbfcode{\sphinxupquote{struct }}\sphinxbfcode{\sphinxupquote{ClassResult}}}%
\pysigstopmultiline
The result value for CLASS format results. 
\subsubsection*{Public Members}
\index{nvidia::inferenceserver::client::InferContext::Result::ClassResult::idx (C++ member)@\spxentry{nvidia::inferenceserver::client::InferContext::Result::ClassResult::idx}\spxextra{C++ member}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:_CPPv4N6nvidia15inferenceserver6client12InferContext6Result11ClassResult3idxE}}%
\pysigstartmultiline
\pysigline{\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:structnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result_1_1ClassResult_1a8480131c8f15b37e32f19400ec5a3d5d}}size\_t \sphinxbfcode{\sphinxupquote{idx}}}%
\pysigstopmultiline
The index of the class in the result vector. 

\end{fulllineitems}

\index{nvidia::inferenceserver::client::InferContext::Result::ClassResult::value (C++ member)@\spxentry{nvidia::inferenceserver::client::InferContext::Result::ClassResult::value}\spxextra{C++ member}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:_CPPv4N6nvidia15inferenceserver6client12InferContext6Result11ClassResult5valueE}}%
\pysigstartmultiline
\pysigline{\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:structnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result_1_1ClassResult_1af0c1a1ef9932a40cd1fbbea3e5ae7b38}}float \sphinxbfcode{\sphinxupquote{value}}}%
\pysigstopmultiline
The value of the class. 

\end{fulllineitems}

\index{nvidia::inferenceserver::client::InferContext::Result::ClassResult::label (C++ member)@\spxentry{nvidia::inferenceserver::client::InferContext::Result::ClassResult::label}\spxextra{C++ member}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:_CPPv4N6nvidia15inferenceserver6client12InferContext6Result11ClassResult5labelE}}%
\pysigstartmultiline
\pysigline{\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:structnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result_1_1ClassResult_1a517c52eae642ee9a37102070da66972a}}std::string \sphinxbfcode{\sphinxupquote{label}}}%
\pysigstopmultiline
The label for the class, if provided by the model. 

\end{fulllineitems}


\end{fulllineitems}


\end{fulllineitems}

\index{nvidia::inferenceserver::client::InferContext::Stat (C++ class)@\spxentry{nvidia::inferenceserver::client::InferContext::Stat}\spxextra{C++ class}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:_CPPv4N6nvidia15inferenceserver6client12InferContext4StatE}}%
\pysigstartmultiline
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:structnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Stat}}\pysigline{\sphinxbfcode{\sphinxupquote{struct }}\sphinxbfcode{\sphinxupquote{Stat}}}%
\pysigstopmultiline
Cumulative statistic of the {\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:classnvidia_1_1inferenceserver_1_1client_1_1InferContext}]{\sphinxcrossref{\DUrole{std,std-ref}{InferContext}}}}. 

\begin{description}
\item[{\sphinxstylestrong{Note}}] \leavevmode
For GRPC protocol, ‘cumulative\_send\_time\_ns’ represents the time for marshaling infer request. ‘cumulative\_receive\_time\_ns’ represents the time for unmarshaling infer response. 

\end{description}

\subsubsection*{Public Functions}
\index{nvidia::inferenceserver::client::InferContext::Stat::Stat (C++ function)@\spxentry{nvidia::inferenceserver::client::InferContext::Stat::Stat}\spxextra{C++ function}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:_CPPv4N6nvidia15inferenceserver6client12InferContext4Stat4StatEv}}%
\pysigstartmultiline
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:structnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Stat_1aa1c18a68160ee9ccb7002fdb40e7b15c}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{Stat}}}{}{}%
\pysigstopmultiline
Create a new {\hyperref[\detokenize{cpp_api/structnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Stat:structnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Stat}]{\sphinxcrossref{\DUrole{std,std-ref}{Stat}}}} object with zero-ed statistics. 

\end{fulllineitems}

\subsubsection*{Public Members}
\index{nvidia::inferenceserver::client::InferContext::Stat::completed\_request\_count (C++ member)@\spxentry{nvidia::inferenceserver::client::InferContext::Stat::completed\_request\_count}\spxextra{C++ member}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:_CPPv4N6nvidia15inferenceserver6client12InferContext4Stat23completed_request_countE}}%
\pysigstartmultiline
\pysigline{\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:structnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Stat_1ad76e90f2ef845eb146acab963bbffd1a}}size\_t \sphinxbfcode{\sphinxupquote{completed\_request\_count}}}%
\pysigstopmultiline
Total number of requests completed. 

\end{fulllineitems}

\index{nvidia::inferenceserver::client::InferContext::Stat::cumulative\_total\_request\_time\_ns (C++ member)@\spxentry{nvidia::inferenceserver::client::InferContext::Stat::cumulative\_total\_request\_time\_ns}\spxextra{C++ member}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:_CPPv4N6nvidia15inferenceserver6client12InferContext4Stat32cumulative_total_request_time_nsE}}%
\pysigstartmultiline
\pysigline{\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:structnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Stat_1abf5e228bd9de072b92cdbfd06b10c8cf}}uint64\_t \sphinxbfcode{\sphinxupquote{cumulative\_total\_request\_time\_ns}}}%
\pysigstopmultiline
Time from the request start until the response is completely received. 

\end{fulllineitems}

\index{nvidia::inferenceserver::client::InferContext::Stat::cumulative\_send\_time\_ns (C++ member)@\spxentry{nvidia::inferenceserver::client::InferContext::Stat::cumulative\_send\_time\_ns}\spxextra{C++ member}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:_CPPv4N6nvidia15inferenceserver6client12InferContext4Stat23cumulative_send_time_nsE}}%
\pysigstartmultiline
\pysigline{\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:structnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Stat_1a5b8451d591204705cfb9d8854f132b18}}uint64\_t \sphinxbfcode{\sphinxupquote{cumulative\_send\_time\_ns}}}%
\pysigstopmultiline
Time from the request start until the last byte is sent. 

\end{fulllineitems}

\index{nvidia::inferenceserver::client::InferContext::Stat::cumulative\_receive\_time\_ns (C++ member)@\spxentry{nvidia::inferenceserver::client::InferContext::Stat::cumulative\_receive\_time\_ns}\spxextra{C++ member}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:_CPPv4N6nvidia15inferenceserver6client12InferContext4Stat26cumulative_receive_time_nsE}}%
\pysigstartmultiline
\pysigline{\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:structnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Stat_1ae113efbf699abdd52d45996dcc7d4689}}uint64\_t \sphinxbfcode{\sphinxupquote{cumulative\_receive\_time\_ns}}}%
\pysigstopmultiline
Time from receiving first byte of the response until the response is completely received. 

\end{fulllineitems}


\end{fulllineitems}


\end{fulllineitems}



\subsubsection{Class InferContext::Input}
\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Input:class-infercontext-input}}\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Input:exhale-class-classnvidia-1-1inferenceserver-1-1client-1-1infercontext-1-1input}}\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Input::doc}}\begin{itemize}
\item {} 
Defined in {\hyperref[\detokenize{cpp_api/file_src_clients_c++_request.h:file-src-clients-c-request-h}]{\sphinxcrossref{\DUrole{std,std-ref}{File request.h}}}}

\end{itemize}


\paragraph{Nested Relationships}
\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Input:nested-relationships}}
This class is a nested type of {\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:exhale-class-classnvidia-1-1inferenceserver-1-1client-1-1infercontext}]{\sphinxcrossref{\DUrole{std,std-ref}{Class InferContext}}}}.


\paragraph{Class Documentation}
\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Input:class-documentation}}\index{nvidia::inferenceserver::client::InferContext::Input (C++ class)@\spxentry{nvidia::inferenceserver::client::InferContext::Input}\spxextra{C++ class}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Input:_CPPv4N6nvidia15inferenceserver6client12InferContext5InputE}}%
\pysigstartmultiline
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Input:classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Input}}\pysigline{\sphinxbfcode{\sphinxupquote{class }}\sphinxbfcode{\sphinxupquote{Input}}}%
\pysigstopmultiline
An input to the model. 
\subsubsection*{Public Functions}
\index{nvidia::inferenceserver::client::InferContext::Input::\textasciitilde{}Input (C++ function)@\spxentry{nvidia::inferenceserver::client::InferContext::Input::\textasciitilde{}Input}\spxextra{C++ function}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Input:_CPPv4N6nvidia15inferenceserver6client12InferContext5InputD0Ev}}%
\pysigstartmultiline
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Input:classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Input_1aeb32510fa7e86063d3341302ff5cc322}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{virtual}} \sphinxbfcode{\sphinxupquote{\textasciitilde{}Input}}}{}{}%
\pysigstopmultiline
Destroy the input. 

\end{fulllineitems}

\index{nvidia::inferenceserver::client::InferContext::Input::Name (C++ function)@\spxentry{nvidia::inferenceserver::client::InferContext::Input::Name}\spxextra{C++ function}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Input:_CPPv4NK6nvidia15inferenceserver6client12InferContext5Input4NameEv}}%
\pysigstartmultiline
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Input:classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Input_1a66b20a9167b8eeb9ddde10430b37fd01}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{virtual}} \sphinxbfcode{\sphinxupquote{const}} std::string \&\sphinxbfcode{\sphinxupquote{Name}}}{}{ \sphinxbfcode{\sphinxupquote{const}} = 0}%
\pysigstopmultiline
\begin{description}
\item[{\sphinxstylestrong{Return}}] \leavevmode
The name of the input. 

\end{description}


\end{fulllineitems}

\index{nvidia::inferenceserver::client::InferContext::Input::ByteSize (C++ function)@\spxentry{nvidia::inferenceserver::client::InferContext::Input::ByteSize}\spxextra{C++ function}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Input:_CPPv4NK6nvidia15inferenceserver6client12InferContext5Input8ByteSizeEv}}%
\pysigstartmultiline
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Input:classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Input_1a34951a4f1c906bf3c147ee84549d8667}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{virtual}} int64\_t \sphinxbfcode{\sphinxupquote{ByteSize}}}{}{ \sphinxbfcode{\sphinxupquote{const}} = 0}%
\pysigstopmultiline
\begin{description}
\item[{\sphinxstylestrong{Return}}] \leavevmode
The size in bytes of this input. This is the size for one instance of the input, not the entire size of a batched input. When the byte-size is not known, for example for non-fixed-sized types like TYPE\_STRING or for inputs with variable-size dimensions, this will return -1. 

\end{description}


\end{fulllineitems}

\index{nvidia::inferenceserver::client::InferContext::Input::TotalByteSize (C++ function)@\spxentry{nvidia::inferenceserver::client::InferContext::Input::TotalByteSize}\spxextra{C++ function}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Input:_CPPv4NK6nvidia15inferenceserver6client12InferContext5Input13TotalByteSizeEv}}%
\pysigstartmultiline
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Input:classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Input_1a07c715257ed5ed2feab0982d9c6fd941}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{virtual}} size\_t \sphinxbfcode{\sphinxupquote{TotalByteSize}}}{}{ \sphinxbfcode{\sphinxupquote{const}} = 0}%
\pysigstopmultiline
\begin{description}
\item[{\sphinxstylestrong{Return}}] \leavevmode
The size in bytes of entire batch of this input. For fixed-sized types this is just {\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Input:classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Input_1a34951a4f1c906bf3c147ee84549d8667}]{\sphinxcrossref{\DUrole{std,std-ref}{ByteSize()}}}} * batch-size, but for non-fixed-sized types like TYPE\_STRING it is the only way to get the entire input size. 

\end{description}


\end{fulllineitems}

\index{nvidia::inferenceserver::client::InferContext::Input::DType (C++ function)@\spxentry{nvidia::inferenceserver::client::InferContext::Input::DType}\spxextra{C++ function}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Input:_CPPv4NK6nvidia15inferenceserver6client12InferContext5Input5DTypeEv}}%
\pysigstartmultiline
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Input:classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Input_1ab90380c6a46ea973a08f86c5a1bb3ea8}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{virtual}} DataType \sphinxbfcode{\sphinxupquote{DType}}}{}{ \sphinxbfcode{\sphinxupquote{const}} = 0}%
\pysigstopmultiline
\begin{description}
\item[{\sphinxstylestrong{Return}}] \leavevmode
The data-type of the input. 

\end{description}


\end{fulllineitems}

\index{nvidia::inferenceserver::client::InferContext::Input::Format (C++ function)@\spxentry{nvidia::inferenceserver::client::InferContext::Input::Format}\spxextra{C++ function}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Input:_CPPv4NK6nvidia15inferenceserver6client12InferContext5Input6FormatEv}}%
\pysigstartmultiline
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Input:classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Input_1a9ab3e04fbb0ad181fd74bf7675249fef}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{virtual}} ModelInput::Format \sphinxbfcode{\sphinxupquote{Format}}}{}{ \sphinxbfcode{\sphinxupquote{const}} = 0}%
\pysigstopmultiline
\begin{description}
\item[{\sphinxstylestrong{Return}}] \leavevmode
The format of the input. 

\end{description}


\end{fulllineitems}

\index{nvidia::inferenceserver::client::InferContext::Input::Dims (C++ function)@\spxentry{nvidia::inferenceserver::client::InferContext::Input::Dims}\spxextra{C++ function}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Input:_CPPv4NK6nvidia15inferenceserver6client12InferContext5Input4DimsEv}}%
\pysigstartmultiline
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Input:classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Input_1a500ee9536afa9d6286cd23ee1308ebda}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{virtual}} \sphinxbfcode{\sphinxupquote{const}} DimsList \&\sphinxbfcode{\sphinxupquote{Dims}}}{}{ \sphinxbfcode{\sphinxupquote{const}} = 0}%
\pysigstopmultiline
\begin{description}
\item[{\sphinxstylestrong{Return}}] \leavevmode
The dimensions/shape of the input specified in the model configuration. Variable-size dimensions are reported as -1. 

\end{description}


\end{fulllineitems}

\index{nvidia::inferenceserver::client::InferContext::Input::Reset (C++ function)@\spxentry{nvidia::inferenceserver::client::InferContext::Input::Reset}\spxextra{C++ function}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Input:_CPPv4N6nvidia15inferenceserver6client12InferContext5Input5ResetEv}}%
\pysigstartmultiline
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Input:classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Input_1a3dee261f7bed6cb3840db299c6b1d246}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{virtual}} {\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1Error:_CPPv4N6nvidia15inferenceserver6client5ErrorE}]{\sphinxcrossref{Error}}} \sphinxbfcode{\sphinxupquote{Reset}}}{}{ = 0}%
\pysigstopmultiline
Prepare this input to receive new tensor values. 

Forget any existing values that were set by previous calls to {\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Input:classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Input_1a414fe9afbb60a48f162bac5439beee4d}]{\sphinxcrossref{\DUrole{std,std-ref}{SetRaw()}}}}. \begin{description}
\item[{\sphinxstylestrong{Return}}] \leavevmode
{\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1Error:classnvidia_1_1inferenceserver_1_1client_1_1Error}]{\sphinxcrossref{\DUrole{std,std-ref}{Error}}}} object indicating success or failure. 

\end{description}


\end{fulllineitems}

\index{nvidia::inferenceserver::client::InferContext::Input::Shape (C++ function)@\spxentry{nvidia::inferenceserver::client::InferContext::Input::Shape}\spxextra{C++ function}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Input:_CPPv4NK6nvidia15inferenceserver6client12InferContext5Input5ShapeEv}}%
\pysigstartmultiline
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Input:classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Input_1aedd9cacfe4e87008521ed63c1eb8e035}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{virtual}} \sphinxbfcode{\sphinxupquote{const}} std::vector\textless{}int64\_t\textgreater{} \&\sphinxbfcode{\sphinxupquote{Shape}}}{}{ \sphinxbfcode{\sphinxupquote{const}} = 0}%
\pysigstopmultiline
Get the shape for this input that was most recently set by SetShape. 

\begin{description}
\item[{\sphinxstylestrong{Return}}] \leavevmode
The shape, or empty vector if SetShape has not been called. 

\end{description}


\end{fulllineitems}

\index{nvidia::inferenceserver::client::InferContext::Input::SetShape (C++ function)@\spxentry{nvidia::inferenceserver::client::InferContext::Input::SetShape}\spxextra{C++ function}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Input:_CPPv4N6nvidia15inferenceserver6client12InferContext5Input8SetShapeERKNSt6vectorI7int64_tEE}}%
\pysigstartmultiline
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Input:classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Input_1a0e2c4e3813ebfda6ecef7e23a7f00de5}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{virtual}} {\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1Error:_CPPv4N6nvidia15inferenceserver6client5ErrorE}]{\sphinxcrossref{Error}}} \sphinxbfcode{\sphinxupquote{SetShape}}}{\sphinxbfcode{\sphinxupquote{const}} std::vector\textless{}int64\_t\textgreater{} \&\sphinxstyleemphasis{dims}}{ = 0}%
\pysigstopmultiline
Set the shape for this input. 

The shape must be set for inputs that have variable-size dimensions and is optional for other inputs. The shape must be set before calling SetRaw or SetFromString. \begin{description}
\item[{\sphinxstylestrong{Return}}] \leavevmode
{\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1Error:classnvidia_1_1inferenceserver_1_1client_1_1Error}]{\sphinxcrossref{\DUrole{std,std-ref}{Error}}}} object indicating success or failure. 

\item[{\sphinxstylestrong{Parameters}}] \leavevmode\begin{itemize}
\item {} 
\sphinxcode{\sphinxupquote{dims}}: The dimensions of the shape. 

\end{itemize}

\end{description}


\end{fulllineitems}

\index{nvidia::inferenceserver::client::InferContext::Input::SetRaw (C++ function)@\spxentry{nvidia::inferenceserver::client::InferContext::Input::SetRaw}\spxextra{C++ function}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Input:_CPPv4N6nvidia15inferenceserver6client12InferContext5Input6SetRawEPK7uint8_t6size_t}}%
\pysigstartmultiline
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Input:classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Input_1a414fe9afbb60a48f162bac5439beee4d}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{virtual}} {\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1Error:_CPPv4N6nvidia15inferenceserver6client5ErrorE}]{\sphinxcrossref{Error}}} \sphinxbfcode{\sphinxupquote{SetRaw}}}{\sphinxbfcode{\sphinxupquote{const}} uint8\_t *\sphinxstyleemphasis{input}, size\_t \sphinxstyleemphasis{input\_byte\_size}}{ = 0}%
\pysigstopmultiline
Set tensor values for this input from a byte array. 

The array is not copied and so it must not be modified or destroyed until this input is no longer needed (that is until the {\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1a4cac8002817c9259851305b14c258f36}]{\sphinxcrossref{\DUrole{std,std-ref}{Run()}}}} call(s) that use the input have completed). For batched inputs this function must be called batch-size times to provide all tensor values for a batch of this input. \begin{description}
\item[{\sphinxstylestrong{Return}}] \leavevmode
{\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1Error:classnvidia_1_1inferenceserver_1_1client_1_1Error}]{\sphinxcrossref{\DUrole{std,std-ref}{Error}}}} object indicating success or failure. 

\item[{\sphinxstylestrong{Parameters}}] \leavevmode\begin{itemize}
\item {} 
\sphinxcode{\sphinxupquote{input}}: The pointer to the array holding the tensor value. 

\item {} 
\sphinxcode{\sphinxupquote{input\_byte\_size}}: The size of the array in bytes, must match the size expected by the input. 

\end{itemize}

\end{description}


\end{fulllineitems}

\index{nvidia::inferenceserver::client::InferContext::Input::SetRaw (C++ function)@\spxentry{nvidia::inferenceserver::client::InferContext::Input::SetRaw}\spxextra{C++ function}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Input:_CPPv4N6nvidia15inferenceserver6client12InferContext5Input6SetRawERKNSt6vectorI7uint8_tEE}}%
\pysigstartmultiline
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Input:classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Input_1a094ec9eece1e10d877db9af69867090e}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{virtual}} {\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1Error:_CPPv4N6nvidia15inferenceserver6client5ErrorE}]{\sphinxcrossref{Error}}} \sphinxbfcode{\sphinxupquote{SetRaw}}}{\sphinxbfcode{\sphinxupquote{const}} std::vector\textless{}uint8\_t\textgreater{} \&\sphinxstyleemphasis{input}}{ = 0}%
\pysigstopmultiline
Set tensor values for this input from a byte vector. 

The vector is not copied and so it must not be modified or destroyed until this input is no longer needed (that is until the {\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1a4cac8002817c9259851305b14c258f36}]{\sphinxcrossref{\DUrole{std,std-ref}{Run()}}}} call(s) that use the input have completed). For batched inputs this function must be called batch-size times to provide all tensor values for a batch of this input. \begin{description}
\item[{\sphinxstylestrong{Return}}] \leavevmode
{\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1Error:classnvidia_1_1inferenceserver_1_1client_1_1Error}]{\sphinxcrossref{\DUrole{std,std-ref}{Error}}}} object indicating success or failure. 

\item[{\sphinxstylestrong{Parameters}}] \leavevmode\begin{itemize}
\item {} 
\sphinxcode{\sphinxupquote{input}}: The vector holding tensor values. 

\end{itemize}

\end{description}


\end{fulllineitems}

\index{nvidia::inferenceserver::client::InferContext::Input::SetFromString (C++ function)@\spxentry{nvidia::inferenceserver::client::InferContext::Input::SetFromString}\spxextra{C++ function}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Input:_CPPv4N6nvidia15inferenceserver6client12InferContext5Input13SetFromStringERKNSt6vectorINSt6stringEEE}}%
\pysigstartmultiline
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Input:classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Input_1a73495802b757c43b67edd334fec341d7}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{virtual}} {\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1Error:_CPPv4N6nvidia15inferenceserver6client5ErrorE}]{\sphinxcrossref{Error}}} \sphinxbfcode{\sphinxupquote{SetFromString}}}{\sphinxbfcode{\sphinxupquote{const}} std::vector\textless{}std::string\textgreater{} \&\sphinxstyleemphasis{input}}{ = 0}%
\pysigstopmultiline
Set tensor values for this input from a vector or strings. 

This method can only be used for tensors with STRING data-type. The strings are assigned in row-major order to the elements of the tensor. The strings are copied and so the ‘input’ does not need to be preserved as with {\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Input:classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Input_1a414fe9afbb60a48f162bac5439beee4d}]{\sphinxcrossref{\DUrole{std,std-ref}{SetRaw()}}}}. For batched inputs this function must be called batch-size times to provide all tensor values for a batch of this input. \begin{description}
\item[{\sphinxstylestrong{Return}}] \leavevmode
{\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1Error:classnvidia_1_1inferenceserver_1_1client_1_1Error}]{\sphinxcrossref{\DUrole{std,std-ref}{Error}}}} object indicating success or failure. 

\item[{\sphinxstylestrong{Parameters}}] \leavevmode\begin{itemize}
\item {} 
\sphinxcode{\sphinxupquote{input}}: The vector holding tensor string values. 

\end{itemize}

\end{description}


\end{fulllineitems}


\end{fulllineitems}



\subsubsection{Class InferContext::Options}
\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Options:class-infercontext-options}}\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Options:exhale-class-classnvidia-1-1inferenceserver-1-1client-1-1infercontext-1-1options}}\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Options::doc}}\begin{itemize}
\item {} 
Defined in {\hyperref[\detokenize{cpp_api/file_src_clients_c++_request.h:file-src-clients-c-request-h}]{\sphinxcrossref{\DUrole{std,std-ref}{File request.h}}}}

\end{itemize}


\paragraph{Nested Relationships}
\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Options:nested-relationships}}
This class is a nested type of {\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:exhale-class-classnvidia-1-1inferenceserver-1-1client-1-1infercontext}]{\sphinxcrossref{\DUrole{std,std-ref}{Class InferContext}}}}.


\paragraph{Class Documentation}
\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Options:class-documentation}}\index{nvidia::inferenceserver::client::InferContext::Options (C++ class)@\spxentry{nvidia::inferenceserver::client::InferContext::Options}\spxextra{C++ class}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Options:_CPPv4N6nvidia15inferenceserver6client12InferContext7OptionsE}}%
\pysigstartmultiline
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Options:classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Options}}\pysigline{\sphinxbfcode{\sphinxupquote{class }}\sphinxbfcode{\sphinxupquote{Options}}}%
\pysigstopmultiline
Run options to be applied to all subsequent {\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1a4cac8002817c9259851305b14c258f36}]{\sphinxcrossref{\DUrole{std,std-ref}{Run()}}}} invocations. 
\subsubsection*{Public Functions}
\index{nvidia::inferenceserver::client::InferContext::Options::\textasciitilde{}Options (C++ function)@\spxentry{nvidia::inferenceserver::client::InferContext::Options::\textasciitilde{}Options}\spxextra{C++ function}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Options:_CPPv4N6nvidia15inferenceserver6client12InferContext7OptionsD0Ev}}%
\pysigstartmultiline
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Options:classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Options_1abcaa16202fc4751a7657808cc4d71884}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{virtual}} \sphinxbfcode{\sphinxupquote{\textasciitilde{}Options}}}{}{}%
\pysigstopmultiline
\end{fulllineitems}

\index{nvidia::inferenceserver::client::InferContext::Options::Flag (C++ function)@\spxentry{nvidia::inferenceserver::client::InferContext::Options::Flag}\spxextra{C++ function}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Options:_CPPv4NK6nvidia15inferenceserver6client12InferContext7Options4FlagEN18InferRequestHeader4FlagE}}%
\pysigstartmultiline
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Options:classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Options_1a23b1ac6a7393c1bad21fc8e7a3373ead}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{virtual}} bool \sphinxbfcode{\sphinxupquote{Flag}}}{InferRequestHeader::Flag \sphinxstyleemphasis{flag}}{ \sphinxbfcode{\sphinxupquote{const}} = 0}%
\pysigstopmultiline
Get the value of a request flag being used for all subsequent inferences. 

Cannot be used with FLAG\_NONE. \begin{description}
\item[{\sphinxstylestrong{Return}}] \leavevmode
The true/false value currently set for the flag. If ‘flag’ is FLAG\_NONE then return false. 

\item[{\sphinxstylestrong{Parameters}}] \leavevmode\begin{itemize}
\item {} 
\sphinxcode{\sphinxupquote{flag}}: The flag to get the value for. 

\end{itemize}

\end{description}


\end{fulllineitems}

\index{nvidia::inferenceserver::client::InferContext::Options::SetFlag (C++ function)@\spxentry{nvidia::inferenceserver::client::InferContext::Options::SetFlag}\spxextra{C++ function}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Options:_CPPv4N6nvidia15inferenceserver6client12InferContext7Options7SetFlagEN18InferRequestHeader4FlagEb}}%
\pysigstartmultiline
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Options:classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Options_1ae12fc02abb1e1f42d1de0e0d4f88c03d}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{virtual}} void \sphinxbfcode{\sphinxupquote{SetFlag}}}{InferRequestHeader::Flag \sphinxstyleemphasis{flag}, bool \sphinxstyleemphasis{value}}{ = 0}%
\pysigstopmultiline
Set a request flag to be used for all subsequent inferences. 

\begin{description}
\item[{\sphinxstylestrong{Parameters}}] \leavevmode\begin{itemize}
\item {} 
\sphinxcode{\sphinxupquote{flag}}: The flag to set. Cannot be used with FLAG\_NONE. 

\item {} 
\sphinxcode{\sphinxupquote{value}}: The true/false value to set for the flag. If ‘flag’ is FLAG\_NONE then do nothing. 

\end{itemize}

\end{description}


\end{fulllineitems}

\index{nvidia::inferenceserver::client::InferContext::Options::Flags (C++ function)@\spxentry{nvidia::inferenceserver::client::InferContext::Options::Flags}\spxextra{C++ function}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Options:_CPPv4NK6nvidia15inferenceserver6client12InferContext7Options5FlagsEv}}%
\pysigstartmultiline
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Options:classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Options_1a51f46889d0f436185379def5a99b67fc}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{virtual}} uint32\_t \sphinxbfcode{\sphinxupquote{Flags}}}{}{ \sphinxbfcode{\sphinxupquote{const}} = 0}%
\pysigstopmultiline
Get the value of all request flags being used for all subsequent inferences. 

\begin{description}
\item[{\sphinxstylestrong{Return}}] \leavevmode
The bitwise-or of flag values as a single uint32\_t value. 

\end{description}


\end{fulllineitems}

\index{nvidia::inferenceserver::client::InferContext::Options::SetFlags (C++ function)@\spxentry{nvidia::inferenceserver::client::InferContext::Options::SetFlags}\spxextra{C++ function}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Options:_CPPv4N6nvidia15inferenceserver6client12InferContext7Options8SetFlagsE8uint32_t}}%
\pysigstartmultiline
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Options:classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Options_1a06defa0f160399a6e81d31bde89cbcc0}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{virtual}} void \sphinxbfcode{\sphinxupquote{SetFlags}}}{uint32\_t \sphinxstyleemphasis{flags}}{ = 0}%
\pysigstopmultiline
Set all request flags to be used for all subsequent inferences. 

\begin{description}
\item[{\sphinxstylestrong{Parameters}}] \leavevmode\begin{itemize}
\item {} 
\sphinxcode{\sphinxupquote{flags}}: The bitwise-or of flag values to set. 

\end{itemize}

\end{description}


\end{fulllineitems}

\index{nvidia::inferenceserver::client::InferContext::Options::BatchSize (C++ function)@\spxentry{nvidia::inferenceserver::client::InferContext::Options::BatchSize}\spxextra{C++ function}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Options:_CPPv4NK6nvidia15inferenceserver6client12InferContext7Options9BatchSizeEv}}%
\pysigstartmultiline
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Options:classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Options_1a6546eb56589394cc4222272afcc23d89}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{virtual}} size\_t \sphinxbfcode{\sphinxupquote{BatchSize}}}{}{ \sphinxbfcode{\sphinxupquote{const}} = 0}%
\pysigstopmultiline
\begin{description}
\item[{\sphinxstylestrong{Return}}] \leavevmode
The batch size to use for all subsequent inferences. 

\end{description}


\end{fulllineitems}

\index{nvidia::inferenceserver::client::InferContext::Options::SetBatchSize (C++ function)@\spxentry{nvidia::inferenceserver::client::InferContext::Options::SetBatchSize}\spxextra{C++ function}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Options:_CPPv4N6nvidia15inferenceserver6client12InferContext7Options12SetBatchSizeE6size_t}}%
\pysigstartmultiline
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Options:classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Options_1a6f390b31a0a8a265c8e15a77f6f9e7c1}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{virtual}} void \sphinxbfcode{\sphinxupquote{SetBatchSize}}}{size\_t \sphinxstyleemphasis{batch\_size}}{ = 0}%
\pysigstopmultiline
Set the batch size to use for all subsequent inferences. 

\begin{description}
\item[{\sphinxstylestrong{Parameters}}] \leavevmode\begin{itemize}
\item {} 
\sphinxcode{\sphinxupquote{batch\_size}}: The batch size. 

\end{itemize}

\end{description}


\end{fulllineitems}

\index{nvidia::inferenceserver::client::InferContext::Options::AddRawResult (C++ function)@\spxentry{nvidia::inferenceserver::client::InferContext::Options::AddRawResult}\spxextra{C++ function}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Options:_CPPv4N6nvidia15inferenceserver6client12InferContext7Options12AddRawResultERKNSt10shared_ptrIN12InferContext6OutputEEE}}%
\pysigstartmultiline
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Options:classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Options_1abb0dceb3499c833684e0dfbe608b3360}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{virtual}} {\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1Error:_CPPv4N6nvidia15inferenceserver6client5ErrorE}]{\sphinxcrossref{Error}}} \sphinxbfcode{\sphinxupquote{AddRawResult}}}{\sphinxbfcode{\sphinxupquote{const}} std::shared\_ptr\textless{}{\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:_CPPv4N6nvidia15inferenceserver6client12InferContextE}]{\sphinxcrossref{InferContext}}}::{\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:_CPPv4N6nvidia15inferenceserver6client12InferContext6OutputE}]{\sphinxcrossref{Output}}}\textgreater{} \&\sphinxstyleemphasis{output}}{ = 0}%
\pysigstopmultiline
Add ‘output’ to the list of requested RAW results. 

{\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1a4cac8002817c9259851305b14c258f36}]{\sphinxcrossref{\DUrole{std,std-ref}{Run()}}}} will return the output’s full tensor as a result. \begin{description}
\item[{\sphinxstylestrong{Return}}] \leavevmode
{\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1Error:classnvidia_1_1inferenceserver_1_1client_1_1Error}]{\sphinxcrossref{\DUrole{std,std-ref}{Error}}}} object indicating success or failure. 

\item[{\sphinxstylestrong{Parameters}}] \leavevmode\begin{itemize}
\item {} 
\sphinxcode{\sphinxupquote{output}}: The output. 

\end{itemize}

\end{description}


\end{fulllineitems}

\index{nvidia::inferenceserver::client::InferContext::Options::AddClassResult (C++ function)@\spxentry{nvidia::inferenceserver::client::InferContext::Options::AddClassResult}\spxextra{C++ function}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Options:_CPPv4N6nvidia15inferenceserver6client12InferContext7Options14AddClassResultERKNSt10shared_ptrIN12InferContext6OutputEEE8uint64_t}}%
\pysigstartmultiline
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Options:classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Options_1a5b584e6fa3d2867d1ec4ce7a0bfee809}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{virtual}} {\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1Error:_CPPv4N6nvidia15inferenceserver6client5ErrorE}]{\sphinxcrossref{Error}}} \sphinxbfcode{\sphinxupquote{AddClassResult}}}{\sphinxbfcode{\sphinxupquote{const}} std::shared\_ptr\textless{}{\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:_CPPv4N6nvidia15inferenceserver6client12InferContextE}]{\sphinxcrossref{InferContext}}}::{\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:_CPPv4N6nvidia15inferenceserver6client12InferContext6OutputE}]{\sphinxcrossref{Output}}}\textgreater{} \&\sphinxstyleemphasis{output}, uint64\_t \sphinxstyleemphasis{k}}{ = 0}%
\pysigstopmultiline
Add ‘output’ to the list of requested CLASS results. 

{\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1a4cac8002817c9259851305b14c258f36}]{\sphinxcrossref{\DUrole{std,std-ref}{Run()}}}} will return the highest ‘k’ values of ‘output’ as a result. \begin{description}
\item[{\sphinxstylestrong{Return}}] \leavevmode
{\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1Error:classnvidia_1_1inferenceserver_1_1client_1_1Error}]{\sphinxcrossref{\DUrole{std,std-ref}{Error}}}} object indicating success or failure. 

\item[{\sphinxstylestrong{Parameters}}] \leavevmode\begin{itemize}
\item {} 
\sphinxcode{\sphinxupquote{output}}: The output. 

\item {} 
\sphinxcode{\sphinxupquote{k}}: Set how many class results to return for the output. 

\end{itemize}

\end{description}


\end{fulllineitems}

\subsubsection*{Public Static Functions}
\index{nvidia::inferenceserver::client::InferContext::Options::Create (C++ function)@\spxentry{nvidia::inferenceserver::client::InferContext::Options::Create}\spxextra{C++ function}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Options:_CPPv4N6nvidia15inferenceserver6client12InferContext7Options6CreateEPNSt10unique_ptrI7OptionsEE}}%
\pysigstartmultiline
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Options:classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Options_1a63d97ebc1993c0034ea6822c1d7600b1}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{static}} {\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1Error:_CPPv4N6nvidia15inferenceserver6client5ErrorE}]{\sphinxcrossref{Error}}} \sphinxbfcode{\sphinxupquote{Create}}}{std::unique\_ptr\textless{}{\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:_CPPv4N6nvidia15inferenceserver6client12InferContext7OptionsE}]{\sphinxcrossref{Options}}}\textgreater{} *\sphinxstyleemphasis{options}}{}%
\pysigstopmultiline
Create a new {\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Options:classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Options}]{\sphinxcrossref{\DUrole{std,std-ref}{Options}}}} object with default values. 

\begin{description}
\item[{\sphinxstylestrong{Return}}] \leavevmode
{\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1Error:classnvidia_1_1inferenceserver_1_1client_1_1Error}]{\sphinxcrossref{\DUrole{std,std-ref}{Error}}}} object indicating success or failure. 

\end{description}


\end{fulllineitems}


\end{fulllineitems}



\subsubsection{Class InferContext::Output}
\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Output:class-infercontext-output}}\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Output:exhale-class-classnvidia-1-1inferenceserver-1-1client-1-1infercontext-1-1output}}\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Output::doc}}\begin{itemize}
\item {} 
Defined in {\hyperref[\detokenize{cpp_api/file_src_clients_c++_request.h:file-src-clients-c-request-h}]{\sphinxcrossref{\DUrole{std,std-ref}{File request.h}}}}

\end{itemize}


\paragraph{Nested Relationships}
\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Output:nested-relationships}}
This class is a nested type of {\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:exhale-class-classnvidia-1-1inferenceserver-1-1client-1-1infercontext}]{\sphinxcrossref{\DUrole{std,std-ref}{Class InferContext}}}}.


\paragraph{Class Documentation}
\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Output:class-documentation}}\index{nvidia::inferenceserver::client::InferContext::Output (C++ class)@\spxentry{nvidia::inferenceserver::client::InferContext::Output}\spxextra{C++ class}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Output:_CPPv4N6nvidia15inferenceserver6client12InferContext6OutputE}}%
\pysigstartmultiline
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Output:classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Output}}\pysigline{\sphinxbfcode{\sphinxupquote{class }}\sphinxbfcode{\sphinxupquote{Output}}}%
\pysigstopmultiline
An output from the model. 
\subsubsection*{Public Functions}
\index{nvidia::inferenceserver::client::InferContext::Output::\textasciitilde{}Output (C++ function)@\spxentry{nvidia::inferenceserver::client::InferContext::Output::\textasciitilde{}Output}\spxextra{C++ function}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Output:_CPPv4N6nvidia15inferenceserver6client12InferContext6OutputD0Ev}}%
\pysigstartmultiline
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Output:classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Output_1a8a685059d16e4a77fb8692dd82de0ba0}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{virtual}} \sphinxbfcode{\sphinxupquote{\textasciitilde{}Output}}}{}{}%
\pysigstopmultiline
Destroy the output. 

\end{fulllineitems}

\index{nvidia::inferenceserver::client::InferContext::Output::Name (C++ function)@\spxentry{nvidia::inferenceserver::client::InferContext::Output::Name}\spxextra{C++ function}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Output:_CPPv4NK6nvidia15inferenceserver6client12InferContext6Output4NameEv}}%
\pysigstartmultiline
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Output:classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Output_1a72fd5ea270c62500d6c182c4f2a035ab}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{virtual}} \sphinxbfcode{\sphinxupquote{const}} std::string \&\sphinxbfcode{\sphinxupquote{Name}}}{}{ \sphinxbfcode{\sphinxupquote{const}} = 0}%
\pysigstopmultiline
\begin{description}
\item[{\sphinxstylestrong{Return}}] \leavevmode
The name of the output. 

\end{description}


\end{fulllineitems}

\index{nvidia::inferenceserver::client::InferContext::Output::DType (C++ function)@\spxentry{nvidia::inferenceserver::client::InferContext::Output::DType}\spxextra{C++ function}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Output:_CPPv4NK6nvidia15inferenceserver6client12InferContext6Output5DTypeEv}}%
\pysigstartmultiline
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Output:classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Output_1a66c02cfce60941e575c6583ad160896c}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{virtual}} DataType \sphinxbfcode{\sphinxupquote{DType}}}{}{ \sphinxbfcode{\sphinxupquote{const}} = 0}%
\pysigstopmultiline
\begin{description}
\item[{\sphinxstylestrong{Return}}] \leavevmode
The data-type of the output. 

\end{description}


\end{fulllineitems}

\index{nvidia::inferenceserver::client::InferContext::Output::Dims (C++ function)@\spxentry{nvidia::inferenceserver::client::InferContext::Output::Dims}\spxextra{C++ function}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Output:_CPPv4NK6nvidia15inferenceserver6client12InferContext6Output4DimsEv}}%
\pysigstartmultiline
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Output:classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Output_1a21f73c9cf17d65831cdbecf904993d05}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{virtual}} \sphinxbfcode{\sphinxupquote{const}} DimsList \&\sphinxbfcode{\sphinxupquote{Dims}}}{}{ \sphinxbfcode{\sphinxupquote{const}} = 0}%
\pysigstopmultiline
\begin{description}
\item[{\sphinxstylestrong{Return}}] \leavevmode
The dimensions/shape of the output specified in the model configuration. Variable-size dimensions are reported as -1. 

\end{description}


\end{fulllineitems}


\end{fulllineitems}



\subsubsection{Class InferContext::Request}
\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Request:class-infercontext-request}}\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Request:exhale-class-classnvidia-1-1inferenceserver-1-1client-1-1infercontext-1-1request}}\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Request::doc}}\begin{itemize}
\item {} 
Defined in {\hyperref[\detokenize{cpp_api/file_src_clients_c++_request.h:file-src-clients-c-request-h}]{\sphinxcrossref{\DUrole{std,std-ref}{File request.h}}}}

\end{itemize}


\paragraph{Nested Relationships}
\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Request:nested-relationships}}
This class is a nested type of {\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:exhale-class-classnvidia-1-1inferenceserver-1-1client-1-1infercontext}]{\sphinxcrossref{\DUrole{std,std-ref}{Class InferContext}}}}.


\paragraph{Class Documentation}
\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Request:class-documentation}}\index{nvidia::inferenceserver::client::InferContext::Request (C++ class)@\spxentry{nvidia::inferenceserver::client::InferContext::Request}\spxextra{C++ class}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Request:_CPPv4N6nvidia15inferenceserver6client12InferContext7RequestE}}%
\pysigstartmultiline
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Request:classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Request}}\pysigline{\sphinxbfcode{\sphinxupquote{class }}\sphinxbfcode{\sphinxupquote{Request}}}%
\pysigstopmultiline
Handle to a inference request. 

The request handle is used to get request results if the request is sent by {\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1a255a59df29a53d6e45ce86663887615f}]{\sphinxcrossref{\DUrole{std,std-ref}{AsyncRun()}}}}. 
\subsubsection*{Public Functions}
\index{nvidia::inferenceserver::client::InferContext::Request::\textasciitilde{}Request (C++ function)@\spxentry{nvidia::inferenceserver::client::InferContext::Request::\textasciitilde{}Request}\spxextra{C++ function}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Request:_CPPv4N6nvidia15inferenceserver6client12InferContext7RequestD0Ev}}%
\pysigstartmultiline
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Request:classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Request_1a3146daca9aed389d56fcbd71123f7206}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{virtual}} \sphinxbfcode{\sphinxupquote{\textasciitilde{}Request}}}{}{}%
\pysigstopmultiline
Destroy the request handle. 

\end{fulllineitems}

\index{nvidia::inferenceserver::client::InferContext::Request::Id (C++ function)@\spxentry{nvidia::inferenceserver::client::InferContext::Request::Id}\spxextra{C++ function}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Request:_CPPv4NK6nvidia15inferenceserver6client12InferContext7Request2IdEv}}%
\pysigstartmultiline
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Request:classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Request_1a796b6c3cac55f00c822f3c1ced007509}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{virtual}} uint64\_t \sphinxbfcode{\sphinxupquote{Id}}}{}{ \sphinxbfcode{\sphinxupquote{const}} = 0}%
\pysigstopmultiline
\begin{description}
\item[{\sphinxstylestrong{Return}}] \leavevmode
The unique identifier of the request. 

\end{description}


\end{fulllineitems}


\end{fulllineitems}



\subsubsection{Class InferContext::RequestTimers}
\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1RequestTimers:class-infercontext-requesttimers}}\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1RequestTimers:exhale-class-classnvidia-1-1inferenceserver-1-1client-1-1infercontext-1-1requesttimers}}\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1RequestTimers::doc}}\begin{itemize}
\item {} 
Defined in {\hyperref[\detokenize{cpp_api/file_src_clients_c++_request.h:file-src-clients-c-request-h}]{\sphinxcrossref{\DUrole{std,std-ref}{File request.h}}}}

\end{itemize}


\paragraph{Nested Relationships}
\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1RequestTimers:nested-relationships}}
This class is a nested type of {\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:exhale-class-classnvidia-1-1inferenceserver-1-1client-1-1infercontext}]{\sphinxcrossref{\DUrole{std,std-ref}{Class InferContext}}}}.


\paragraph{Class Documentation}
\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1RequestTimers:class-documentation}}\index{nvidia::inferenceserver::client::InferContext::RequestTimers (C++ class)@\spxentry{nvidia::inferenceserver::client::InferContext::RequestTimers}\spxextra{C++ class}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1RequestTimers:_CPPv4N6nvidia15inferenceserver6client12InferContext13RequestTimersE}}%
\pysigstartmultiline
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1RequestTimers:classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1RequestTimers}}\pysigline{\sphinxbfcode{\sphinxupquote{class }}\sphinxbfcode{\sphinxupquote{RequestTimers}}}%
\pysigstopmultiline
Timer to record the timestamp for different stages of request handling. 
\subsubsection*{Public Types}
\index{nvidia::inferenceserver::client::InferContext::RequestTimers::Kind (C++ type)@\spxentry{nvidia::inferenceserver::client::InferContext::RequestTimers::Kind}\spxextra{C++ type}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1RequestTimers:_CPPv4N6nvidia15inferenceserver6client12InferContext13RequestTimers4KindE}}%
\pysigstartmultiline
\pysigline{\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1RequestTimers:classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1RequestTimers_1ad3d72a1b49c1e9ebb2bbeb15e04401f8}}\sphinxbfcode{\sphinxupquote{enum }}\sphinxbfcode{\sphinxupquote{Kind}}}%
\pysigstopmultiline
The kind of the timer. 

\sphinxstyleemphasis{Values:}
\index{nvidia::inferenceserver::client::InferContext::RequestTimers::REQUEST\_START (C++ enumerator)@\spxentry{nvidia::inferenceserver::client::InferContext::RequestTimers::REQUEST\_START}\spxextra{C++ enumerator}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1RequestTimers:_CPPv4N6nvidia15inferenceserver6client12InferContext13RequestTimers13REQUEST_STARTE}}%
\pysigstartmultiline
\pysigline{\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1RequestTimers:classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1RequestTimers_1ad3d72a1b49c1e9ebb2bbeb15e04401f8a2ca0e66d9f91476f9f00f61c5a834cbb}}\sphinxbfcode{\sphinxupquote{REQUEST\_START}}}%
\pysigstopmultiline
The start of request handling. 

\end{fulllineitems}

\index{nvidia::inferenceserver::client::InferContext::RequestTimers::REQUEST\_END (C++ enumerator)@\spxentry{nvidia::inferenceserver::client::InferContext::RequestTimers::REQUEST\_END}\spxextra{C++ enumerator}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1RequestTimers:_CPPv4N6nvidia15inferenceserver6client12InferContext13RequestTimers11REQUEST_ENDE}}%
\pysigstartmultiline
\pysigline{\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1RequestTimers:classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1RequestTimers_1ad3d72a1b49c1e9ebb2bbeb15e04401f8ad793ca80c1d0010ecc2f269b6a4b621a}}\sphinxbfcode{\sphinxupquote{REQUEST\_END}}}%
\pysigstopmultiline
The end of request handling. 

\end{fulllineitems}

\index{nvidia::inferenceserver::client::InferContext::RequestTimers::SEND\_START (C++ enumerator)@\spxentry{nvidia::inferenceserver::client::InferContext::RequestTimers::SEND\_START}\spxextra{C++ enumerator}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1RequestTimers:_CPPv4N6nvidia15inferenceserver6client12InferContext13RequestTimers10SEND_STARTE}}%
\pysigstartmultiline
\pysigline{\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1RequestTimers:classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1RequestTimers_1ad3d72a1b49c1e9ebb2bbeb15e04401f8a4cb619f22003a3f9334144b07c71d1f3}}\sphinxbfcode{\sphinxupquote{SEND\_START}}}%
\pysigstopmultiline
The start of sending request bytes to the server (i.e. first byte). 

\end{fulllineitems}

\index{nvidia::inferenceserver::client::InferContext::RequestTimers::SEND\_END (C++ enumerator)@\spxentry{nvidia::inferenceserver::client::InferContext::RequestTimers::SEND\_END}\spxextra{C++ enumerator}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1RequestTimers:_CPPv4N6nvidia15inferenceserver6client12InferContext13RequestTimers8SEND_ENDE}}%
\pysigstartmultiline
\pysigline{\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1RequestTimers:classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1RequestTimers_1ad3d72a1b49c1e9ebb2bbeb15e04401f8a8330cc174bf610d2c3afaa2d68e535b2}}\sphinxbfcode{\sphinxupquote{SEND\_END}}}%
\pysigstopmultiline
The end of sending request bytes to the server (i.e. last byte). 

\end{fulllineitems}

\index{nvidia::inferenceserver::client::InferContext::RequestTimers::RECEIVE\_START (C++ enumerator)@\spxentry{nvidia::inferenceserver::client::InferContext::RequestTimers::RECEIVE\_START}\spxextra{C++ enumerator}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1RequestTimers:_CPPv4N6nvidia15inferenceserver6client12InferContext13RequestTimers13RECEIVE_STARTE}}%
\pysigstartmultiline
\pysigline{\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1RequestTimers:classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1RequestTimers_1ad3d72a1b49c1e9ebb2bbeb15e04401f8aec6eb4b12033d2b794c2a0f3d31c2930}}\sphinxbfcode{\sphinxupquote{RECEIVE\_START}}}%
\pysigstopmultiline
The start of receiving response bytes from the server (i.e. 

first byte). 

\end{fulllineitems}

\index{nvidia::inferenceserver::client::InferContext::RequestTimers::RECEIVE\_END (C++ enumerator)@\spxentry{nvidia::inferenceserver::client::InferContext::RequestTimers::RECEIVE\_END}\spxextra{C++ enumerator}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1RequestTimers:_CPPv4N6nvidia15inferenceserver6client12InferContext13RequestTimers11RECEIVE_ENDE}}%
\pysigstartmultiline
\pysigline{\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1RequestTimers:classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1RequestTimers_1ad3d72a1b49c1e9ebb2bbeb15e04401f8a66feb30d6ad2348be0c107324aceb3b7}}\sphinxbfcode{\sphinxupquote{RECEIVE\_END}}}%
\pysigstopmultiline
The end of receiving response bytes from the server (i.e. 

last byte). 

\end{fulllineitems}


\end{fulllineitems}

\subsubsection*{Public Functions}
\index{nvidia::inferenceserver::client::InferContext::RequestTimers::RequestTimers (C++ function)@\spxentry{nvidia::inferenceserver::client::InferContext::RequestTimers::RequestTimers}\spxextra{C++ function}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1RequestTimers:_CPPv4N6nvidia15inferenceserver6client12InferContext13RequestTimers13RequestTimersEv}}%
\pysigstartmultiline
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1RequestTimers:classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1RequestTimers_1ac2935983f1de4bd33820357823c36e2f}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{RequestTimers}}}{}{}%
\pysigstopmultiline
Construct a timer with zero-ed timestamps. 

\end{fulllineitems}

\index{nvidia::inferenceserver::client::InferContext::RequestTimers::Reset (C++ function)@\spxentry{nvidia::inferenceserver::client::InferContext::RequestTimers::Reset}\spxextra{C++ function}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1RequestTimers:_CPPv4N6nvidia15inferenceserver6client12InferContext13RequestTimers5ResetEv}}%
\pysigstartmultiline
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1RequestTimers:classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1RequestTimers_1aa5ace810b5d34b9f6c3a9ae9a3d21dda}}\pysiglinewithargsret{{\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1Error:_CPPv4N6nvidia15inferenceserver6client5ErrorE}]{\sphinxcrossref{Error}}} \sphinxbfcode{\sphinxupquote{Reset}}}{}{}%
\pysigstopmultiline
Reset all timestamp values to zero. 

Must be called before re-using the timer. \begin{description}
\item[{\sphinxstylestrong{Return}}] \leavevmode
{\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1Error:classnvidia_1_1inferenceserver_1_1client_1_1Error}]{\sphinxcrossref{\DUrole{std,std-ref}{Error}}}} object indicating success or failure. 

\end{description}


\end{fulllineitems}

\index{nvidia::inferenceserver::client::InferContext::RequestTimers::Record (C++ function)@\spxentry{nvidia::inferenceserver::client::InferContext::RequestTimers::Record}\spxextra{C++ function}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1RequestTimers:_CPPv4N6nvidia15inferenceserver6client12InferContext13RequestTimers6RecordE4Kind}}%
\pysigstartmultiline
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1RequestTimers:classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1RequestTimers_1ab15f971203dcbd5ed114709318f2232b}}\pysiglinewithargsret{{\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1Error:_CPPv4N6nvidia15inferenceserver6client5ErrorE}]{\sphinxcrossref{Error}}} \sphinxbfcode{\sphinxupquote{Record}}}{{\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1RequestTimers:_CPPv4N6nvidia15inferenceserver6client12InferContext13RequestTimers4KindE}]{\sphinxcrossref{Kind}}} \sphinxstyleemphasis{kind}}{}%
\pysigstopmultiline
Record the current timestamp for a request stage. 

\begin{description}
\item[{\sphinxstylestrong{Return}}] \leavevmode
{\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1Error:classnvidia_1_1inferenceserver_1_1client_1_1Error}]{\sphinxcrossref{\DUrole{std,std-ref}{Error}}}} object indicating success or failure. 

\item[{\sphinxstylestrong{Parameters}}] \leavevmode\begin{itemize}
\item {} 
\sphinxcode{\sphinxupquote{kind}}: The Kind of the timestamp. 

\end{itemize}

\end{description}


\end{fulllineitems}


\end{fulllineitems}



\subsubsection{Class InferContext::Result}
\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result:class-infercontext-result}}\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result:exhale-class-classnvidia-1-1inferenceserver-1-1client-1-1infercontext-1-1result}}\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result::doc}}\begin{itemize}
\item {} 
Defined in {\hyperref[\detokenize{cpp_api/file_src_clients_c++_request.h:file-src-clients-c-request-h}]{\sphinxcrossref{\DUrole{std,std-ref}{File request.h}}}}

\end{itemize}


\paragraph{Nested Relationships}
\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result:nested-relationships}}
This class is a nested type of {\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:exhale-class-classnvidia-1-1inferenceserver-1-1client-1-1infercontext}]{\sphinxcrossref{\DUrole{std,std-ref}{Class InferContext}}}}.


\subparagraph{Nested Types}
\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result:nested-types}}\begin{itemize}
\item {} 
{\hyperref[\detokenize{cpp_api/structnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result_1_1ClassResult:exhale-struct-structnvidia-1-1inferenceserver-1-1client-1-1infercontext-1-1result-1-1classresult}]{\sphinxcrossref{\DUrole{std,std-ref}{Struct Result::ClassResult}}}}

\end{itemize}


\paragraph{Class Documentation}
\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result:class-documentation}}\index{nvidia::inferenceserver::client::InferContext::Result (C++ class)@\spxentry{nvidia::inferenceserver::client::InferContext::Result}\spxextra{C++ class}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result:_CPPv4N6nvidia15inferenceserver6client12InferContext6ResultE}}%
\pysigstartmultiline
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result:classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result}}\pysigline{\sphinxbfcode{\sphinxupquote{class }}\sphinxbfcode{\sphinxupquote{Result}}}%
\pysigstopmultiline
An inference result corresponding to an output. 
\subsubsection*{Public Types}
\index{nvidia::inferenceserver::client::InferContext::Result::ResultFormat (C++ type)@\spxentry{nvidia::inferenceserver::client::InferContext::Result::ResultFormat}\spxextra{C++ type}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result:_CPPv4N6nvidia15inferenceserver6client12InferContext6Result12ResultFormatE}}%
\pysigstartmultiline
\pysigline{\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result:classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result_1a8bb9d36d1808a030dda919af87746dad}}\sphinxbfcode{\sphinxupquote{enum }}\sphinxbfcode{\sphinxupquote{ResultFormat}}}%
\pysigstopmultiline
Format in which result is returned. 

\sphinxstyleemphasis{Values:}
\index{nvidia::inferenceserver::client::InferContext::Result::RAW (C++ enumerator)@\spxentry{nvidia::inferenceserver::client::InferContext::Result::RAW}\spxextra{C++ enumerator}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result:_CPPv4N6nvidia15inferenceserver6client12InferContext6Result3RAWE}}%
\pysigstartmultiline
\pysigline{\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result:classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result_1a8bb9d36d1808a030dda919af87746dadada3b38eab85f839509454eb2f3df8ea6}}\sphinxbfcode{\sphinxupquote{RAW}} = 0}%
\pysigstopmultiline
RAW format is the entire result tensor of values. 

\end{fulllineitems}

\index{nvidia::inferenceserver::client::InferContext::Result::CLASS (C++ enumerator)@\spxentry{nvidia::inferenceserver::client::InferContext::Result::CLASS}\spxextra{C++ enumerator}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result:_CPPv4N6nvidia15inferenceserver6client12InferContext6Result5CLASSE}}%
\pysigstartmultiline
\pysigline{\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result:classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result_1a8bb9d36d1808a030dda919af87746dada9f91171ba1a8902be2821d6ba488dbd4}}\sphinxbfcode{\sphinxupquote{CLASS}} = 1}%
\pysigstopmultiline
CLASS format is the top-k highest probability values of the result and the associated class label (if provided by the model). 

\end{fulllineitems}


\end{fulllineitems}

\subsubsection*{Public Functions}
\index{nvidia::inferenceserver::client::InferContext::Result::\textasciitilde{}Result (C++ function)@\spxentry{nvidia::inferenceserver::client::InferContext::Result::\textasciitilde{}Result}\spxextra{C++ function}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result:_CPPv4N6nvidia15inferenceserver6client12InferContext6ResultD0Ev}}%
\pysigstartmultiline
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result:classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result_1aa87dddd7be7a18f787946a6548398d3b}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{virtual}} \sphinxbfcode{\sphinxupquote{\textasciitilde{}Result}}}{}{}%
\pysigstopmultiline
Destroy the result. 

\end{fulllineitems}

\index{nvidia::inferenceserver::client::InferContext::Result::ModelName (C++ function)@\spxentry{nvidia::inferenceserver::client::InferContext::Result::ModelName}\spxextra{C++ function}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result:_CPPv4NK6nvidia15inferenceserver6client12InferContext6Result9ModelNameEv}}%
\pysigstartmultiline
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result:classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result_1a78206cf92579339875ad3a44318a28f7}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{virtual}} \sphinxbfcode{\sphinxupquote{const}} std::string \&\sphinxbfcode{\sphinxupquote{ModelName}}}{}{ \sphinxbfcode{\sphinxupquote{const}} = 0}%
\pysigstopmultiline
\begin{description}
\item[{\sphinxstylestrong{Return}}] \leavevmode
The name of the model that produced this result. 

\end{description}


\end{fulllineitems}

\index{nvidia::inferenceserver::client::InferContext::Result::ModelVersion (C++ function)@\spxentry{nvidia::inferenceserver::client::InferContext::Result::ModelVersion}\spxextra{C++ function}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result:_CPPv4NK6nvidia15inferenceserver6client12InferContext6Result12ModelVersionEv}}%
\pysigstartmultiline
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result:classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result_1a9a81d8a1588920dc784fcb4b2520f566}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{virtual}} int64\_t \sphinxbfcode{\sphinxupquote{ModelVersion}}}{}{ \sphinxbfcode{\sphinxupquote{const}} = 0}%
\pysigstopmultiline
\begin{description}
\item[{\sphinxstylestrong{Return}}] \leavevmode
The version of the model that produced this result. 

\end{description}


\end{fulllineitems}

\index{nvidia::inferenceserver::client::InferContext::Result::GetOutput (C++ function)@\spxentry{nvidia::inferenceserver::client::InferContext::Result::GetOutput}\spxextra{C++ function}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result:_CPPv4NK6nvidia15inferenceserver6client12InferContext6Result9GetOutputEv}}%
\pysigstartmultiline
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result:classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result_1a7687d7d07c181ba27f32654341390a92}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{virtual}} \sphinxbfcode{\sphinxupquote{const}} std::shared\_ptr\textless{}{\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:_CPPv4N6nvidia15inferenceserver6client12InferContext6OutputE}]{\sphinxcrossref{Output}}}\textgreater{} \sphinxbfcode{\sphinxupquote{GetOutput}}}{}{ \sphinxbfcode{\sphinxupquote{const}} = 0}%
\pysigstopmultiline
\begin{description}
\item[{\sphinxstylestrong{Return}}] \leavevmode
The {\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Output:classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Output}]{\sphinxcrossref{\DUrole{std,std-ref}{Output}}}} object corresponding to this result. 

\end{description}


\end{fulllineitems}

\index{nvidia::inferenceserver::client::InferContext::Result::GetRawShape (C++ function)@\spxentry{nvidia::inferenceserver::client::InferContext::Result::GetRawShape}\spxextra{C++ function}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result:_CPPv4NK6nvidia15inferenceserver6client12InferContext6Result11GetRawShapeEPNSt6vectorI7int64_tEE}}%
\pysigstartmultiline
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result:classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result_1aeeec1b3448cf681e3ec8ac3c3c589374}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{virtual}} {\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1Error:_CPPv4N6nvidia15inferenceserver6client5ErrorE}]{\sphinxcrossref{Error}}} \sphinxbfcode{\sphinxupquote{GetRawShape}}}{std::vector\textless{}int64\_t\textgreater{} *\sphinxstyleemphasis{shape}}{ \sphinxbfcode{\sphinxupquote{const}} = 0}%
\pysigstopmultiline
Get the shape of a raw result. 

The shape does not include the batch dimension. \begin{description}
\item[{\sphinxstylestrong{Return}}] \leavevmode
{\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1Error:classnvidia_1_1inferenceserver_1_1client_1_1Error}]{\sphinxcrossref{\DUrole{std,std-ref}{Error}}}} object indicating success or failure. 

\item[{\sphinxstylestrong{Parameters}}] \leavevmode\begin{itemize}
\item {} 
\sphinxcode{\sphinxupquote{shape}}: Returns the shape. 

\end{itemize}

\end{description}


\end{fulllineitems}

\index{nvidia::inferenceserver::client::InferContext::Result::GetRaw (C++ function)@\spxentry{nvidia::inferenceserver::client::InferContext::Result::GetRaw}\spxextra{C++ function}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result:_CPPv4NK6nvidia15inferenceserver6client12InferContext6Result6GetRawE6size_tPPKNSt6vectorI7uint8_tEE}}%
\pysigstartmultiline
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result:classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result_1af97122884c92553992261c501dbe3095}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{virtual}} {\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1Error:_CPPv4N6nvidia15inferenceserver6client5ErrorE}]{\sphinxcrossref{Error}}} \sphinxbfcode{\sphinxupquote{GetRaw}}}{size\_t \sphinxstyleemphasis{batch\_idx}, \sphinxbfcode{\sphinxupquote{const}} std::vector\textless{}uint8\_t\textgreater{} **\sphinxstyleemphasis{buf}}{ \sphinxbfcode{\sphinxupquote{const}} = 0}%
\pysigstopmultiline
Get a reference to entire raw result data for a specific batch entry. 

Returns error if this result is not RAW format. \begin{description}
\item[{\sphinxstylestrong{Return}}] \leavevmode
{\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1Error:classnvidia_1_1inferenceserver_1_1client_1_1Error}]{\sphinxcrossref{\DUrole{std,std-ref}{Error}}}} object indicating success or failure. 

\item[{\sphinxstylestrong{Parameters}}] \leavevmode\begin{itemize}
\item {} 
\sphinxcode{\sphinxupquote{batch\_idx}}: Returns the results for this entry of the batch. 

\item {} 
\sphinxcode{\sphinxupquote{buf}}: Returns the vector of result bytes. 

\end{itemize}

\end{description}


\end{fulllineitems}

\index{nvidia::inferenceserver::client::InferContext::Result::GetRawAtCursor (C++ function)@\spxentry{nvidia::inferenceserver::client::InferContext::Result::GetRawAtCursor}\spxextra{C++ function}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result:_CPPv4N6nvidia15inferenceserver6client12InferContext6Result14GetRawAtCursorE6size_tPPK7uint8_t6size_t}}%
\pysigstartmultiline
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result:classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result_1a2a13887812b025896d569f041db7e2c8}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{virtual}} {\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1Error:_CPPv4N6nvidia15inferenceserver6client5ErrorE}]{\sphinxcrossref{Error}}} \sphinxbfcode{\sphinxupquote{GetRawAtCursor}}}{size\_t \sphinxstyleemphasis{batch\_idx}, \sphinxbfcode{\sphinxupquote{const}} uint8\_t **\sphinxstyleemphasis{buf}, size\_t \sphinxstyleemphasis{adv\_byte\_size}}{ = 0}%
\pysigstopmultiline
Get a reference to raw result data for a specific batch entry at the current “cursor” and advance the cursor by the specified number of bytes. 

More typically use {\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result:classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result_1a2a13887812b025896d569f041db7e2c8}]{\sphinxcrossref{\DUrole{std,std-ref}{GetRawAtCursor\textless{}T\textgreater{}()}}}} method to return the data as a specific type T. Use {\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result:classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result_1adb27437d415b569a3a4b7700c56002bf}]{\sphinxcrossref{\DUrole{std,std-ref}{ResetCursor()}}}} to reset the cursor to the beginning of the result. Returns error if this result is not RAW format. \begin{description}
\item[{\sphinxstylestrong{Return}}] \leavevmode
{\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1Error:classnvidia_1_1inferenceserver_1_1client_1_1Error}]{\sphinxcrossref{\DUrole{std,std-ref}{Error}}}} object indicating success or failure. 

\item[{\sphinxstylestrong{Parameters}}] \leavevmode\begin{itemize}
\item {} 
\sphinxcode{\sphinxupquote{batch\_idx}}: Returns results for this entry of the batch. 

\item {} 
\sphinxcode{\sphinxupquote{buf}}: Returns pointer to ‘adv\_byte\_size’ bytes of data. 

\item {} 
\sphinxcode{\sphinxupquote{adv\_byte\_size}}: The number of bytes of data to get a reference to. 

\end{itemize}

\end{description}


\end{fulllineitems}

\index{nvidia::inferenceserver::client::InferContext::Result::GetRawAtCursor (C++ function)@\spxentry{nvidia::inferenceserver::client::InferContext::Result::GetRawAtCursor}\spxextra{C++ function}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result:_CPPv4N6nvidia15inferenceserver6client12InferContext6Result14GetRawAtCursorE6size_tP1T}}%
\pysigstartmultiline
\pysigline{\sphinxbfcode{\sphinxupquote{template }}\textless{}\sphinxbfcode{\sphinxupquote{typename}} T\textgreater{}}\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result:classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result_1a64d951fd64068b9d2e3da103f47390fe}}\pysiglinewithargsret{{\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1Error:_CPPv4N6nvidia15inferenceserver6client5ErrorE}]{\sphinxcrossref{Error}}} \sphinxbfcode{\sphinxupquote{GetRawAtCursor}}}{size\_t \sphinxstyleemphasis{batch\_idx}, T *\sphinxstyleemphasis{out}}{}%
\pysigstopmultiline
Read a value for a specific batch entry at the current “cursor” from the result tensor as the specified type T and advance the cursor. 

Use {\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result:classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result_1adb27437d415b569a3a4b7700c56002bf}]{\sphinxcrossref{\DUrole{std,std-ref}{ResetCursor()}}}} to reset the cursor to the beginning of the result. Returns error if this result is not RAW format. \begin{description}
\item[{\sphinxstylestrong{Return}}] \leavevmode
{\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1Error:classnvidia_1_1inferenceserver_1_1client_1_1Error}]{\sphinxcrossref{\DUrole{std,std-ref}{Error}}}} object indicating success or failure. 

\item[{\sphinxstylestrong{Parameters}}] \leavevmode\begin{itemize}
\item {} 
\sphinxcode{\sphinxupquote{batch\_idx}}: Returns results for this entry of the batch. 

\item {} 
\sphinxcode{\sphinxupquote{out}}: Returns the value at the cursor. 

\end{itemize}

\end{description}


\end{fulllineitems}

\index{nvidia::inferenceserver::client::InferContext::Result::GetClassCount (C++ function)@\spxentry{nvidia::inferenceserver::client::InferContext::Result::GetClassCount}\spxextra{C++ function}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result:_CPPv4NK6nvidia15inferenceserver6client12InferContext6Result13GetClassCountE6size_tP6size_t}}%
\pysigstartmultiline
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result:classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result_1ac50ae04862c58a004547cac5debc7c98}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{virtual}} {\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1Error:_CPPv4N6nvidia15inferenceserver6client5ErrorE}]{\sphinxcrossref{Error}}} \sphinxbfcode{\sphinxupquote{GetClassCount}}}{size\_t \sphinxstyleemphasis{batch\_idx}, size\_t *\sphinxstyleemphasis{cnt}}{ \sphinxbfcode{\sphinxupquote{const}} = 0}%
\pysigstopmultiline
Get the number of class results for a batch. 

Returns error if this result is not CLASS format. \begin{description}
\item[{\sphinxstylestrong{Return}}] \leavevmode
{\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1Error:classnvidia_1_1inferenceserver_1_1client_1_1Error}]{\sphinxcrossref{\DUrole{std,std-ref}{Error}}}} object indicating success or failure. 

\item[{\sphinxstylestrong{Parameters}}] \leavevmode\begin{itemize}
\item {} 
\sphinxcode{\sphinxupquote{batch\_idx}}: The index in the batch. 

\item {} 
\sphinxcode{\sphinxupquote{cnt}}: Returns the number of {\hyperref[\detokenize{cpp_api/structnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result_1_1ClassResult:structnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result_1_1ClassResult}]{\sphinxcrossref{\DUrole{std,std-ref}{ClassResult}}}} entries for the batch entry. 

\end{itemize}

\end{description}


\end{fulllineitems}

\index{nvidia::inferenceserver::client::InferContext::Result::GetClassAtCursor (C++ function)@\spxentry{nvidia::inferenceserver::client::InferContext::Result::GetClassAtCursor}\spxextra{C++ function}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result:_CPPv4N6nvidia15inferenceserver6client12InferContext6Result16GetClassAtCursorE6size_tP11ClassResult}}%
\pysigstartmultiline
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result:classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result_1a0b78337cb9b55608b310f226dd6e8287}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{virtual}} {\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1Error:_CPPv4N6nvidia15inferenceserver6client5ErrorE}]{\sphinxcrossref{Error}}} \sphinxbfcode{\sphinxupquote{GetClassAtCursor}}}{size\_t \sphinxstyleemphasis{batch\_idx}, {\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:_CPPv4N6nvidia15inferenceserver6client12InferContext6Result11ClassResultE}]{\sphinxcrossref{ClassResult}}} *\sphinxstyleemphasis{result}}{ = 0}%
\pysigstopmultiline
Get the {\hyperref[\detokenize{cpp_api/structnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result_1_1ClassResult:structnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result_1_1ClassResult}]{\sphinxcrossref{\DUrole{std,std-ref}{ClassResult}}}} result for a specific batch entry at the current cursor. 

Use {\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result:classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result_1adb27437d415b569a3a4b7700c56002bf}]{\sphinxcrossref{\DUrole{std,std-ref}{ResetCursor()}}}} to reset the cursor to the beginning of the result. Returns error if this result is not CLASS format. \begin{description}
\item[{\sphinxstylestrong{Return}}] \leavevmode
{\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1Error:classnvidia_1_1inferenceserver_1_1client_1_1Error}]{\sphinxcrossref{\DUrole{std,std-ref}{Error}}}} object indicating success or failure. 

\item[{\sphinxstylestrong{Parameters}}] \leavevmode\begin{itemize}
\item {} 
\sphinxcode{\sphinxupquote{batch\_idx}}: The index in the batch. 

\item {} 
\sphinxcode{\sphinxupquote{result}}: Returns the {\hyperref[\detokenize{cpp_api/structnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result_1_1ClassResult:structnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result_1_1ClassResult}]{\sphinxcrossref{\DUrole{std,std-ref}{ClassResult}}}} value for the batch at the cursor. 

\end{itemize}

\end{description}


\end{fulllineitems}

\index{nvidia::inferenceserver::client::InferContext::Result::ResetCursors (C++ function)@\spxentry{nvidia::inferenceserver::client::InferContext::Result::ResetCursors}\spxextra{C++ function}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result:_CPPv4N6nvidia15inferenceserver6client12InferContext6Result12ResetCursorsEv}}%
\pysigstartmultiline
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result:classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result_1a304de867ffb5121c21874e11a4e9d34a}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{virtual}} {\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1Error:_CPPv4N6nvidia15inferenceserver6client5ErrorE}]{\sphinxcrossref{Error}}} \sphinxbfcode{\sphinxupquote{ResetCursors}}}{}{ = 0}%
\pysigstopmultiline
Reset cursor to beginning of result for all batch entries. 

\begin{description}
\item[{\sphinxstylestrong{Return}}] \leavevmode
{\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1Error:classnvidia_1_1inferenceserver_1_1client_1_1Error}]{\sphinxcrossref{\DUrole{std,std-ref}{Error}}}} object indicating success or failure. 

\end{description}


\end{fulllineitems}

\index{nvidia::inferenceserver::client::InferContext::Result::ResetCursor (C++ function)@\spxentry{nvidia::inferenceserver::client::InferContext::Result::ResetCursor}\spxextra{C++ function}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result:_CPPv4N6nvidia15inferenceserver6client12InferContext6Result11ResetCursorE6size_t}}%
\pysigstartmultiline
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result:classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result_1adb27437d415b569a3a4b7700c56002bf}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{virtual}} {\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1Error:_CPPv4N6nvidia15inferenceserver6client5ErrorE}]{\sphinxcrossref{Error}}} \sphinxbfcode{\sphinxupquote{ResetCursor}}}{size\_t \sphinxstyleemphasis{batch\_idx}}{ = 0}%
\pysigstopmultiline
Reset cursor to beginning of result for specified batch entry. 

\begin{description}
\item[{\sphinxstylestrong{Return}}] \leavevmode
{\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1Error:classnvidia_1_1inferenceserver_1_1client_1_1Error}]{\sphinxcrossref{\DUrole{std,std-ref}{Error}}}} object indicating success or failure. 

\item[{\sphinxstylestrong{Parameters}}] \leavevmode\begin{itemize}
\item {} 
\sphinxcode{\sphinxupquote{batch\_idx}}: The index in the batch. 

\end{itemize}

\end{description}


\end{fulllineitems}

\index{nvidia::inferenceserver::client::InferContext::Result::GetRawAtCursor (C++ function)@\spxentry{nvidia::inferenceserver::client::InferContext::Result::GetRawAtCursor}\spxextra{C++ function}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result:_CPPv4N6nvidia15inferenceserver6client12InferContext6Result14GetRawAtCursorE6size_tPNSt6stringE}}%
\pysigstartmultiline
\pysigline{\sphinxbfcode{\sphinxupquote{template }}\textless{}\textgreater{}}\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result:classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result_1a742f381607b9915cb2fc558511f491c1}}\pysiglinewithargsret{{\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1Error:_CPPv4N6nvidia15inferenceserver6client5ErrorE}]{\sphinxcrossref{Error}}} \sphinxbfcode{\sphinxupquote{GetRawAtCursor}}}{size\_t \sphinxstyleemphasis{batch\_idx}, std::string *\sphinxstyleemphasis{out}}{}%
\pysigstopmultiline
\end{fulllineitems}

\index{nvidia::inferenceserver::client::InferContext::Result::ClassResult (C++ class)@\spxentry{nvidia::inferenceserver::client::InferContext::Result::ClassResult}\spxextra{C++ class}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result:_CPPv4N6nvidia15inferenceserver6client12InferContext6Result11ClassResultE}}%
\pysigstartmultiline
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result:structnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result_1_1ClassResult}}\pysigline{\sphinxbfcode{\sphinxupquote{struct }}\sphinxbfcode{\sphinxupquote{ClassResult}}}%
\pysigstopmultiline
The result value for CLASS format results. 
\subsubsection*{Public Members}
\index{nvidia::inferenceserver::client::InferContext::Result::ClassResult::idx (C++ member)@\spxentry{nvidia::inferenceserver::client::InferContext::Result::ClassResult::idx}\spxextra{C++ member}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result:_CPPv4N6nvidia15inferenceserver6client12InferContext6Result11ClassResult3idxE}}%
\pysigstartmultiline
\pysigline{\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result:structnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result_1_1ClassResult_1a8480131c8f15b37e32f19400ec5a3d5d}}size\_t \sphinxbfcode{\sphinxupquote{idx}}}%
\pysigstopmultiline
The index of the class in the result vector. 

\end{fulllineitems}

\index{nvidia::inferenceserver::client::InferContext::Result::ClassResult::value (C++ member)@\spxentry{nvidia::inferenceserver::client::InferContext::Result::ClassResult::value}\spxextra{C++ member}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result:_CPPv4N6nvidia15inferenceserver6client12InferContext6Result11ClassResult5valueE}}%
\pysigstartmultiline
\pysigline{\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result:structnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result_1_1ClassResult_1af0c1a1ef9932a40cd1fbbea3e5ae7b38}}float \sphinxbfcode{\sphinxupquote{value}}}%
\pysigstopmultiline
The value of the class. 

\end{fulllineitems}

\index{nvidia::inferenceserver::client::InferContext::Result::ClassResult::label (C++ member)@\spxentry{nvidia::inferenceserver::client::InferContext::Result::ClassResult::label}\spxextra{C++ member}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result:_CPPv4N6nvidia15inferenceserver6client12InferContext6Result11ClassResult5labelE}}%
\pysigstartmultiline
\pysigline{\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result:structnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result_1_1ClassResult_1a517c52eae642ee9a37102070da66972a}}std::string \sphinxbfcode{\sphinxupquote{label}}}%
\pysigstopmultiline
The label for the class, if provided by the model. 

\end{fulllineitems}


\end{fulllineitems}


\end{fulllineitems}



\subsubsection{Class InferGrpcContext}
\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferGrpcContext:class-infergrpccontext}}\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferGrpcContext:exhale-class-classnvidia-1-1inferenceserver-1-1client-1-1infergrpccontext}}\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferGrpcContext::doc}}\begin{itemize}
\item {} 
Defined in {\hyperref[\detokenize{cpp_api/file_src_clients_c++_request.h:file-src-clients-c-request-h}]{\sphinxcrossref{\DUrole{std,std-ref}{File request.h}}}}

\end{itemize}


\paragraph{Inheritance Relationships}
\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferGrpcContext:inheritance-relationships}}

\subparagraph{Base Type}
\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferGrpcContext:base-type}}\begin{itemize}
\item {} 
\sphinxcode{\sphinxupquote{public nvidia::inferenceserver::client::InferContext}} ({\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:exhale-class-classnvidia-1-1inferenceserver-1-1client-1-1infercontext}]{\sphinxcrossref{\DUrole{std,std-ref}{Class InferContext}}}})

\end{itemize}


\subparagraph{Derived Type}
\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferGrpcContext:derived-type}}\begin{itemize}
\item {} 
\sphinxcode{\sphinxupquote{public nvidia::inferenceserver::client::InferGrpcStreamContext}} ({\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferGrpcStreamContext:exhale-class-classnvidia-1-1inferenceserver-1-1client-1-1infergrpcstreamcontext}]{\sphinxcrossref{\DUrole{std,std-ref}{Class InferGrpcStreamContext}}}})

\end{itemize}


\paragraph{Class Documentation}
\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferGrpcContext:class-documentation}}\index{nvidia::inferenceserver::client::InferGrpcContext (C++ class)@\spxentry{nvidia::inferenceserver::client::InferGrpcContext}\spxextra{C++ class}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferGrpcContext:_CPPv4N6nvidia15inferenceserver6client16InferGrpcContextE}}%
\pysigstartmultiline
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferGrpcContext:classnvidia_1_1inferenceserver_1_1client_1_1InferGrpcContext}}\pysigline{\sphinxbfcode{\sphinxupquote{class }}\sphinxbfcode{\sphinxupquote{InferGrpcContext}} : \sphinxbfcode{\sphinxupquote{public}} nvidia::inferenceserver::client::{\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:_CPPv4N6nvidia15inferenceserver6client12InferContextE}]{\sphinxcrossref{InferContext}}}}%
\pysigstopmultiline
{\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferGrpcContext:classnvidia_1_1inferenceserver_1_1client_1_1InferGrpcContext}]{\sphinxcrossref{\DUrole{std,std-ref}{InferGrpcContext}}}} is the GRPC instantiation of {\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:classnvidia_1_1inferenceserver_1_1client_1_1InferContext}]{\sphinxcrossref{\DUrole{std,std-ref}{InferContext}}}}. 

Subclassed by {\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferGrpcStreamContext:classnvidia_1_1inferenceserver_1_1client_1_1InferGrpcStreamContext}]{\sphinxcrossref{\DUrole{std,std-ref}{nvidia::inferenceserver::client::InferGrpcStreamContext}}}}
\subsubsection*{Public Functions}
\index{nvidia::inferenceserver::client::InferGrpcContext::\textasciitilde{}InferGrpcContext (C++ function)@\spxentry{nvidia::inferenceserver::client::InferGrpcContext::\textasciitilde{}InferGrpcContext}\spxextra{C++ function}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferGrpcContext:_CPPv4N6nvidia15inferenceserver6client16InferGrpcContextD0Ev}}%
\pysigstartmultiline
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferGrpcContext:classnvidia_1_1inferenceserver_1_1client_1_1InferGrpcContext_1afc628d254dba38d178f8dcd606b56328}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{virtual}} \sphinxbfcode{\sphinxupquote{\textasciitilde{}InferGrpcContext}}}{}{}%
\pysigstopmultiline
\end{fulllineitems}

\index{nvidia::inferenceserver::client::InferGrpcContext::Run (C++ function)@\spxentry{nvidia::inferenceserver::client::InferGrpcContext::Run}\spxextra{C++ function}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferGrpcContext:_CPPv4N6nvidia15inferenceserver6client16InferGrpcContext3RunEP9ResultMap}}%
\pysigstartmultiline
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferGrpcContext:classnvidia_1_1inferenceserver_1_1client_1_1InferGrpcContext_1a17c4f2f5007867d9f6b5444f9088f6c5}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{virtual}} {\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1Error:_CPPv4N6nvidia15inferenceserver6client5ErrorE}]{\sphinxcrossref{Error}}} \sphinxbfcode{\sphinxupquote{Run}}}{ResultMap *\sphinxstyleemphasis{results}}{}%
\pysigstopmultiline
Send a synchronous request to the inference server to perform an inference to produce results for the outputs specified in the most recent call to {\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1a7cf1746d814561727fd3b07b0f8349f2}]{\sphinxcrossref{\DUrole{std,std-ref}{SetRunOptions()}}}}. 

\begin{description}
\item[{\sphinxstylestrong{Return}}] \leavevmode
{\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1Error:classnvidia_1_1inferenceserver_1_1client_1_1Error}]{\sphinxcrossref{\DUrole{std,std-ref}{Error}}}} object indicating success or failure. 

\item[{\sphinxstylestrong{Parameters}}] \leavevmode\begin{itemize}
\item {} 
\sphinxcode{\sphinxupquote{results}}: Returns Result objects holding inference results as a map from output name to Result object. 

\end{itemize}

\end{description}


\end{fulllineitems}

\index{nvidia::inferenceserver::client::InferGrpcContext::AsyncRun (C++ function)@\spxentry{nvidia::inferenceserver::client::InferGrpcContext::AsyncRun}\spxextra{C++ function}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferGrpcContext:_CPPv4N6nvidia15inferenceserver6client16InferGrpcContext8AsyncRunEPNSt10shared_ptrI7RequestEE}}%
\pysigstartmultiline
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferGrpcContext:classnvidia_1_1inferenceserver_1_1client_1_1InferGrpcContext_1adcd341f23d4b3a609c2117c698522a31}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{virtual}} {\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1Error:_CPPv4N6nvidia15inferenceserver6client5ErrorE}]{\sphinxcrossref{Error}}} \sphinxbfcode{\sphinxupquote{AsyncRun}}}{std::shared\_ptr\textless{}Request\textgreater{} *\sphinxstyleemphasis{async\_request}}{}%
\pysigstopmultiline
Send an asynchronous request to the inference server to perform an inference to produce results for the outputs specified in the most recent call to {\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1a7cf1746d814561727fd3b07b0f8349f2}]{\sphinxcrossref{\DUrole{std,std-ref}{SetRunOptions()}}}}. 

\begin{description}
\item[{\sphinxstylestrong{Return}}] \leavevmode
{\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1Error:classnvidia_1_1inferenceserver_1_1client_1_1Error}]{\sphinxcrossref{\DUrole{std,std-ref}{Error}}}} object indicating success or failure. 

\item[{\sphinxstylestrong{Parameters}}] \leavevmode\begin{itemize}
\item {} 
\sphinxcode{\sphinxupquote{async\_request}}: Returns a Request object that can be used to retrieve the inference results for the request. 

\end{itemize}

\end{description}


\end{fulllineitems}

\index{nvidia::inferenceserver::client::InferGrpcContext::GetAsyncRunResults (C++ function)@\spxentry{nvidia::inferenceserver::client::InferGrpcContext::GetAsyncRunResults}\spxextra{C++ function}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferGrpcContext:_CPPv4N6nvidia15inferenceserver6client16InferGrpcContext18GetAsyncRunResultsEP9ResultMapRKNSt10shared_ptrI7RequestEEb}}%
\pysigstartmultiline
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferGrpcContext:classnvidia_1_1inferenceserver_1_1client_1_1InferGrpcContext_1af701c02c211163d37ac940a9816cf9ab}}\pysiglinewithargsret{{\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1Error:_CPPv4N6nvidia15inferenceserver6client5ErrorE}]{\sphinxcrossref{Error}}} \sphinxbfcode{\sphinxupquote{GetAsyncRunResults}}}{ResultMap *\sphinxstyleemphasis{results}, \sphinxbfcode{\sphinxupquote{const}} std::shared\_ptr\textless{}Request\textgreater{} \&\sphinxstyleemphasis{async\_request}, bool \sphinxstyleemphasis{wait}}{}%
\pysigstopmultiline
Get the results of the asynchronous request referenced by ‘async\_request’. 

\begin{description}
\item[{\sphinxstylestrong{Return}}] \leavevmode
{\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1Error:classnvidia_1_1inferenceserver_1_1client_1_1Error}]{\sphinxcrossref{\DUrole{std,std-ref}{Error}}}} object indicating success or failure. Success will be returned only if the request has been completed succesfully. UNAVAILABLE will be returned if ‘wait’ is false and the request is not ready. 

\item[{\sphinxstylestrong{Parameters}}] \leavevmode\begin{itemize}
\item {} 
\sphinxcode{\sphinxupquote{results}}: Returns Result objects holding inference results as a map from output name to Result object. 

\item {} 
\sphinxcode{\sphinxupquote{async\_request}}: Request handle to retrieve results. 

\item {} 
\sphinxcode{\sphinxupquote{wait}}: If true, block until the request completes. Otherwise, return immediately. 

\end{itemize}

\end{description}


\end{fulllineitems}

\subsubsection*{Public Static Functions}
\index{nvidia::inferenceserver::client::InferGrpcContext::Create (C++ function)@\spxentry{nvidia::inferenceserver::client::InferGrpcContext::Create}\spxextra{C++ function}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferGrpcContext:_CPPv4N6nvidia15inferenceserver6client16InferGrpcContext6CreateEPNSt10unique_ptrI12InferContextEERKNSt6stringERKNSt6stringE7int64_tb}}%
\pysigstartmultiline
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferGrpcContext:classnvidia_1_1inferenceserver_1_1client_1_1InferGrpcContext_1a432bf0981eb512786263764f090ee83d}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{static}} {\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1Error:_CPPv4N6nvidia15inferenceserver6client5ErrorE}]{\sphinxcrossref{Error}}} \sphinxbfcode{\sphinxupquote{Create}}}{std::unique\_ptr\textless{}{\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:_CPPv4N6nvidia15inferenceserver6client12InferContextE}]{\sphinxcrossref{InferContext}}}\textgreater{} *\sphinxstyleemphasis{ctx}, \sphinxbfcode{\sphinxupquote{const}} std::string \&\sphinxstyleemphasis{server\_url}, \sphinxbfcode{\sphinxupquote{const}} std::string \&\sphinxstyleemphasis{model\_name}, int64\_t \sphinxstyleemphasis{model\_version} = -1, bool \sphinxstyleemphasis{verbose} = false}{}%
\pysigstopmultiline
Create context that performs inference for a non-sequence model using the GRPC protocol. 

\begin{description}
\item[{\sphinxstylestrong{Return}}] \leavevmode
{\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1Error:classnvidia_1_1inferenceserver_1_1client_1_1Error}]{\sphinxcrossref{\DUrole{std,std-ref}{Error}}}} object indicating success or failure. 

\item[{\sphinxstylestrong{Parameters}}] \leavevmode\begin{itemize}
\item {} 
\sphinxcode{\sphinxupquote{ctx}}: Returns a new {\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferGrpcContext:classnvidia_1_1inferenceserver_1_1client_1_1InferGrpcContext}]{\sphinxcrossref{\DUrole{std,std-ref}{InferGrpcContext}}}} object. 

\item {} 
\sphinxcode{\sphinxupquote{server\_url}}: The inference server name and port. 

\item {} 
\sphinxcode{\sphinxupquote{model\_name}}: The name of the model to get status for. 

\item {} 
\sphinxcode{\sphinxupquote{model\_version}}: The version of the model to use for inference, or -1 to indicate that the latest (i.e. highest version number) version should be used. 

\item {} 
\sphinxcode{\sphinxupquote{verbose}}: If true generate verbose output when contacting the inference server. 

\end{itemize}

\end{description}


\end{fulllineitems}

\index{nvidia::inferenceserver::client::InferGrpcContext::Create (C++ function)@\spxentry{nvidia::inferenceserver::client::InferGrpcContext::Create}\spxextra{C++ function}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferGrpcContext:_CPPv4N6nvidia15inferenceserver6client16InferGrpcContext6CreateEPNSt10unique_ptrI12InferContextEE13CorrelationIDRKNSt6stringERKNSt6stringE7int64_tb}}%
\pysigstartmultiline
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferGrpcContext:classnvidia_1_1inferenceserver_1_1client_1_1InferGrpcContext_1a724b50d78b302e38603fb9a7c1aa8ee0}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{static}} {\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1Error:_CPPv4N6nvidia15inferenceserver6client5ErrorE}]{\sphinxcrossref{Error}}} \sphinxbfcode{\sphinxupquote{Create}}}{std::unique\_ptr\textless{}{\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:_CPPv4N6nvidia15inferenceserver6client12InferContextE}]{\sphinxcrossref{InferContext}}}\textgreater{} *\sphinxstyleemphasis{ctx}, CorrelationID \sphinxstyleemphasis{correlation\_id}, \sphinxbfcode{\sphinxupquote{const}} std::string \&\sphinxstyleemphasis{server\_url}, \sphinxbfcode{\sphinxupquote{const}} std::string \&\sphinxstyleemphasis{model\_name}, int64\_t \sphinxstyleemphasis{model\_version} = -1, bool \sphinxstyleemphasis{verbose} = false}{}%
\pysigstopmultiline
Create context that performs inference for a sequence model using a given correlation ID and the GRPC protocol. 

\begin{description}
\item[{\sphinxstylestrong{Return}}] \leavevmode
{\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1Error:classnvidia_1_1inferenceserver_1_1client_1_1Error}]{\sphinxcrossref{\DUrole{std,std-ref}{Error}}}} object indicating success or failure. 

\item[{\sphinxstylestrong{Parameters}}] \leavevmode\begin{itemize}
\item {} 
\sphinxcode{\sphinxupquote{ctx}}: Returns a new {\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferGrpcContext:classnvidia_1_1inferenceserver_1_1client_1_1InferGrpcContext}]{\sphinxcrossref{\DUrole{std,std-ref}{InferGrpcContext}}}} object. 

\item {} 
\sphinxcode{\sphinxupquote{correlation\_id}}: The correlation ID to use for all inferences performed with this context. A value of 0 (zero) indicates that no correlation ID should be used. 

\item {} 
\sphinxcode{\sphinxupquote{server\_url}}: The inference server name and port. 

\item {} 
\sphinxcode{\sphinxupquote{model\_name}}: The name of the model to get status for. 

\item {} 
\sphinxcode{\sphinxupquote{model\_version}}: The version of the model to use for inference, or -1 to indicate that the latest (i.e. highest version number) version should be used. 

\item {} 
\sphinxcode{\sphinxupquote{verbose}}: If true generate verbose output when contacting the inference server. 

\end{itemize}

\end{description}


\end{fulllineitems}

\subsubsection*{Protected Functions}
\index{nvidia::inferenceserver::client::InferGrpcContext::InferGrpcContext (C++ function)@\spxentry{nvidia::inferenceserver::client::InferGrpcContext::InferGrpcContext}\spxextra{C++ function}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferGrpcContext:_CPPv4N6nvidia15inferenceserver6client16InferGrpcContext16InferGrpcContextERKNSt6stringERKNSt6stringE7int64_t13CorrelationIDb}}%
\pysigstartmultiline
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferGrpcContext:classnvidia_1_1inferenceserver_1_1client_1_1InferGrpcContext_1abcd04c87e15144a95f81dceb54f5dd28}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{InferGrpcContext}}}{\sphinxbfcode{\sphinxupquote{const}} std::string\&, \sphinxbfcode{\sphinxupquote{const}} std::string\&, int64\_t, CorrelationID, bool}{}%
\pysigstopmultiline
\end{fulllineitems}

\index{nvidia::inferenceserver::client::InferGrpcContext::InitHelper (C++ function)@\spxentry{nvidia::inferenceserver::client::InferGrpcContext::InitHelper}\spxextra{C++ function}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferGrpcContext:_CPPv4N6nvidia15inferenceserver6client16InferGrpcContext10InitHelperERKNSt6stringERKNSt6stringEb}}%
\pysigstartmultiline
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferGrpcContext:classnvidia_1_1inferenceserver_1_1client_1_1InferGrpcContext_1a9b1f7432db5b39108de896b41f5300cd}}\pysiglinewithargsret{{\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1Error:_CPPv4N6nvidia15inferenceserver6client5ErrorE}]{\sphinxcrossref{Error}}} \sphinxbfcode{\sphinxupquote{InitHelper}}}{\sphinxbfcode{\sphinxupquote{const}} std::string \&\sphinxstyleemphasis{server\_url}, \sphinxbfcode{\sphinxupquote{const}} std::string \&\sphinxstyleemphasis{model\_name}, bool \sphinxstyleemphasis{verbose}}{}%
\pysigstopmultiline
\end{fulllineitems}

\index{nvidia::inferenceserver::client::InferGrpcContext::AsyncTransfer (C++ function)@\spxentry{nvidia::inferenceserver::client::InferGrpcContext::AsyncTransfer}\spxextra{C++ function}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferGrpcContext:_CPPv4N6nvidia15inferenceserver6client16InferGrpcContext13AsyncTransferEv}}%
\pysigstartmultiline
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferGrpcContext:classnvidia_1_1inferenceserver_1_1client_1_1InferGrpcContext_1a3f4f41c57b4e74c9ad6e8803c2514ec0}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{virtual}} void \sphinxbfcode{\sphinxupquote{AsyncTransfer}}}{}{}%
\pysigstopmultiline
\end{fulllineitems}

\index{nvidia::inferenceserver::client::InferGrpcContext::PreRunProcessing (C++ function)@\spxentry{nvidia::inferenceserver::client::InferGrpcContext::PreRunProcessing}\spxextra{C++ function}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferGrpcContext:_CPPv4N6nvidia15inferenceserver6client16InferGrpcContext16PreRunProcessingERNSt10shared_ptrI7RequestEE}}%
\pysigstartmultiline
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferGrpcContext:classnvidia_1_1inferenceserver_1_1client_1_1InferGrpcContext_1a5902b0ce9b16f0a055bbc0fc09c69031}}\pysiglinewithargsret{{\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1Error:_CPPv4N6nvidia15inferenceserver6client5ErrorE}]{\sphinxcrossref{Error}}} \sphinxbfcode{\sphinxupquote{PreRunProcessing}}}{std::shared\_ptr\textless{}Request\textgreater{} \&\sphinxstyleemphasis{request}}{}%
\pysigstopmultiline
\end{fulllineitems}

\subsubsection*{Protected Attributes}
\index{nvidia::inferenceserver::client::InferGrpcContext::async\_request\_completion\_queue\_ (C++ member)@\spxentry{nvidia::inferenceserver::client::InferGrpcContext::async\_request\_completion\_queue\_}\spxextra{C++ member}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferGrpcContext:_CPPv4N6nvidia15inferenceserver6client16InferGrpcContext31async_request_completion_queue_E}}%
\pysigstartmultiline
\pysigline{\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferGrpcContext:classnvidia_1_1inferenceserver_1_1client_1_1InferGrpcContext_1a95e7c7274daeceb82669b9f95aef5054}}grpc::CompletionQueue \sphinxbfcode{\sphinxupquote{async\_request\_completion\_queue\_}}}%
\pysigstopmultiline
\end{fulllineitems}

\index{nvidia::inferenceserver::client::InferGrpcContext::stub\_ (C++ member)@\spxentry{nvidia::inferenceserver::client::InferGrpcContext::stub\_}\spxextra{C++ member}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferGrpcContext:_CPPv4N6nvidia15inferenceserver6client16InferGrpcContext5stub_E}}%
\pysigstartmultiline
\pysigline{\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferGrpcContext:classnvidia_1_1inferenceserver_1_1client_1_1InferGrpcContext_1af175710b44ad280ec46e980228b53398}}std::unique\_ptr\textless{}GRPCService::Stub\textgreater{} \sphinxbfcode{\sphinxupquote{stub\_}}}%
\pysigstopmultiline
\end{fulllineitems}

\index{nvidia::inferenceserver::client::InferGrpcContext::request\_ (C++ member)@\spxentry{nvidia::inferenceserver::client::InferGrpcContext::request\_}\spxextra{C++ member}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferGrpcContext:_CPPv4N6nvidia15inferenceserver6client16InferGrpcContext8request_E}}%
\pysigstartmultiline
\pysigline{\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferGrpcContext:classnvidia_1_1inferenceserver_1_1client_1_1InferGrpcContext_1a964d67791bbd912f2dc13b1472aaaf33}}InferRequest \sphinxbfcode{\sphinxupquote{request\_}}}%
\pysigstopmultiline
\end{fulllineitems}


\end{fulllineitems}



\subsubsection{Class InferGrpcStreamContext}
\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferGrpcStreamContext:class-infergrpcstreamcontext}}\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferGrpcStreamContext:exhale-class-classnvidia-1-1inferenceserver-1-1client-1-1infergrpcstreamcontext}}\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferGrpcStreamContext::doc}}\begin{itemize}
\item {} 
Defined in {\hyperref[\detokenize{cpp_api/file_src_clients_c++_request.h:file-src-clients-c-request-h}]{\sphinxcrossref{\DUrole{std,std-ref}{File request.h}}}}

\end{itemize}


\paragraph{Inheritance Relationships}
\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferGrpcStreamContext:inheritance-relationships}}

\subparagraph{Base Type}
\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferGrpcStreamContext:base-type}}\begin{itemize}
\item {} 
\sphinxcode{\sphinxupquote{public nvidia::inferenceserver::client::InferGrpcContext}} ({\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferGrpcContext:exhale-class-classnvidia-1-1inferenceserver-1-1client-1-1infergrpccontext}]{\sphinxcrossref{\DUrole{std,std-ref}{Class InferGrpcContext}}}})

\end{itemize}


\paragraph{Class Documentation}
\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferGrpcStreamContext:class-documentation}}\index{nvidia::inferenceserver::client::InferGrpcStreamContext (C++ class)@\spxentry{nvidia::inferenceserver::client::InferGrpcStreamContext}\spxextra{C++ class}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferGrpcStreamContext:_CPPv4N6nvidia15inferenceserver6client22InferGrpcStreamContextE}}%
\pysigstartmultiline
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferGrpcStreamContext:classnvidia_1_1inferenceserver_1_1client_1_1InferGrpcStreamContext}}\pysigline{\sphinxbfcode{\sphinxupquote{class }}\sphinxbfcode{\sphinxupquote{InferGrpcStreamContext}} : \sphinxbfcode{\sphinxupquote{public}} nvidia::inferenceserver::client::{\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferGrpcContext:_CPPv4N6nvidia15inferenceserver6client16InferGrpcContextE}]{\sphinxcrossref{InferGrpcContext}}}}%
\pysigstopmultiline
{\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferGrpcStreamContext:classnvidia_1_1inferenceserver_1_1client_1_1InferGrpcStreamContext}]{\sphinxcrossref{\DUrole{std,std-ref}{InferGrpcStreamContext}}}} is the streaming instantiation of {\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferGrpcContext:classnvidia_1_1inferenceserver_1_1client_1_1InferGrpcContext}]{\sphinxcrossref{\DUrole{std,std-ref}{InferGrpcContext}}}}. 

All synchronous and asynchronous requests sent from this context will be sent in the same stream. 
\subsubsection*{Public Functions}
\index{nvidia::inferenceserver::client::InferGrpcStreamContext::\textasciitilde{}InferGrpcStreamContext (C++ function)@\spxentry{nvidia::inferenceserver::client::InferGrpcStreamContext::\textasciitilde{}InferGrpcStreamContext}\spxextra{C++ function}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferGrpcStreamContext:_CPPv4N6nvidia15inferenceserver6client22InferGrpcStreamContextD0Ev}}%
\pysigstartmultiline
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferGrpcStreamContext:classnvidia_1_1inferenceserver_1_1client_1_1InferGrpcStreamContext_1a378f8f70646b1256db2e78266b08b1c1}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{\textasciitilde{}InferGrpcStreamContext}}}{}{}%
\pysigstopmultiline
\end{fulllineitems}

\index{nvidia::inferenceserver::client::InferGrpcStreamContext::Run (C++ function)@\spxentry{nvidia::inferenceserver::client::InferGrpcStreamContext::Run}\spxextra{C++ function}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferGrpcStreamContext:_CPPv4N6nvidia15inferenceserver6client22InferGrpcStreamContext3RunEP9ResultMap}}%
\pysigstartmultiline
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferGrpcStreamContext:classnvidia_1_1inferenceserver_1_1client_1_1InferGrpcStreamContext_1a5e42ef49aa0ed810934a18ca1dbb71e4}}\pysiglinewithargsret{{\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1Error:_CPPv4N6nvidia15inferenceserver6client5ErrorE}]{\sphinxcrossref{Error}}} \sphinxbfcode{\sphinxupquote{Run}}}{ResultMap *\sphinxstyleemphasis{results}}{}%
\pysigstopmultiline
Send a synchronous request to the inference server to perform an inference to produce results for the outputs specified in the most recent call to {\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1a7cf1746d814561727fd3b07b0f8349f2}]{\sphinxcrossref{\DUrole{std,std-ref}{SetRunOptions()}}}}. 

\begin{description}
\item[{\sphinxstylestrong{Return}}] \leavevmode
{\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1Error:classnvidia_1_1inferenceserver_1_1client_1_1Error}]{\sphinxcrossref{\DUrole{std,std-ref}{Error}}}} object indicating success or failure. 

\item[{\sphinxstylestrong{Parameters}}] \leavevmode\begin{itemize}
\item {} 
\sphinxcode{\sphinxupquote{results}}: Returns Result objects holding inference results as a map from output name to Result object. 

\end{itemize}

\end{description}


\end{fulllineitems}

\index{nvidia::inferenceserver::client::InferGrpcStreamContext::AsyncRun (C++ function)@\spxentry{nvidia::inferenceserver::client::InferGrpcStreamContext::AsyncRun}\spxextra{C++ function}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferGrpcStreamContext:_CPPv4N6nvidia15inferenceserver6client22InferGrpcStreamContext8AsyncRunEPNSt10shared_ptrI7RequestEE}}%
\pysigstartmultiline
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferGrpcStreamContext:classnvidia_1_1inferenceserver_1_1client_1_1InferGrpcStreamContext_1a86e4987e666b37c5493373ae1b068d2f}}\pysiglinewithargsret{{\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1Error:_CPPv4N6nvidia15inferenceserver6client5ErrorE}]{\sphinxcrossref{Error}}} \sphinxbfcode{\sphinxupquote{AsyncRun}}}{std::shared\_ptr\textless{}Request\textgreater{} *\sphinxstyleemphasis{async\_request}}{}%
\pysigstopmultiline
Send an asynchronous request to the inference server to perform an inference to produce results for the outputs specified in the most recent call to {\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1a7cf1746d814561727fd3b07b0f8349f2}]{\sphinxcrossref{\DUrole{std,std-ref}{SetRunOptions()}}}}. 

\begin{description}
\item[{\sphinxstylestrong{Return}}] \leavevmode
{\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1Error:classnvidia_1_1inferenceserver_1_1client_1_1Error}]{\sphinxcrossref{\DUrole{std,std-ref}{Error}}}} object indicating success or failure. 

\item[{\sphinxstylestrong{Parameters}}] \leavevmode\begin{itemize}
\item {} 
\sphinxcode{\sphinxupquote{async\_request}}: Returns a Request object that can be used to retrieve the inference results for the request. 

\end{itemize}

\end{description}


\end{fulllineitems}

\subsubsection*{Public Static Functions}
\index{nvidia::inferenceserver::client::InferGrpcStreamContext::Create (C++ function)@\spxentry{nvidia::inferenceserver::client::InferGrpcStreamContext::Create}\spxextra{C++ function}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferGrpcStreamContext:_CPPv4N6nvidia15inferenceserver6client22InferGrpcStreamContext6CreateEPNSt10unique_ptrI12InferContextEERKNSt6stringERKNSt6stringE7int64_tb}}%
\pysigstartmultiline
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferGrpcStreamContext:classnvidia_1_1inferenceserver_1_1client_1_1InferGrpcStreamContext_1a6a356d8b5495f6ba471ed989712c8e64}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{static}} {\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1Error:_CPPv4N6nvidia15inferenceserver6client5ErrorE}]{\sphinxcrossref{Error}}} \sphinxbfcode{\sphinxupquote{Create}}}{std::unique\_ptr\textless{}{\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:_CPPv4N6nvidia15inferenceserver6client12InferContextE}]{\sphinxcrossref{InferContext}}}\textgreater{} *\sphinxstyleemphasis{ctx}, \sphinxbfcode{\sphinxupquote{const}} std::string \&\sphinxstyleemphasis{server\_url}, \sphinxbfcode{\sphinxupquote{const}} std::string \&\sphinxstyleemphasis{model\_name}, int64\_t \sphinxstyleemphasis{model\_version} = -1, bool \sphinxstyleemphasis{verbose} = false}{}%
\pysigstopmultiline
Create streaming context that performs inference for a non-sequence model using the GRPC protocol. 

\begin{description}
\item[{\sphinxstylestrong{Return}}] \leavevmode
{\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1Error:classnvidia_1_1inferenceserver_1_1client_1_1Error}]{\sphinxcrossref{\DUrole{std,std-ref}{Error}}}} object indicating success or failure. 

\item[{\sphinxstylestrong{Parameters}}] \leavevmode\begin{itemize}
\item {} 
\sphinxcode{\sphinxupquote{ctx}}: Returns a new {\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferGrpcContext:classnvidia_1_1inferenceserver_1_1client_1_1InferGrpcContext}]{\sphinxcrossref{\DUrole{std,std-ref}{InferGrpcContext}}}} object. 

\item {} 
\sphinxcode{\sphinxupquote{server\_url}}: The inference server name and port. 

\item {} 
\sphinxcode{\sphinxupquote{model\_name}}: The name of the model to get status for. 

\item {} 
\sphinxcode{\sphinxupquote{model\_version}}: The version of the model to use for inference, or -1 to indicate that the latest (i.e. highest version number) version should be used. 

\item {} 
\sphinxcode{\sphinxupquote{verbose}}: If true generate verbose output when contacting the inference server. 

\end{itemize}

\end{description}


\end{fulllineitems}

\index{nvidia::inferenceserver::client::InferGrpcStreamContext::Create (C++ function)@\spxentry{nvidia::inferenceserver::client::InferGrpcStreamContext::Create}\spxextra{C++ function}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferGrpcStreamContext:_CPPv4N6nvidia15inferenceserver6client22InferGrpcStreamContext6CreateEPNSt10unique_ptrI12InferContextEE13CorrelationIDRKNSt6stringERKNSt6stringE7int64_tb}}%
\pysigstartmultiline
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferGrpcStreamContext:classnvidia_1_1inferenceserver_1_1client_1_1InferGrpcStreamContext_1aa1f3dc161b5bed3a3e4d73ea997f898c}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{static}} {\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1Error:_CPPv4N6nvidia15inferenceserver6client5ErrorE}]{\sphinxcrossref{Error}}} \sphinxbfcode{\sphinxupquote{Create}}}{std::unique\_ptr\textless{}{\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:_CPPv4N6nvidia15inferenceserver6client12InferContextE}]{\sphinxcrossref{InferContext}}}\textgreater{} *\sphinxstyleemphasis{ctx}, CorrelationID \sphinxstyleemphasis{correlation\_id}, \sphinxbfcode{\sphinxupquote{const}} std::string \&\sphinxstyleemphasis{server\_url}, \sphinxbfcode{\sphinxupquote{const}} std::string \&\sphinxstyleemphasis{model\_name}, int64\_t \sphinxstyleemphasis{model\_version} = -1, bool \sphinxstyleemphasis{verbose} = false}{}%
\pysigstopmultiline
Create streaming context that performs inference for a sequence model using a given correlation ID and the GRPC protocol. 

\begin{description}
\item[{\sphinxstylestrong{Return}}] \leavevmode
{\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1Error:classnvidia_1_1inferenceserver_1_1client_1_1Error}]{\sphinxcrossref{\DUrole{std,std-ref}{Error}}}} object indicating success or failure. 

\item[{\sphinxstylestrong{Parameters}}] \leavevmode\begin{itemize}
\item {} 
\sphinxcode{\sphinxupquote{ctx}}: Returns a new {\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferGrpcContext:classnvidia_1_1inferenceserver_1_1client_1_1InferGrpcContext}]{\sphinxcrossref{\DUrole{std,std-ref}{InferGrpcContext}}}} object. 

\item {} 
\sphinxcode{\sphinxupquote{correlation\_id}}: The correlation ID to use for all inferences performed with this context. A value of 0 (zero) indicates that no correlation ID should be used. 

\item {} 
\sphinxcode{\sphinxupquote{server\_url}}: The inference server name and port. 

\item {} 
\sphinxcode{\sphinxupquote{model\_name}}: The name of the model to get status for. 

\item {} 
\sphinxcode{\sphinxupquote{model\_version}}: The version of the model to use for inference, or -1 to indicate that the latest (i.e. highest version number) version should be used. 

\item {} 
\sphinxcode{\sphinxupquote{verbose}}: If true generate verbose output when contacting the inference server. 

\end{itemize}

\end{description}


\end{fulllineitems}


\end{fulllineitems}



\subsubsection{Class InferHttpContext}
\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferHttpContext:class-inferhttpcontext}}\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferHttpContext:exhale-class-classnvidia-1-1inferenceserver-1-1client-1-1inferhttpcontext}}\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferHttpContext::doc}}\begin{itemize}
\item {} 
Defined in {\hyperref[\detokenize{cpp_api/file_src_clients_c++_request.h:file-src-clients-c-request-h}]{\sphinxcrossref{\DUrole{std,std-ref}{File request.h}}}}

\end{itemize}


\paragraph{Inheritance Relationships}
\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferHttpContext:inheritance-relationships}}

\subparagraph{Base Type}
\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferHttpContext:base-type}}\begin{itemize}
\item {} 
\sphinxcode{\sphinxupquote{public nvidia::inferenceserver::client::InferContext}} ({\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:exhale-class-classnvidia-1-1inferenceserver-1-1client-1-1infercontext}]{\sphinxcrossref{\DUrole{std,std-ref}{Class InferContext}}}})

\end{itemize}


\paragraph{Class Documentation}
\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferHttpContext:class-documentation}}\index{nvidia::inferenceserver::client::InferHttpContext (C++ class)@\spxentry{nvidia::inferenceserver::client::InferHttpContext}\spxextra{C++ class}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferHttpContext:_CPPv4N6nvidia15inferenceserver6client16InferHttpContextE}}%
\pysigstartmultiline
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferHttpContext:classnvidia_1_1inferenceserver_1_1client_1_1InferHttpContext}}\pysigline{\sphinxbfcode{\sphinxupquote{class }}\sphinxbfcode{\sphinxupquote{InferHttpContext}} : \sphinxbfcode{\sphinxupquote{public}} nvidia::inferenceserver::client::{\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:_CPPv4N6nvidia15inferenceserver6client12InferContextE}]{\sphinxcrossref{InferContext}}}}%
\pysigstopmultiline
{\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferHttpContext:classnvidia_1_1inferenceserver_1_1client_1_1InferHttpContext}]{\sphinxcrossref{\DUrole{std,std-ref}{InferHttpContext}}}} is the HTTP instantiation of {\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:classnvidia_1_1inferenceserver_1_1client_1_1InferContext}]{\sphinxcrossref{\DUrole{std,std-ref}{InferContext}}}}. 
\subsubsection*{Public Functions}
\index{nvidia::inferenceserver::client::InferHttpContext::\textasciitilde{}InferHttpContext (C++ function)@\spxentry{nvidia::inferenceserver::client::InferHttpContext::\textasciitilde{}InferHttpContext}\spxextra{C++ function}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferHttpContext:_CPPv4N6nvidia15inferenceserver6client16InferHttpContextD0Ev}}%
\pysigstartmultiline
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferHttpContext:classnvidia_1_1inferenceserver_1_1client_1_1InferHttpContext_1a0f1905f3c0e275655dadc2ea9fe074c0}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{\textasciitilde{}InferHttpContext}}}{}{}%
\pysigstopmultiline
\end{fulllineitems}

\index{nvidia::inferenceserver::client::InferHttpContext::Run (C++ function)@\spxentry{nvidia::inferenceserver::client::InferHttpContext::Run}\spxextra{C++ function}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferHttpContext:_CPPv4N6nvidia15inferenceserver6client16InferHttpContext3RunEP9ResultMap}}%
\pysigstartmultiline
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferHttpContext:classnvidia_1_1inferenceserver_1_1client_1_1InferHttpContext_1a9d2e82db48e7a11ea2ee21e39113e267}}\pysiglinewithargsret{{\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1Error:_CPPv4N6nvidia15inferenceserver6client5ErrorE}]{\sphinxcrossref{Error}}} \sphinxbfcode{\sphinxupquote{Run}}}{ResultMap *\sphinxstyleemphasis{results}}{}%
\pysigstopmultiline
Send a synchronous request to the inference server to perform an inference to produce results for the outputs specified in the most recent call to {\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1a7cf1746d814561727fd3b07b0f8349f2}]{\sphinxcrossref{\DUrole{std,std-ref}{SetRunOptions()}}}}. 

\begin{description}
\item[{\sphinxstylestrong{Return}}] \leavevmode
{\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1Error:classnvidia_1_1inferenceserver_1_1client_1_1Error}]{\sphinxcrossref{\DUrole{std,std-ref}{Error}}}} object indicating success or failure. 

\item[{\sphinxstylestrong{Parameters}}] \leavevmode\begin{itemize}
\item {} 
\sphinxcode{\sphinxupquote{results}}: Returns Result objects holding inference results as a map from output name to Result object. 

\end{itemize}

\end{description}


\end{fulllineitems}

\index{nvidia::inferenceserver::client::InferHttpContext::AsyncRun (C++ function)@\spxentry{nvidia::inferenceserver::client::InferHttpContext::AsyncRun}\spxextra{C++ function}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferHttpContext:_CPPv4N6nvidia15inferenceserver6client16InferHttpContext8AsyncRunEPNSt10shared_ptrI7RequestEE}}%
\pysigstartmultiline
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferHttpContext:classnvidia_1_1inferenceserver_1_1client_1_1InferHttpContext_1a625d8c3a79b6d3cf08fd442b0d0b2540}}\pysiglinewithargsret{{\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1Error:_CPPv4N6nvidia15inferenceserver6client5ErrorE}]{\sphinxcrossref{Error}}} \sphinxbfcode{\sphinxupquote{AsyncRun}}}{std::shared\_ptr\textless{}Request\textgreater{} *\sphinxstyleemphasis{async\_request}}{}%
\pysigstopmultiline
Send an asynchronous request to the inference server to perform an inference to produce results for the outputs specified in the most recent call to {\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1a7cf1746d814561727fd3b07b0f8349f2}]{\sphinxcrossref{\DUrole{std,std-ref}{SetRunOptions()}}}}. 

\begin{description}
\item[{\sphinxstylestrong{Return}}] \leavevmode
{\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1Error:classnvidia_1_1inferenceserver_1_1client_1_1Error}]{\sphinxcrossref{\DUrole{std,std-ref}{Error}}}} object indicating success or failure. 

\item[{\sphinxstylestrong{Parameters}}] \leavevmode\begin{itemize}
\item {} 
\sphinxcode{\sphinxupquote{async\_request}}: Returns a Request object that can be used to retrieve the inference results for the request. 

\end{itemize}

\end{description}


\end{fulllineitems}

\index{nvidia::inferenceserver::client::InferHttpContext::GetAsyncRunResults (C++ function)@\spxentry{nvidia::inferenceserver::client::InferHttpContext::GetAsyncRunResults}\spxextra{C++ function}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferHttpContext:_CPPv4N6nvidia15inferenceserver6client16InferHttpContext18GetAsyncRunResultsEP9ResultMapRKNSt10shared_ptrI7RequestEEb}}%
\pysigstartmultiline
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferHttpContext:classnvidia_1_1inferenceserver_1_1client_1_1InferHttpContext_1a646ec6ca28f8c38a23b4e85c3ad8e8ec}}\pysiglinewithargsret{{\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1Error:_CPPv4N6nvidia15inferenceserver6client5ErrorE}]{\sphinxcrossref{Error}}} \sphinxbfcode{\sphinxupquote{GetAsyncRunResults}}}{ResultMap *\sphinxstyleemphasis{results}, \sphinxbfcode{\sphinxupquote{const}} std::shared\_ptr\textless{}Request\textgreater{} \&\sphinxstyleemphasis{async\_request}, bool \sphinxstyleemphasis{wait}}{}%
\pysigstopmultiline
Get the results of the asynchronous request referenced by ‘async\_request’. 

\begin{description}
\item[{\sphinxstylestrong{Return}}] \leavevmode
{\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1Error:classnvidia_1_1inferenceserver_1_1client_1_1Error}]{\sphinxcrossref{\DUrole{std,std-ref}{Error}}}} object indicating success or failure. Success will be returned only if the request has been completed succesfully. UNAVAILABLE will be returned if ‘wait’ is false and the request is not ready. 

\item[{\sphinxstylestrong{Parameters}}] \leavevmode\begin{itemize}
\item {} 
\sphinxcode{\sphinxupquote{results}}: Returns Result objects holding inference results as a map from output name to Result object. 

\item {} 
\sphinxcode{\sphinxupquote{async\_request}}: Request handle to retrieve results. 

\item {} 
\sphinxcode{\sphinxupquote{wait}}: If true, block until the request completes. Otherwise, return immediately. 

\end{itemize}

\end{description}


\end{fulllineitems}

\subsubsection*{Public Static Functions}
\index{nvidia::inferenceserver::client::InferHttpContext::Create (C++ function)@\spxentry{nvidia::inferenceserver::client::InferHttpContext::Create}\spxextra{C++ function}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferHttpContext:_CPPv4N6nvidia15inferenceserver6client16InferHttpContext6CreateEPNSt10unique_ptrI12InferContextEERKNSt6stringERKNSt6stringE7int64_tb}}%
\pysigstartmultiline
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferHttpContext:classnvidia_1_1inferenceserver_1_1client_1_1InferHttpContext_1a7c210c15455fde64a31c6925f3a8b906}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{static}} {\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1Error:_CPPv4N6nvidia15inferenceserver6client5ErrorE}]{\sphinxcrossref{Error}}} \sphinxbfcode{\sphinxupquote{Create}}}{std::unique\_ptr\textless{}{\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:_CPPv4N6nvidia15inferenceserver6client12InferContextE}]{\sphinxcrossref{InferContext}}}\textgreater{} *\sphinxstyleemphasis{ctx}, \sphinxbfcode{\sphinxupquote{const}} std::string \&\sphinxstyleemphasis{server\_url}, \sphinxbfcode{\sphinxupquote{const}} std::string \&\sphinxstyleemphasis{model\_name}, int64\_t \sphinxstyleemphasis{model\_version} = -1, bool \sphinxstyleemphasis{verbose} = false}{}%
\pysigstopmultiline
Create context that performs inference for a non-sequence model using HTTP protocol. 

\begin{description}
\item[{\sphinxstylestrong{Return}}] \leavevmode
{\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1Error:classnvidia_1_1inferenceserver_1_1client_1_1Error}]{\sphinxcrossref{\DUrole{std,std-ref}{Error}}}} object indicating success or failure. 

\item[{\sphinxstylestrong{Parameters}}] \leavevmode\begin{itemize}
\item {} 
\sphinxcode{\sphinxupquote{ctx}}: Returns a new {\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferHttpContext:classnvidia_1_1inferenceserver_1_1client_1_1InferHttpContext}]{\sphinxcrossref{\DUrole{std,std-ref}{InferHttpContext}}}} object. 

\item {} 
\sphinxcode{\sphinxupquote{server\_url}}: The inference server name and port. 

\item {} 
\sphinxcode{\sphinxupquote{model\_name}}: The name of the model to get status for. 

\item {} 
\sphinxcode{\sphinxupquote{model\_version}}: The version of the model to use for inference, or -1 to indicate that the latest (i.e. highest version number) version should be used. 

\item {} 
\sphinxcode{\sphinxupquote{verbose}}: If true generate verbose output when contacting the inference server. 

\end{itemize}

\end{description}


\end{fulllineitems}

\index{nvidia::inferenceserver::client::InferHttpContext::Create (C++ function)@\spxentry{nvidia::inferenceserver::client::InferHttpContext::Create}\spxextra{C++ function}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferHttpContext:_CPPv4N6nvidia15inferenceserver6client16InferHttpContext6CreateEPNSt10unique_ptrI12InferContextEE13CorrelationIDRKNSt6stringERKNSt6stringE7int64_tb}}%
\pysigstartmultiline
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferHttpContext:classnvidia_1_1inferenceserver_1_1client_1_1InferHttpContext_1ab59a20c6f183e55f0791d07e3d8c223b}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{static}} {\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1Error:_CPPv4N6nvidia15inferenceserver6client5ErrorE}]{\sphinxcrossref{Error}}} \sphinxbfcode{\sphinxupquote{Create}}}{std::unique\_ptr\textless{}{\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:_CPPv4N6nvidia15inferenceserver6client12InferContextE}]{\sphinxcrossref{InferContext}}}\textgreater{} *\sphinxstyleemphasis{ctx}, CorrelationID \sphinxstyleemphasis{correlation\_id}, \sphinxbfcode{\sphinxupquote{const}} std::string \&\sphinxstyleemphasis{server\_url}, \sphinxbfcode{\sphinxupquote{const}} std::string \&\sphinxstyleemphasis{model\_name}, int64\_t \sphinxstyleemphasis{model\_version} = -1, bool \sphinxstyleemphasis{verbose} = false}{}%
\pysigstopmultiline
Create context that performs inference for a sequence model using a given correlation ID and the HTTP protocol. 

\begin{description}
\item[{\sphinxstylestrong{Return}}] \leavevmode
{\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1Error:classnvidia_1_1inferenceserver_1_1client_1_1Error}]{\sphinxcrossref{\DUrole{std,std-ref}{Error}}}} object indicating success or failure. 

\item[{\sphinxstylestrong{Parameters}}] \leavevmode\begin{itemize}
\item {} 
\sphinxcode{\sphinxupquote{ctx}}: Returns a new {\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferHttpContext:classnvidia_1_1inferenceserver_1_1client_1_1InferHttpContext}]{\sphinxcrossref{\DUrole{std,std-ref}{InferHttpContext}}}} object. 

\item {} 
\sphinxcode{\sphinxupquote{correlation\_id}}: The correlation ID to use for all inferences performed with this context. A value of 0 (zero) indicates that no correlation ID should be used. 

\item {} 
\sphinxcode{\sphinxupquote{server\_url}}: The inference server name and port. 

\item {} 
\sphinxcode{\sphinxupquote{model\_name}}: The name of the model to get status for. 

\item {} 
\sphinxcode{\sphinxupquote{model\_version}}: The version of the model to use for inference, or -1 to indicate that the latest (i.e. highest version number) version should be used. 

\item {} 
\sphinxcode{\sphinxupquote{verbose}}: If true generate verbose output when contacting the inference server. 

\end{itemize}

\end{description}


\end{fulllineitems}


\end{fulllineitems}



\subsubsection{Class ProfileContext}
\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ProfileContext:class-profilecontext}}\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ProfileContext:exhale-class-classnvidia-1-1inferenceserver-1-1client-1-1profilecontext}}\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ProfileContext::doc}}\begin{itemize}
\item {} 
Defined in {\hyperref[\detokenize{cpp_api/file_src_clients_c++_request.h:file-src-clients-c-request-h}]{\sphinxcrossref{\DUrole{std,std-ref}{File request.h}}}}

\end{itemize}


\paragraph{Inheritance Relationships}
\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ProfileContext:inheritance-relationships}}

\subparagraph{Derived Types}
\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ProfileContext:derived-types}}\begin{itemize}
\item {} 
\sphinxcode{\sphinxupquote{public nvidia::inferenceserver::client::ProfileGrpcContext}} ({\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ProfileGrpcContext:exhale-class-classnvidia-1-1inferenceserver-1-1client-1-1profilegrpccontext}]{\sphinxcrossref{\DUrole{std,std-ref}{Class ProfileGrpcContext}}}})

\item {} 
\sphinxcode{\sphinxupquote{public nvidia::inferenceserver::client::ProfileHttpContext}} ({\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ProfileHttpContext:exhale-class-classnvidia-1-1inferenceserver-1-1client-1-1profilehttpcontext}]{\sphinxcrossref{\DUrole{std,std-ref}{Class ProfileHttpContext}}}})

\end{itemize}


\paragraph{Class Documentation}
\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ProfileContext:class-documentation}}\index{nvidia::inferenceserver::client::ProfileContext (C++ class)@\spxentry{nvidia::inferenceserver::client::ProfileContext}\spxextra{C++ class}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ProfileContext:_CPPv4N6nvidia15inferenceserver6client14ProfileContextE}}%
\pysigstartmultiline
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ProfileContext:classnvidia_1_1inferenceserver_1_1client_1_1ProfileContext}}\pysigline{\sphinxbfcode{\sphinxupquote{class }}\sphinxbfcode{\sphinxupquote{ProfileContext}}}%
\pysigstopmultiline
A {\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ProfileContext:classnvidia_1_1inferenceserver_1_1client_1_1ProfileContext}]{\sphinxcrossref{\DUrole{std,std-ref}{ProfileContext}}}} object is used to control profiling on the inference server. 

Once created a {\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ProfileContext:classnvidia_1_1inferenceserver_1_1client_1_1ProfileContext}]{\sphinxcrossref{\DUrole{std,std-ref}{ProfileContext}}}} object can be used repeatedly.

A {\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ProfileContext:classnvidia_1_1inferenceserver_1_1client_1_1ProfileContext}]{\sphinxcrossref{\DUrole{std,std-ref}{ProfileContext}}}} object can use either HTTP protocol or GRPC protocol depending on the Create function ({\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ProfileHttpContext:classnvidia_1_1inferenceserver_1_1client_1_1ProfileHttpContext_1a5a4cab76d0a3f8af074883be690ca8d5}]{\sphinxcrossref{\DUrole{std,std-ref}{ProfileHttpContext::Create}}}} or {\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ProfileGrpcContext:classnvidia_1_1inferenceserver_1_1client_1_1ProfileGrpcContext_1aea9bbc1359b9e491afcbe066386a8baa}]{\sphinxcrossref{\DUrole{std,std-ref}{ProfileGrpcContext::Create}}}}). For example:


\begin{sphinxVerbatim}[commandchars=\\\{\}]
std::unique\PYGZus{}ptr\PYGZlt{}ProfileContext\PYGZgt{} ctx;
ProfileGrpcContext::Create(\PYGZam{}ctx, \PYGZdq{}localhost:8000\PYGZdq{});
ctx\PYGZhy{}\PYGZgt{}StartProfile();
...
ctx\PYGZhy{}\PYGZgt{}StopProfile();
...
\end{sphinxVerbatim}


\begin{description}
\item[{\sphinxstylestrong{Note}}] \leavevmode
ProfileContext::Create methods are thread-safe. StartProfiling() and StopProfiling() are not thread-safe. For a given {\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ProfileContext:classnvidia_1_1inferenceserver_1_1client_1_1ProfileContext}]{\sphinxcrossref{\DUrole{std,std-ref}{ProfileContext}}}}, calls to these methods must be serialized. 

\end{description}


Subclassed by {\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ProfileGrpcContext:classnvidia_1_1inferenceserver_1_1client_1_1ProfileGrpcContext}]{\sphinxcrossref{\DUrole{std,std-ref}{nvidia::inferenceserver::client::ProfileGrpcContext}}}}, {\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ProfileHttpContext:classnvidia_1_1inferenceserver_1_1client_1_1ProfileHttpContext}]{\sphinxcrossref{\DUrole{std,std-ref}{nvidia::inferenceserver::client::ProfileHttpContext}}}}
\subsubsection*{Public Functions}
\index{nvidia::inferenceserver::client::ProfileContext::StartProfile (C++ function)@\spxentry{nvidia::inferenceserver::client::ProfileContext::StartProfile}\spxextra{C++ function}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ProfileContext:_CPPv4N6nvidia15inferenceserver6client14ProfileContext12StartProfileEv}}%
\pysigstartmultiline
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ProfileContext:classnvidia_1_1inferenceserver_1_1client_1_1ProfileContext_1a1ab52943642937237a61474900c3e063}}\pysiglinewithargsret{{\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1Error:_CPPv4N6nvidia15inferenceserver6client5ErrorE}]{\sphinxcrossref{Error}}} \sphinxbfcode{\sphinxupquote{StartProfile}}}{}{}%
\pysigstopmultiline
Start profiling on the inference server. 

\begin{description}
\item[{\sphinxstylestrong{Return}}] \leavevmode
{\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1Error:classnvidia_1_1inferenceserver_1_1client_1_1Error}]{\sphinxcrossref{\DUrole{std,std-ref}{Error}}}} object indicating success or failure. 

\end{description}


\end{fulllineitems}

\index{nvidia::inferenceserver::client::ProfileContext::StopProfile (C++ function)@\spxentry{nvidia::inferenceserver::client::ProfileContext::StopProfile}\spxextra{C++ function}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ProfileContext:_CPPv4N6nvidia15inferenceserver6client14ProfileContext11StopProfileEv}}%
\pysigstartmultiline
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ProfileContext:classnvidia_1_1inferenceserver_1_1client_1_1ProfileContext_1a70de85f4829ce4042c3696a3fdf2f091}}\pysiglinewithargsret{{\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1Error:_CPPv4N6nvidia15inferenceserver6client5ErrorE}]{\sphinxcrossref{Error}}} \sphinxbfcode{\sphinxupquote{StopProfile}}}{}{}%
\pysigstopmultiline
Stop profiling on the inference server. 

\end{fulllineitems}

\subsubsection*{Protected Functions}
\index{nvidia::inferenceserver::client::ProfileContext::ProfileContext (C++ function)@\spxentry{nvidia::inferenceserver::client::ProfileContext::ProfileContext}\spxextra{C++ function}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ProfileContext:_CPPv4N6nvidia15inferenceserver6client14ProfileContext14ProfileContextEb}}%
\pysigstartmultiline
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ProfileContext:classnvidia_1_1inferenceserver_1_1client_1_1ProfileContext_1a7781bbb5753b2fe3df63b2c67f481bda}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{ProfileContext}}}{bool}{}%
\pysigstopmultiline
\end{fulllineitems}

\index{nvidia::inferenceserver::client::ProfileContext::SendCommand (C++ function)@\spxentry{nvidia::inferenceserver::client::ProfileContext::SendCommand}\spxextra{C++ function}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ProfileContext:_CPPv4N6nvidia15inferenceserver6client14ProfileContext11SendCommandERKNSt6stringE}}%
\pysigstartmultiline
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ProfileContext:classnvidia_1_1inferenceserver_1_1client_1_1ProfileContext_1a9e56d2aaa5f73cbc20a17c189cdbecff}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{virtual}} {\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1Error:_CPPv4N6nvidia15inferenceserver6client5ErrorE}]{\sphinxcrossref{Error}}} \sphinxbfcode{\sphinxupquote{SendCommand}}}{\sphinxbfcode{\sphinxupquote{const}} std::string \&\sphinxstyleemphasis{cmd\_str}}{ = 0}%
\pysigstopmultiline
\end{fulllineitems}

\subsubsection*{Protected Attributes}
\index{nvidia::inferenceserver::client::ProfileContext::verbose\_ (C++ member)@\spxentry{nvidia::inferenceserver::client::ProfileContext::verbose\_}\spxextra{C++ member}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ProfileContext:_CPPv4N6nvidia15inferenceserver6client14ProfileContext8verbose_E}}%
\pysigstartmultiline
\pysigline{\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ProfileContext:classnvidia_1_1inferenceserver_1_1client_1_1ProfileContext_1a37bff4c6755f5c2be784b195658781e0}}\sphinxbfcode{\sphinxupquote{const}} bool \sphinxbfcode{\sphinxupquote{verbose\_}}}%
\pysigstopmultiline
\end{fulllineitems}


\end{fulllineitems}



\subsubsection{Class ProfileGrpcContext}
\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ProfileGrpcContext:class-profilegrpccontext}}\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ProfileGrpcContext:exhale-class-classnvidia-1-1inferenceserver-1-1client-1-1profilegrpccontext}}\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ProfileGrpcContext::doc}}\begin{itemize}
\item {} 
Defined in {\hyperref[\detokenize{cpp_api/file_src_clients_c++_request.h:file-src-clients-c-request-h}]{\sphinxcrossref{\DUrole{std,std-ref}{File request.h}}}}

\end{itemize}


\paragraph{Inheritance Relationships}
\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ProfileGrpcContext:inheritance-relationships}}

\subparagraph{Base Type}
\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ProfileGrpcContext:base-type}}\begin{itemize}
\item {} 
\sphinxcode{\sphinxupquote{public nvidia::inferenceserver::client::ProfileContext}} ({\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ProfileContext:exhale-class-classnvidia-1-1inferenceserver-1-1client-1-1profilecontext}]{\sphinxcrossref{\DUrole{std,std-ref}{Class ProfileContext}}}})

\end{itemize}


\paragraph{Class Documentation}
\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ProfileGrpcContext:class-documentation}}\index{nvidia::inferenceserver::client::ProfileGrpcContext (C++ class)@\spxentry{nvidia::inferenceserver::client::ProfileGrpcContext}\spxextra{C++ class}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ProfileGrpcContext:_CPPv4N6nvidia15inferenceserver6client18ProfileGrpcContextE}}%
\pysigstartmultiline
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ProfileGrpcContext:classnvidia_1_1inferenceserver_1_1client_1_1ProfileGrpcContext}}\pysigline{\sphinxbfcode{\sphinxupquote{class }}\sphinxbfcode{\sphinxupquote{ProfileGrpcContext}} : \sphinxbfcode{\sphinxupquote{public}} nvidia::inferenceserver::client::{\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ProfileContext:_CPPv4N6nvidia15inferenceserver6client14ProfileContextE}]{\sphinxcrossref{ProfileContext}}}}%
\pysigstopmultiline~\subsubsection*{Public Static Functions}
\index{nvidia::inferenceserver::client::ProfileGrpcContext::Create (C++ function)@\spxentry{nvidia::inferenceserver::client::ProfileGrpcContext::Create}\spxextra{C++ function}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ProfileGrpcContext:_CPPv4N6nvidia15inferenceserver6client18ProfileGrpcContext6CreateEPNSt10unique_ptrI14ProfileContextEERKNSt6stringEb}}%
\pysigstartmultiline
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ProfileGrpcContext:classnvidia_1_1inferenceserver_1_1client_1_1ProfileGrpcContext_1aea9bbc1359b9e491afcbe066386a8baa}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{static}} {\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1Error:_CPPv4N6nvidia15inferenceserver6client5ErrorE}]{\sphinxcrossref{Error}}} \sphinxbfcode{\sphinxupquote{Create}}}{std::unique\_ptr\textless{}{\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ProfileContext:_CPPv4N6nvidia15inferenceserver6client14ProfileContextE}]{\sphinxcrossref{ProfileContext}}}\textgreater{} *\sphinxstyleemphasis{ctx}, \sphinxbfcode{\sphinxupquote{const}} std::string \&\sphinxstyleemphasis{server\_url}, bool \sphinxstyleemphasis{verbose} = false}{}%
\pysigstopmultiline
Create context that controls profiling on a server using GRPC protocol. 

\begin{description}
\item[{\sphinxstylestrong{Return}}] \leavevmode
{\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1Error:classnvidia_1_1inferenceserver_1_1client_1_1Error}]{\sphinxcrossref{\DUrole{std,std-ref}{Error}}}} object indicating success or failure. 

\item[{\sphinxstylestrong{Parameters}}] \leavevmode\begin{itemize}
\item {} 
\sphinxcode{\sphinxupquote{ctx}}: Returns the new {\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ProfileContext:classnvidia_1_1inferenceserver_1_1client_1_1ProfileContext}]{\sphinxcrossref{\DUrole{std,std-ref}{ProfileContext}}}} object. 

\item {} 
\sphinxcode{\sphinxupquote{server\_url}}: The inference server name and port. 

\item {} 
\sphinxcode{\sphinxupquote{verbose}}: If true generate verbose output when contacting the inference server. 

\end{itemize}

\end{description}


\end{fulllineitems}


\end{fulllineitems}



\subsubsection{Class ProfileHttpContext}
\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ProfileHttpContext:class-profilehttpcontext}}\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ProfileHttpContext:exhale-class-classnvidia-1-1inferenceserver-1-1client-1-1profilehttpcontext}}\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ProfileHttpContext::doc}}\begin{itemize}
\item {} 
Defined in {\hyperref[\detokenize{cpp_api/file_src_clients_c++_request.h:file-src-clients-c-request-h}]{\sphinxcrossref{\DUrole{std,std-ref}{File request.h}}}}

\end{itemize}


\paragraph{Inheritance Relationships}
\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ProfileHttpContext:inheritance-relationships}}

\subparagraph{Base Type}
\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ProfileHttpContext:base-type}}\begin{itemize}
\item {} 
\sphinxcode{\sphinxupquote{public nvidia::inferenceserver::client::ProfileContext}} ({\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ProfileContext:exhale-class-classnvidia-1-1inferenceserver-1-1client-1-1profilecontext}]{\sphinxcrossref{\DUrole{std,std-ref}{Class ProfileContext}}}})

\end{itemize}


\paragraph{Class Documentation}
\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ProfileHttpContext:class-documentation}}\index{nvidia::inferenceserver::client::ProfileHttpContext (C++ class)@\spxentry{nvidia::inferenceserver::client::ProfileHttpContext}\spxextra{C++ class}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ProfileHttpContext:_CPPv4N6nvidia15inferenceserver6client18ProfileHttpContextE}}%
\pysigstartmultiline
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ProfileHttpContext:classnvidia_1_1inferenceserver_1_1client_1_1ProfileHttpContext}}\pysigline{\sphinxbfcode{\sphinxupquote{class }}\sphinxbfcode{\sphinxupquote{ProfileHttpContext}} : \sphinxbfcode{\sphinxupquote{public}} nvidia::inferenceserver::client::{\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ProfileContext:_CPPv4N6nvidia15inferenceserver6client14ProfileContextE}]{\sphinxcrossref{ProfileContext}}}}%
\pysigstopmultiline
{\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ProfileHttpContext:classnvidia_1_1inferenceserver_1_1client_1_1ProfileHttpContext}]{\sphinxcrossref{\DUrole{std,std-ref}{ProfileHttpContext}}}} is the HTTP instantiation of {\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ProfileContext:classnvidia_1_1inferenceserver_1_1client_1_1ProfileContext}]{\sphinxcrossref{\DUrole{std,std-ref}{ProfileContext}}}}. 
\subsubsection*{Public Static Functions}
\index{nvidia::inferenceserver::client::ProfileHttpContext::Create (C++ function)@\spxentry{nvidia::inferenceserver::client::ProfileHttpContext::Create}\spxextra{C++ function}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ProfileHttpContext:_CPPv4N6nvidia15inferenceserver6client18ProfileHttpContext6CreateEPNSt10unique_ptrI14ProfileContextEERKNSt6stringEb}}%
\pysigstartmultiline
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ProfileHttpContext:classnvidia_1_1inferenceserver_1_1client_1_1ProfileHttpContext_1a5a4cab76d0a3f8af074883be690ca8d5}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{static}} {\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1Error:_CPPv4N6nvidia15inferenceserver6client5ErrorE}]{\sphinxcrossref{Error}}} \sphinxbfcode{\sphinxupquote{Create}}}{std::unique\_ptr\textless{}{\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ProfileContext:_CPPv4N6nvidia15inferenceserver6client14ProfileContextE}]{\sphinxcrossref{ProfileContext}}}\textgreater{} *\sphinxstyleemphasis{ctx}, \sphinxbfcode{\sphinxupquote{const}} std::string \&\sphinxstyleemphasis{server\_url}, bool \sphinxstyleemphasis{verbose} = false}{}%
\pysigstopmultiline
Create context that controls profiling on a server using HTTP protocol. 

\begin{description}
\item[{\sphinxstylestrong{Return}}] \leavevmode
{\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1Error:classnvidia_1_1inferenceserver_1_1client_1_1Error}]{\sphinxcrossref{\DUrole{std,std-ref}{Error}}}} object indicating success or failure. 

\item[{\sphinxstylestrong{Parameters}}] \leavevmode\begin{itemize}
\item {} 
\sphinxcode{\sphinxupquote{ctx}}: Returns the new {\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ProfileContext:classnvidia_1_1inferenceserver_1_1client_1_1ProfileContext}]{\sphinxcrossref{\DUrole{std,std-ref}{ProfileContext}}}} object. 

\item {} 
\sphinxcode{\sphinxupquote{server\_url}}: The inference server name and port. 

\item {} 
\sphinxcode{\sphinxupquote{verbose}}: If true generate verbose output when contacting the inference server. 

\end{itemize}

\end{description}


\end{fulllineitems}


\end{fulllineitems}



\subsubsection{Class ServerHealthContext}
\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ServerHealthContext:class-serverhealthcontext}}\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ServerHealthContext:exhale-class-classnvidia-1-1inferenceserver-1-1client-1-1serverhealthcontext}}\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ServerHealthContext::doc}}\begin{itemize}
\item {} 
Defined in {\hyperref[\detokenize{cpp_api/file_src_clients_c++_request.h:file-src-clients-c-request-h}]{\sphinxcrossref{\DUrole{std,std-ref}{File request.h}}}}

\end{itemize}


\paragraph{Inheritance Relationships}
\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ServerHealthContext:inheritance-relationships}}

\subparagraph{Derived Types}
\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ServerHealthContext:derived-types}}\begin{itemize}
\item {} 
\sphinxcode{\sphinxupquote{public nvidia::inferenceserver::client::ServerHealthGrpcContext}} ({\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ServerHealthGrpcContext:exhale-class-classnvidia-1-1inferenceserver-1-1client-1-1serverhealthgrpccontext}]{\sphinxcrossref{\DUrole{std,std-ref}{Class ServerHealthGrpcContext}}}})

\item {} 
\sphinxcode{\sphinxupquote{public nvidia::inferenceserver::client::ServerHealthHttpContext}} ({\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ServerHealthHttpContext:exhale-class-classnvidia-1-1inferenceserver-1-1client-1-1serverhealthhttpcontext}]{\sphinxcrossref{\DUrole{std,std-ref}{Class ServerHealthHttpContext}}}})

\end{itemize}


\paragraph{Class Documentation}
\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ServerHealthContext:class-documentation}}\index{nvidia::inferenceserver::client::ServerHealthContext (C++ class)@\spxentry{nvidia::inferenceserver::client::ServerHealthContext}\spxextra{C++ class}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ServerHealthContext:_CPPv4N6nvidia15inferenceserver6client19ServerHealthContextE}}%
\pysigstartmultiline
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ServerHealthContext:classnvidia_1_1inferenceserver_1_1client_1_1ServerHealthContext}}\pysigline{\sphinxbfcode{\sphinxupquote{class }}\sphinxbfcode{\sphinxupquote{ServerHealthContext}}}%
\pysigstopmultiline
A {\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ServerHealthContext:classnvidia_1_1inferenceserver_1_1client_1_1ServerHealthContext}]{\sphinxcrossref{\DUrole{std,std-ref}{ServerHealthContext}}}} object is used to query an inference server for health information. 

Once created a {\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ServerHealthContext:classnvidia_1_1inferenceserver_1_1client_1_1ServerHealthContext}]{\sphinxcrossref{\DUrole{std,std-ref}{ServerHealthContext}}}} object can be used repeatedly to get health from the server. A {\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ServerHealthContext:classnvidia_1_1inferenceserver_1_1client_1_1ServerHealthContext}]{\sphinxcrossref{\DUrole{std,std-ref}{ServerHealthContext}}}} object can use either HTTP protocol or GRPC protocol depending on the Create function ({\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ServerHealthHttpContext:classnvidia_1_1inferenceserver_1_1client_1_1ServerHealthHttpContext_1a8646eed51789a020f617fe030fa8b1c4}]{\sphinxcrossref{\DUrole{std,std-ref}{ServerHealthHttpContext::Create}}}} or {\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ServerHealthGrpcContext:classnvidia_1_1inferenceserver_1_1client_1_1ServerHealthGrpcContext_1a2929e161d9d4886b48423a48eb83e21a}]{\sphinxcrossref{\DUrole{std,std-ref}{ServerHealthGrpcContext::Create}}}}). For example:


\begin{sphinxVerbatim}[commandchars=\\\{\}]
std::unique\PYGZus{}ptr\PYGZlt{}ServerHealthContext\PYGZgt{} ctx;
  ServerHealthHttpContext::Create(\PYGZam{}ctx, \PYGZdq{}localhost:8000\PYGZdq{});
  bool ready;
  ctx\PYGZhy{}\PYGZgt{}GetReady(\PYGZam{}ready);
  ...
  bool live;
  ctx\PYGZhy{}\PYGZgt{}GetLive(\PYGZam{}live);
  ...
\end{sphinxVerbatim}


\begin{description}
\item[{\sphinxstylestrong{Note}}] \leavevmode
ServerHealthContext::Create methods are thread-safe. {\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ServerHealthContext:classnvidia_1_1inferenceserver_1_1client_1_1ServerHealthContext_1a487bfc16d869317d0f4cb93144311054}]{\sphinxcrossref{\DUrole{std,std-ref}{GetReady()}}}} and {\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ServerHealthContext:classnvidia_1_1inferenceserver_1_1client_1_1ServerHealthContext_1abf7c75af3e61a2f92b481c47b0617c9c}]{\sphinxcrossref{\DUrole{std,std-ref}{GetLive()}}}} are not thread-safe. For a given {\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ServerHealthContext:classnvidia_1_1inferenceserver_1_1client_1_1ServerHealthContext}]{\sphinxcrossref{\DUrole{std,std-ref}{ServerHealthContext}}}}, calls to {\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ServerHealthContext:classnvidia_1_1inferenceserver_1_1client_1_1ServerHealthContext_1a487bfc16d869317d0f4cb93144311054}]{\sphinxcrossref{\DUrole{std,std-ref}{GetReady()}}}} and {\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ServerHealthContext:classnvidia_1_1inferenceserver_1_1client_1_1ServerHealthContext_1abf7c75af3e61a2f92b481c47b0617c9c}]{\sphinxcrossref{\DUrole{std,std-ref}{GetLive()}}}} must be serialized. 

\end{description}


Subclassed by {\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ServerHealthGrpcContext:classnvidia_1_1inferenceserver_1_1client_1_1ServerHealthGrpcContext}]{\sphinxcrossref{\DUrole{std,std-ref}{nvidia::inferenceserver::client::ServerHealthGrpcContext}}}}, {\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ServerHealthHttpContext:classnvidia_1_1inferenceserver_1_1client_1_1ServerHealthHttpContext}]{\sphinxcrossref{\DUrole{std,std-ref}{nvidia::inferenceserver::client::ServerHealthHttpContext}}}}
\subsubsection*{Public Functions}
\index{nvidia::inferenceserver::client::ServerHealthContext::GetReady (C++ function)@\spxentry{nvidia::inferenceserver::client::ServerHealthContext::GetReady}\spxextra{C++ function}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ServerHealthContext:_CPPv4N6nvidia15inferenceserver6client19ServerHealthContext8GetReadyEPb}}%
\pysigstartmultiline
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ServerHealthContext:classnvidia_1_1inferenceserver_1_1client_1_1ServerHealthContext_1a487bfc16d869317d0f4cb93144311054}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{virtual}} {\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1Error:_CPPv4N6nvidia15inferenceserver6client5ErrorE}]{\sphinxcrossref{Error}}} \sphinxbfcode{\sphinxupquote{GetReady}}}{bool *\sphinxstyleemphasis{ready}}{ = 0}%
\pysigstopmultiline
Contact the inference server and get readiness state. 

\begin{description}
\item[{\sphinxstylestrong{Return}}] \leavevmode
{\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1Error:classnvidia_1_1inferenceserver_1_1client_1_1Error}]{\sphinxcrossref{\DUrole{std,std-ref}{Error}}}} object indicating success or failure of the request. 

\item[{\sphinxstylestrong{Parameters}}] \leavevmode\begin{itemize}
\item {} 
\sphinxcode{\sphinxupquote{ready}}: Returns the readiness state of the server. 

\end{itemize}

\end{description}


\end{fulllineitems}

\index{nvidia::inferenceserver::client::ServerHealthContext::GetLive (C++ function)@\spxentry{nvidia::inferenceserver::client::ServerHealthContext::GetLive}\spxextra{C++ function}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ServerHealthContext:_CPPv4N6nvidia15inferenceserver6client19ServerHealthContext7GetLiveEPb}}%
\pysigstartmultiline
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ServerHealthContext:classnvidia_1_1inferenceserver_1_1client_1_1ServerHealthContext_1abf7c75af3e61a2f92b481c47b0617c9c}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{virtual}} {\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1Error:_CPPv4N6nvidia15inferenceserver6client5ErrorE}]{\sphinxcrossref{Error}}} \sphinxbfcode{\sphinxupquote{GetLive}}}{bool *\sphinxstyleemphasis{live}}{ = 0}%
\pysigstopmultiline
Contact the inference server and get liveness state. 

\begin{description}
\item[{\sphinxstylestrong{Return}}] \leavevmode
{\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1Error:classnvidia_1_1inferenceserver_1_1client_1_1Error}]{\sphinxcrossref{\DUrole{std,std-ref}{Error}}}} object indicating success or failure of the request. 

\item[{\sphinxstylestrong{Parameters}}] \leavevmode\begin{itemize}
\item {} 
\sphinxcode{\sphinxupquote{ready}}: Returns the liveness state of the server. 

\end{itemize}

\end{description}


\end{fulllineitems}

\subsubsection*{Protected Functions}
\index{nvidia::inferenceserver::client::ServerHealthContext::ServerHealthContext (C++ function)@\spxentry{nvidia::inferenceserver::client::ServerHealthContext::ServerHealthContext}\spxextra{C++ function}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ServerHealthContext:_CPPv4N6nvidia15inferenceserver6client19ServerHealthContext19ServerHealthContextEb}}%
\pysigstartmultiline
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ServerHealthContext:classnvidia_1_1inferenceserver_1_1client_1_1ServerHealthContext_1ac70a31c16337c59f1ace99658a7e8d37}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{ServerHealthContext}}}{bool}{}%
\pysigstopmultiline
\end{fulllineitems}

\subsubsection*{Protected Attributes}
\index{nvidia::inferenceserver::client::ServerHealthContext::verbose\_ (C++ member)@\spxentry{nvidia::inferenceserver::client::ServerHealthContext::verbose\_}\spxextra{C++ member}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ServerHealthContext:_CPPv4N6nvidia15inferenceserver6client19ServerHealthContext8verbose_E}}%
\pysigstartmultiline
\pysigline{\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ServerHealthContext:classnvidia_1_1inferenceserver_1_1client_1_1ServerHealthContext_1a85f64722e02a0a62a76984e10e4c43d4}}\sphinxbfcode{\sphinxupquote{const}} bool \sphinxbfcode{\sphinxupquote{verbose\_}}}%
\pysigstopmultiline
\end{fulllineitems}


\end{fulllineitems}



\subsubsection{Class ServerHealthGrpcContext}
\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ServerHealthGrpcContext:class-serverhealthgrpccontext}}\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ServerHealthGrpcContext:exhale-class-classnvidia-1-1inferenceserver-1-1client-1-1serverhealthgrpccontext}}\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ServerHealthGrpcContext::doc}}\begin{itemize}
\item {} 
Defined in {\hyperref[\detokenize{cpp_api/file_src_clients_c++_request.h:file-src-clients-c-request-h}]{\sphinxcrossref{\DUrole{std,std-ref}{File request.h}}}}

\end{itemize}


\paragraph{Inheritance Relationships}
\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ServerHealthGrpcContext:inheritance-relationships}}

\subparagraph{Base Type}
\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ServerHealthGrpcContext:base-type}}\begin{itemize}
\item {} 
\sphinxcode{\sphinxupquote{public nvidia::inferenceserver::client::ServerHealthContext}} ({\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ServerHealthContext:exhale-class-classnvidia-1-1inferenceserver-1-1client-1-1serverhealthcontext}]{\sphinxcrossref{\DUrole{std,std-ref}{Class ServerHealthContext}}}})

\end{itemize}


\paragraph{Class Documentation}
\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ServerHealthGrpcContext:class-documentation}}\index{nvidia::inferenceserver::client::ServerHealthGrpcContext (C++ class)@\spxentry{nvidia::inferenceserver::client::ServerHealthGrpcContext}\spxextra{C++ class}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ServerHealthGrpcContext:_CPPv4N6nvidia15inferenceserver6client23ServerHealthGrpcContextE}}%
\pysigstartmultiline
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ServerHealthGrpcContext:classnvidia_1_1inferenceserver_1_1client_1_1ServerHealthGrpcContext}}\pysigline{\sphinxbfcode{\sphinxupquote{class }}\sphinxbfcode{\sphinxupquote{ServerHealthGrpcContext}} : \sphinxbfcode{\sphinxupquote{public}} nvidia::inferenceserver::client::{\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ServerHealthContext:_CPPv4N6nvidia15inferenceserver6client19ServerHealthContextE}]{\sphinxcrossref{ServerHealthContext}}}}%
\pysigstopmultiline
{\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ServerHealthGrpcContext:classnvidia_1_1inferenceserver_1_1client_1_1ServerHealthGrpcContext}]{\sphinxcrossref{\DUrole{std,std-ref}{ServerHealthGrpcContext}}}} is the GRPC instantiation of {\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ServerHealthContext:classnvidia_1_1inferenceserver_1_1client_1_1ServerHealthContext}]{\sphinxcrossref{\DUrole{std,std-ref}{ServerHealthContext}}}}. 
\subsubsection*{Public Functions}
\index{nvidia::inferenceserver::client::ServerHealthGrpcContext::GetReady (C++ function)@\spxentry{nvidia::inferenceserver::client::ServerHealthGrpcContext::GetReady}\spxextra{C++ function}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ServerHealthGrpcContext:_CPPv4N6nvidia15inferenceserver6client23ServerHealthGrpcContext8GetReadyEPb}}%
\pysigstartmultiline
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ServerHealthGrpcContext:classnvidia_1_1inferenceserver_1_1client_1_1ServerHealthGrpcContext_1ab7eb83267e0b344b688afd3b70c1498d}}\pysiglinewithargsret{{\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1Error:_CPPv4N6nvidia15inferenceserver6client5ErrorE}]{\sphinxcrossref{Error}}} \sphinxbfcode{\sphinxupquote{GetReady}}}{bool *\sphinxstyleemphasis{ready}}{}%
\pysigstopmultiline
Contact the inference server and get readiness state. 

\begin{description}
\item[{\sphinxstylestrong{Return}}] \leavevmode
{\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1Error:classnvidia_1_1inferenceserver_1_1client_1_1Error}]{\sphinxcrossref{\DUrole{std,std-ref}{Error}}}} object indicating success or failure of the request. 

\item[{\sphinxstylestrong{Parameters}}] \leavevmode\begin{itemize}
\item {} 
\sphinxcode{\sphinxupquote{ready}}: Returns the readiness state of the server. 

\end{itemize}

\end{description}


\end{fulllineitems}

\index{nvidia::inferenceserver::client::ServerHealthGrpcContext::GetLive (C++ function)@\spxentry{nvidia::inferenceserver::client::ServerHealthGrpcContext::GetLive}\spxextra{C++ function}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ServerHealthGrpcContext:_CPPv4N6nvidia15inferenceserver6client23ServerHealthGrpcContext7GetLiveEPb}}%
\pysigstartmultiline
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ServerHealthGrpcContext:classnvidia_1_1inferenceserver_1_1client_1_1ServerHealthGrpcContext_1a6015f1c052eb2a7a8d161a128d8c62b1}}\pysiglinewithargsret{{\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1Error:_CPPv4N6nvidia15inferenceserver6client5ErrorE}]{\sphinxcrossref{Error}}} \sphinxbfcode{\sphinxupquote{GetLive}}}{bool *\sphinxstyleemphasis{live}}{}%
\pysigstopmultiline
Contact the inference server and get liveness state. 

\begin{description}
\item[{\sphinxstylestrong{Return}}] \leavevmode
{\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1Error:classnvidia_1_1inferenceserver_1_1client_1_1Error}]{\sphinxcrossref{\DUrole{std,std-ref}{Error}}}} object indicating success or failure of the request. 

\item[{\sphinxstylestrong{Parameters}}] \leavevmode\begin{itemize}
\item {} 
\sphinxcode{\sphinxupquote{ready}}: Returns the liveness state of the server. 

\end{itemize}

\end{description}


\end{fulllineitems}

\subsubsection*{Public Static Functions}
\index{nvidia::inferenceserver::client::ServerHealthGrpcContext::Create (C++ function)@\spxentry{nvidia::inferenceserver::client::ServerHealthGrpcContext::Create}\spxextra{C++ function}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ServerHealthGrpcContext:_CPPv4N6nvidia15inferenceserver6client23ServerHealthGrpcContext6CreateEPNSt10unique_ptrI19ServerHealthContextEERKNSt6stringEb}}%
\pysigstartmultiline
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ServerHealthGrpcContext:classnvidia_1_1inferenceserver_1_1client_1_1ServerHealthGrpcContext_1a2929e161d9d4886b48423a48eb83e21a}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{static}} {\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1Error:_CPPv4N6nvidia15inferenceserver6client5ErrorE}]{\sphinxcrossref{Error}}} \sphinxbfcode{\sphinxupquote{Create}}}{std::unique\_ptr\textless{}{\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ServerHealthContext:_CPPv4N6nvidia15inferenceserver6client19ServerHealthContextE}]{\sphinxcrossref{ServerHealthContext}}}\textgreater{} *\sphinxstyleemphasis{ctx}, \sphinxbfcode{\sphinxupquote{const}} std::string \&\sphinxstyleemphasis{server\_url}, bool \sphinxstyleemphasis{verbose} = false}{}%
\pysigstopmultiline
Create a context that returns health information about server. 

\begin{description}
\item[{\sphinxstylestrong{Return}}] \leavevmode
{\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1Error:classnvidia_1_1inferenceserver_1_1client_1_1Error}]{\sphinxcrossref{\DUrole{std,std-ref}{Error}}}} object indicating success or failure. 

\item[{\sphinxstylestrong{Parameters}}] \leavevmode\begin{itemize}
\item {} 
\sphinxcode{\sphinxupquote{ctx}}: Returns a new {\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ServerHealthGrpcContext:classnvidia_1_1inferenceserver_1_1client_1_1ServerHealthGrpcContext}]{\sphinxcrossref{\DUrole{std,std-ref}{ServerHealthGrpcContext}}}} object. 

\item {} 
\sphinxcode{\sphinxupquote{server\_url}}: The inference server name and port. 

\item {} 
\sphinxcode{\sphinxupquote{verbose}}: If true generate verbose output when contacting the inference server. 

\end{itemize}

\end{description}


\end{fulllineitems}


\end{fulllineitems}



\subsubsection{Class ServerHealthHttpContext}
\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ServerHealthHttpContext:class-serverhealthhttpcontext}}\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ServerHealthHttpContext:exhale-class-classnvidia-1-1inferenceserver-1-1client-1-1serverhealthhttpcontext}}\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ServerHealthHttpContext::doc}}\begin{itemize}
\item {} 
Defined in {\hyperref[\detokenize{cpp_api/file_src_clients_c++_request.h:file-src-clients-c-request-h}]{\sphinxcrossref{\DUrole{std,std-ref}{File request.h}}}}

\end{itemize}


\paragraph{Inheritance Relationships}
\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ServerHealthHttpContext:inheritance-relationships}}

\subparagraph{Base Type}
\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ServerHealthHttpContext:base-type}}\begin{itemize}
\item {} 
\sphinxcode{\sphinxupquote{public nvidia::inferenceserver::client::ServerHealthContext}} ({\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ServerHealthContext:exhale-class-classnvidia-1-1inferenceserver-1-1client-1-1serverhealthcontext}]{\sphinxcrossref{\DUrole{std,std-ref}{Class ServerHealthContext}}}})

\end{itemize}


\paragraph{Class Documentation}
\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ServerHealthHttpContext:class-documentation}}\index{nvidia::inferenceserver::client::ServerHealthHttpContext (C++ class)@\spxentry{nvidia::inferenceserver::client::ServerHealthHttpContext}\spxextra{C++ class}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ServerHealthHttpContext:_CPPv4N6nvidia15inferenceserver6client23ServerHealthHttpContextE}}%
\pysigstartmultiline
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ServerHealthHttpContext:classnvidia_1_1inferenceserver_1_1client_1_1ServerHealthHttpContext}}\pysigline{\sphinxbfcode{\sphinxupquote{class }}\sphinxbfcode{\sphinxupquote{ServerHealthHttpContext}} : \sphinxbfcode{\sphinxupquote{public}} nvidia::inferenceserver::client::{\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ServerHealthContext:_CPPv4N6nvidia15inferenceserver6client19ServerHealthContextE}]{\sphinxcrossref{ServerHealthContext}}}}%
\pysigstopmultiline
{\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ServerHealthHttpContext:classnvidia_1_1inferenceserver_1_1client_1_1ServerHealthHttpContext}]{\sphinxcrossref{\DUrole{std,std-ref}{ServerHealthHttpContext}}}} is the HTTP instantiation of {\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ServerHealthContext:classnvidia_1_1inferenceserver_1_1client_1_1ServerHealthContext}]{\sphinxcrossref{\DUrole{std,std-ref}{ServerHealthContext}}}}. 
\subsubsection*{Public Functions}
\index{nvidia::inferenceserver::client::ServerHealthHttpContext::GetReady (C++ function)@\spxentry{nvidia::inferenceserver::client::ServerHealthHttpContext::GetReady}\spxextra{C++ function}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ServerHealthHttpContext:_CPPv4N6nvidia15inferenceserver6client23ServerHealthHttpContext8GetReadyEPb}}%
\pysigstartmultiline
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ServerHealthHttpContext:classnvidia_1_1inferenceserver_1_1client_1_1ServerHealthHttpContext_1afb0a65b3486407f7ab33f1a43af115da}}\pysiglinewithargsret{{\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1Error:_CPPv4N6nvidia15inferenceserver6client5ErrorE}]{\sphinxcrossref{Error}}} \sphinxbfcode{\sphinxupquote{GetReady}}}{bool *\sphinxstyleemphasis{ready}}{}%
\pysigstopmultiline
Contact the inference server and get readiness state. 

\begin{description}
\item[{\sphinxstylestrong{Return}}] \leavevmode
{\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1Error:classnvidia_1_1inferenceserver_1_1client_1_1Error}]{\sphinxcrossref{\DUrole{std,std-ref}{Error}}}} object indicating success or failure of the request. 

\item[{\sphinxstylestrong{Parameters}}] \leavevmode\begin{itemize}
\item {} 
\sphinxcode{\sphinxupquote{ready}}: Returns the readiness state of the server. 

\end{itemize}

\end{description}


\end{fulllineitems}

\index{nvidia::inferenceserver::client::ServerHealthHttpContext::GetLive (C++ function)@\spxentry{nvidia::inferenceserver::client::ServerHealthHttpContext::GetLive}\spxextra{C++ function}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ServerHealthHttpContext:_CPPv4N6nvidia15inferenceserver6client23ServerHealthHttpContext7GetLiveEPb}}%
\pysigstartmultiline
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ServerHealthHttpContext:classnvidia_1_1inferenceserver_1_1client_1_1ServerHealthHttpContext_1ad6c96a9a3dcc20400ce3d610f80497b4}}\pysiglinewithargsret{{\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1Error:_CPPv4N6nvidia15inferenceserver6client5ErrorE}]{\sphinxcrossref{Error}}} \sphinxbfcode{\sphinxupquote{GetLive}}}{bool *\sphinxstyleemphasis{live}}{}%
\pysigstopmultiline
Contact the inference server and get liveness state. 

\begin{description}
\item[{\sphinxstylestrong{Return}}] \leavevmode
{\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1Error:classnvidia_1_1inferenceserver_1_1client_1_1Error}]{\sphinxcrossref{\DUrole{std,std-ref}{Error}}}} object indicating success or failure of the request. 

\item[{\sphinxstylestrong{Parameters}}] \leavevmode\begin{itemize}
\item {} 
\sphinxcode{\sphinxupquote{ready}}: Returns the liveness state of the server. 

\end{itemize}

\end{description}


\end{fulllineitems}

\subsubsection*{Public Static Functions}
\index{nvidia::inferenceserver::client::ServerHealthHttpContext::Create (C++ function)@\spxentry{nvidia::inferenceserver::client::ServerHealthHttpContext::Create}\spxextra{C++ function}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ServerHealthHttpContext:_CPPv4N6nvidia15inferenceserver6client23ServerHealthHttpContext6CreateEPNSt10unique_ptrI19ServerHealthContextEERKNSt6stringEb}}%
\pysigstartmultiline
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ServerHealthHttpContext:classnvidia_1_1inferenceserver_1_1client_1_1ServerHealthHttpContext_1a8646eed51789a020f617fe030fa8b1c4}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{static}} {\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1Error:_CPPv4N6nvidia15inferenceserver6client5ErrorE}]{\sphinxcrossref{Error}}} \sphinxbfcode{\sphinxupquote{Create}}}{std::unique\_ptr\textless{}{\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ServerHealthContext:_CPPv4N6nvidia15inferenceserver6client19ServerHealthContextE}]{\sphinxcrossref{ServerHealthContext}}}\textgreater{} *\sphinxstyleemphasis{ctx}, \sphinxbfcode{\sphinxupquote{const}} std::string \&\sphinxstyleemphasis{server\_url}, bool \sphinxstyleemphasis{verbose} = false}{}%
\pysigstopmultiline
Create a context that returns health information. 

\begin{description}
\item[{\sphinxstylestrong{Return}}] \leavevmode
{\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1Error:classnvidia_1_1inferenceserver_1_1client_1_1Error}]{\sphinxcrossref{\DUrole{std,std-ref}{Error}}}} object indicating success or failure. 

\item[{\sphinxstylestrong{Parameters}}] \leavevmode\begin{itemize}
\item {} 
\sphinxcode{\sphinxupquote{ctx}}: Returns a new {\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ServerHealthHttpContext:classnvidia_1_1inferenceserver_1_1client_1_1ServerHealthHttpContext}]{\sphinxcrossref{\DUrole{std,std-ref}{ServerHealthHttpContext}}}} object. 

\item {} 
\sphinxcode{\sphinxupquote{server\_url}}: The inference server name and port. 

\item {} 
\sphinxcode{\sphinxupquote{verbose}}: If true generate verbose output when contacting the inference server. 

\end{itemize}

\end{description}


\end{fulllineitems}


\end{fulllineitems}



\subsubsection{Class ServerStatusContext}
\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ServerStatusContext:class-serverstatuscontext}}\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ServerStatusContext:exhale-class-classnvidia-1-1inferenceserver-1-1client-1-1serverstatuscontext}}\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ServerStatusContext::doc}}\begin{itemize}
\item {} 
Defined in {\hyperref[\detokenize{cpp_api/file_src_clients_c++_request.h:file-src-clients-c-request-h}]{\sphinxcrossref{\DUrole{std,std-ref}{File request.h}}}}

\end{itemize}


\paragraph{Inheritance Relationships}
\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ServerStatusContext:inheritance-relationships}}

\subparagraph{Derived Types}
\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ServerStatusContext:derived-types}}\begin{itemize}
\item {} 
\sphinxcode{\sphinxupquote{public nvidia::inferenceserver::client::ServerStatusGrpcContext}} ({\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ServerStatusGrpcContext:exhale-class-classnvidia-1-1inferenceserver-1-1client-1-1serverstatusgrpccontext}]{\sphinxcrossref{\DUrole{std,std-ref}{Class ServerStatusGrpcContext}}}})

\item {} 
\sphinxcode{\sphinxupquote{public nvidia::inferenceserver::client::ServerStatusHttpContext}} ({\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ServerStatusHttpContext:exhale-class-classnvidia-1-1inferenceserver-1-1client-1-1serverstatushttpcontext}]{\sphinxcrossref{\DUrole{std,std-ref}{Class ServerStatusHttpContext}}}})

\end{itemize}


\paragraph{Class Documentation}
\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ServerStatusContext:class-documentation}}\index{nvidia::inferenceserver::client::ServerStatusContext (C++ class)@\spxentry{nvidia::inferenceserver::client::ServerStatusContext}\spxextra{C++ class}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ServerStatusContext:_CPPv4N6nvidia15inferenceserver6client19ServerStatusContextE}}%
\pysigstartmultiline
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ServerStatusContext:classnvidia_1_1inferenceserver_1_1client_1_1ServerStatusContext}}\pysigline{\sphinxbfcode{\sphinxupquote{class }}\sphinxbfcode{\sphinxupquote{ServerStatusContext}}}%
\pysigstopmultiline
A {\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ServerStatusContext:classnvidia_1_1inferenceserver_1_1client_1_1ServerStatusContext}]{\sphinxcrossref{\DUrole{std,std-ref}{ServerStatusContext}}}} object is used to query an inference server for status information, including information about the models available on that server. 

Once created a {\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ServerStatusContext:classnvidia_1_1inferenceserver_1_1client_1_1ServerStatusContext}]{\sphinxcrossref{\DUrole{std,std-ref}{ServerStatusContext}}}} object can be used repeatedly to get status from the server. A {\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ServerStatusContext:classnvidia_1_1inferenceserver_1_1client_1_1ServerStatusContext}]{\sphinxcrossref{\DUrole{std,std-ref}{ServerStatusContext}}}} object can use either HTTP protocol or GRPC protocol depending on the Create function ({\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ServerStatusHttpContext:classnvidia_1_1inferenceserver_1_1client_1_1ServerStatusHttpContext_1ab4b5945eb886c8a4e643f249d1bee2b0}]{\sphinxcrossref{\DUrole{std,std-ref}{ServerStatusHttpContext::Create}}}} or {\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ServerStatusGrpcContext:classnvidia_1_1inferenceserver_1_1client_1_1ServerStatusGrpcContext_1ae7ce9ca2694b29d64a41a438ec795486}]{\sphinxcrossref{\DUrole{std,std-ref}{ServerStatusGrpcContext::Create}}}}). For example:


\begin{sphinxVerbatim}[commandchars=\\\{\}]
std::unique\PYGZus{}ptr\PYGZlt{}ServerStatusContext\PYGZgt{} ctx;
ServerStatusHttpContext::Create(\PYGZam{}ctx, \PYGZdq{}localhost:8000\PYGZdq{});
ServerStatus status;
ctx\PYGZhy{}\PYGZgt{}GetServerStatus(\PYGZam{}status);
...
ctx\PYGZhy{}\PYGZgt{}GetServerStatus(\PYGZam{}status);
...
\end{sphinxVerbatim}


\begin{description}
\item[{\sphinxstylestrong{Note}}] \leavevmode
ServerStatusContext::Create methods are thread-safe. {\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ServerStatusContext:classnvidia_1_1inferenceserver_1_1client_1_1ServerStatusContext_1a15b5f463eb3c8315b56f21b67b151d04}]{\sphinxcrossref{\DUrole{std,std-ref}{GetServerStatus()}}}} is not thread-safe. For a given {\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ServerStatusContext:classnvidia_1_1inferenceserver_1_1client_1_1ServerStatusContext}]{\sphinxcrossref{\DUrole{std,std-ref}{ServerStatusContext}}}}, calls to {\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ServerStatusContext:classnvidia_1_1inferenceserver_1_1client_1_1ServerStatusContext_1a15b5f463eb3c8315b56f21b67b151d04}]{\sphinxcrossref{\DUrole{std,std-ref}{GetServerStatus()}}}} must be serialized. 

\end{description}


Subclassed by {\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ServerStatusGrpcContext:classnvidia_1_1inferenceserver_1_1client_1_1ServerStatusGrpcContext}]{\sphinxcrossref{\DUrole{std,std-ref}{nvidia::inferenceserver::client::ServerStatusGrpcContext}}}}, {\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ServerStatusHttpContext:classnvidia_1_1inferenceserver_1_1client_1_1ServerStatusHttpContext}]{\sphinxcrossref{\DUrole{std,std-ref}{nvidia::inferenceserver::client::ServerStatusHttpContext}}}}
\subsubsection*{Public Functions}
\index{nvidia::inferenceserver::client::ServerStatusContext::GetServerStatus (C++ function)@\spxentry{nvidia::inferenceserver::client::ServerStatusContext::GetServerStatus}\spxextra{C++ function}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ServerStatusContext:_CPPv4N6nvidia15inferenceserver6client19ServerStatusContext15GetServerStatusEP12ServerStatus}}%
\pysigstartmultiline
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ServerStatusContext:classnvidia_1_1inferenceserver_1_1client_1_1ServerStatusContext_1a15b5f463eb3c8315b56f21b67b151d04}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{virtual}} {\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1Error:_CPPv4N6nvidia15inferenceserver6client5ErrorE}]{\sphinxcrossref{Error}}} \sphinxbfcode{\sphinxupquote{GetServerStatus}}}{ServerStatus *\sphinxstyleemphasis{status}}{ = 0}%
\pysigstopmultiline
Contact the inference server and get status. 

\begin{description}
\item[{\sphinxstylestrong{Return}}] \leavevmode
{\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1Error:classnvidia_1_1inferenceserver_1_1client_1_1Error}]{\sphinxcrossref{\DUrole{std,std-ref}{Error}}}} object indicating success or failure of the request. 

\item[{\sphinxstylestrong{Parameters}}] \leavevmode\begin{itemize}
\item {} 
\sphinxcode{\sphinxupquote{status}}: Returns the status. 

\end{itemize}

\end{description}


\end{fulllineitems}

\subsubsection*{Protected Functions}
\index{nvidia::inferenceserver::client::ServerStatusContext::ServerStatusContext (C++ function)@\spxentry{nvidia::inferenceserver::client::ServerStatusContext::ServerStatusContext}\spxextra{C++ function}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ServerStatusContext:_CPPv4N6nvidia15inferenceserver6client19ServerStatusContext19ServerStatusContextEb}}%
\pysigstartmultiline
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ServerStatusContext:classnvidia_1_1inferenceserver_1_1client_1_1ServerStatusContext_1af8209b3b4715d25a66431c1e3b9fe27e}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{ServerStatusContext}}}{bool}{}%
\pysigstopmultiline
\end{fulllineitems}

\subsubsection*{Protected Attributes}
\index{nvidia::inferenceserver::client::ServerStatusContext::verbose\_ (C++ member)@\spxentry{nvidia::inferenceserver::client::ServerStatusContext::verbose\_}\spxextra{C++ member}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ServerStatusContext:_CPPv4N6nvidia15inferenceserver6client19ServerStatusContext8verbose_E}}%
\pysigstartmultiline
\pysigline{\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ServerStatusContext:classnvidia_1_1inferenceserver_1_1client_1_1ServerStatusContext_1aade2733abe3cf60dfa59bc9352c0fbf2}}\sphinxbfcode{\sphinxupquote{const}} bool \sphinxbfcode{\sphinxupquote{verbose\_}}}%
\pysigstopmultiline
\end{fulllineitems}


\end{fulllineitems}



\subsubsection{Class ServerStatusGrpcContext}
\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ServerStatusGrpcContext:class-serverstatusgrpccontext}}\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ServerStatusGrpcContext:exhale-class-classnvidia-1-1inferenceserver-1-1client-1-1serverstatusgrpccontext}}\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ServerStatusGrpcContext::doc}}\begin{itemize}
\item {} 
Defined in {\hyperref[\detokenize{cpp_api/file_src_clients_c++_request.h:file-src-clients-c-request-h}]{\sphinxcrossref{\DUrole{std,std-ref}{File request.h}}}}

\end{itemize}


\paragraph{Inheritance Relationships}
\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ServerStatusGrpcContext:inheritance-relationships}}

\subparagraph{Base Type}
\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ServerStatusGrpcContext:base-type}}\begin{itemize}
\item {} 
\sphinxcode{\sphinxupquote{public nvidia::inferenceserver::client::ServerStatusContext}} ({\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ServerStatusContext:exhale-class-classnvidia-1-1inferenceserver-1-1client-1-1serverstatuscontext}]{\sphinxcrossref{\DUrole{std,std-ref}{Class ServerStatusContext}}}})

\end{itemize}


\paragraph{Class Documentation}
\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ServerStatusGrpcContext:class-documentation}}\index{nvidia::inferenceserver::client::ServerStatusGrpcContext (C++ class)@\spxentry{nvidia::inferenceserver::client::ServerStatusGrpcContext}\spxextra{C++ class}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ServerStatusGrpcContext:_CPPv4N6nvidia15inferenceserver6client23ServerStatusGrpcContextE}}%
\pysigstartmultiline
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ServerStatusGrpcContext:classnvidia_1_1inferenceserver_1_1client_1_1ServerStatusGrpcContext}}\pysigline{\sphinxbfcode{\sphinxupquote{class }}\sphinxbfcode{\sphinxupquote{ServerStatusGrpcContext}} : \sphinxbfcode{\sphinxupquote{public}} nvidia::inferenceserver::client::{\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ServerStatusContext:_CPPv4N6nvidia15inferenceserver6client19ServerStatusContextE}]{\sphinxcrossref{ServerStatusContext}}}}%
\pysigstopmultiline
{\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ServerStatusGrpcContext:classnvidia_1_1inferenceserver_1_1client_1_1ServerStatusGrpcContext}]{\sphinxcrossref{\DUrole{std,std-ref}{ServerStatusGrpcContext}}}} is the GRPC instantiation of {\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ServerStatusContext:classnvidia_1_1inferenceserver_1_1client_1_1ServerStatusContext}]{\sphinxcrossref{\DUrole{std,std-ref}{ServerStatusContext}}}}. 
\subsubsection*{Public Functions}
\index{nvidia::inferenceserver::client::ServerStatusGrpcContext::GetServerStatus (C++ function)@\spxentry{nvidia::inferenceserver::client::ServerStatusGrpcContext::GetServerStatus}\spxextra{C++ function}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ServerStatusGrpcContext:_CPPv4N6nvidia15inferenceserver6client23ServerStatusGrpcContext15GetServerStatusEP12ServerStatus}}%
\pysigstartmultiline
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ServerStatusGrpcContext:classnvidia_1_1inferenceserver_1_1client_1_1ServerStatusGrpcContext_1a140af3b6fd9c1353a444fa54a65acd74}}\pysiglinewithargsret{{\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1Error:_CPPv4N6nvidia15inferenceserver6client5ErrorE}]{\sphinxcrossref{Error}}} \sphinxbfcode{\sphinxupquote{GetServerStatus}}}{ServerStatus *\sphinxstyleemphasis{status}}{}%
\pysigstopmultiline
Contact the inference server and get status. 

\begin{description}
\item[{\sphinxstylestrong{Return}}] \leavevmode
{\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1Error:classnvidia_1_1inferenceserver_1_1client_1_1Error}]{\sphinxcrossref{\DUrole{std,std-ref}{Error}}}} object indicating success or failure. 

\item[{\sphinxstylestrong{Parameters}}] \leavevmode\begin{itemize}
\item {} 
\sphinxcode{\sphinxupquote{status}}: Returns the status. 

\end{itemize}

\end{description}


\end{fulllineitems}

\subsubsection*{Public Static Functions}
\index{nvidia::inferenceserver::client::ServerStatusGrpcContext::Create (C++ function)@\spxentry{nvidia::inferenceserver::client::ServerStatusGrpcContext::Create}\spxextra{C++ function}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ServerStatusGrpcContext:_CPPv4N6nvidia15inferenceserver6client23ServerStatusGrpcContext6CreateEPNSt10unique_ptrI19ServerStatusContextEERKNSt6stringEb}}%
\pysigstartmultiline
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ServerStatusGrpcContext:classnvidia_1_1inferenceserver_1_1client_1_1ServerStatusGrpcContext_1ae7ce9ca2694b29d64a41a438ec795486}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{static}} {\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1Error:_CPPv4N6nvidia15inferenceserver6client5ErrorE}]{\sphinxcrossref{Error}}} \sphinxbfcode{\sphinxupquote{Create}}}{std::unique\_ptr\textless{}{\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ServerStatusContext:_CPPv4N6nvidia15inferenceserver6client19ServerStatusContextE}]{\sphinxcrossref{ServerStatusContext}}}\textgreater{} *\sphinxstyleemphasis{ctx}, \sphinxbfcode{\sphinxupquote{const}} std::string \&\sphinxstyleemphasis{server\_url}, bool \sphinxstyleemphasis{verbose} = false}{}%
\pysigstopmultiline
Create a context that returns information about an inference server and all models on the server using GRPC protocol. 

\begin{description}
\item[{\sphinxstylestrong{Return}}] \leavevmode
{\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1Error:classnvidia_1_1inferenceserver_1_1client_1_1Error}]{\sphinxcrossref{\DUrole{std,std-ref}{Error}}}} object indicating success or failure. 

\item[{\sphinxstylestrong{Parameters}}] \leavevmode\begin{itemize}
\item {} 
\sphinxcode{\sphinxupquote{ctx}}: Returns a new {\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ServerStatusGrpcContext:classnvidia_1_1inferenceserver_1_1client_1_1ServerStatusGrpcContext}]{\sphinxcrossref{\DUrole{std,std-ref}{ServerStatusGrpcContext}}}} object. 

\item {} 
\sphinxcode{\sphinxupquote{server\_url}}: The inference server name and port. 

\item {} 
\sphinxcode{\sphinxupquote{verbose}}: If true generate verbose output when contacting the inference server. 

\end{itemize}

\end{description}


\end{fulllineitems}

\index{nvidia::inferenceserver::client::ServerStatusGrpcContext::Create (C++ function)@\spxentry{nvidia::inferenceserver::client::ServerStatusGrpcContext::Create}\spxextra{C++ function}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ServerStatusGrpcContext:_CPPv4N6nvidia15inferenceserver6client23ServerStatusGrpcContext6CreateEPNSt10unique_ptrI19ServerStatusContextEERKNSt6stringERKNSt6stringEb}}%
\pysigstartmultiline
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ServerStatusGrpcContext:classnvidia_1_1inferenceserver_1_1client_1_1ServerStatusGrpcContext_1a67bee0616301552fe2b90a26d4d6d131}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{static}} {\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1Error:_CPPv4N6nvidia15inferenceserver6client5ErrorE}]{\sphinxcrossref{Error}}} \sphinxbfcode{\sphinxupquote{Create}}}{std::unique\_ptr\textless{}{\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ServerStatusContext:_CPPv4N6nvidia15inferenceserver6client19ServerStatusContextE}]{\sphinxcrossref{ServerStatusContext}}}\textgreater{} *\sphinxstyleemphasis{ctx}, \sphinxbfcode{\sphinxupquote{const}} std::string \&\sphinxstyleemphasis{server\_url}, \sphinxbfcode{\sphinxupquote{const}} std::string \&\sphinxstyleemphasis{model\_name}, bool \sphinxstyleemphasis{verbose} = false}{}%
\pysigstopmultiline
Create a context that returns information about an inference server and one model on the sever using GRPC protocol. 

\begin{description}
\item[{\sphinxstylestrong{Return}}] \leavevmode
{\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1Error:classnvidia_1_1inferenceserver_1_1client_1_1Error}]{\sphinxcrossref{\DUrole{std,std-ref}{Error}}}} object indicating success or failure. 

\item[{\sphinxstylestrong{Parameters}}] \leavevmode\begin{itemize}
\item {} 
\sphinxcode{\sphinxupquote{ctx}}: Returns a new {\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ServerStatusGrpcContext:classnvidia_1_1inferenceserver_1_1client_1_1ServerStatusGrpcContext}]{\sphinxcrossref{\DUrole{std,std-ref}{ServerStatusGrpcContext}}}} object. 

\item {} 
\sphinxcode{\sphinxupquote{server\_url}}: The inference server name and port. 

\item {} 
\sphinxcode{\sphinxupquote{model\_name}}: The name of the model to get status for. 

\item {} 
\sphinxcode{\sphinxupquote{verbose}}: If true generate verbose output when contacting the inference server. 

\end{itemize}

\end{description}


\end{fulllineitems}


\end{fulllineitems}



\subsubsection{Class ServerStatusHttpContext}
\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ServerStatusHttpContext:class-serverstatushttpcontext}}\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ServerStatusHttpContext:exhale-class-classnvidia-1-1inferenceserver-1-1client-1-1serverstatushttpcontext}}\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ServerStatusHttpContext::doc}}\begin{itemize}
\item {} 
Defined in {\hyperref[\detokenize{cpp_api/file_src_clients_c++_request.h:file-src-clients-c-request-h}]{\sphinxcrossref{\DUrole{std,std-ref}{File request.h}}}}

\end{itemize}


\paragraph{Inheritance Relationships}
\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ServerStatusHttpContext:inheritance-relationships}}

\subparagraph{Base Type}
\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ServerStatusHttpContext:base-type}}\begin{itemize}
\item {} 
\sphinxcode{\sphinxupquote{public nvidia::inferenceserver::client::ServerStatusContext}} ({\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ServerStatusContext:exhale-class-classnvidia-1-1inferenceserver-1-1client-1-1serverstatuscontext}]{\sphinxcrossref{\DUrole{std,std-ref}{Class ServerStatusContext}}}})

\end{itemize}


\paragraph{Class Documentation}
\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ServerStatusHttpContext:class-documentation}}\index{nvidia::inferenceserver::client::ServerStatusHttpContext (C++ class)@\spxentry{nvidia::inferenceserver::client::ServerStatusHttpContext}\spxextra{C++ class}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ServerStatusHttpContext:_CPPv4N6nvidia15inferenceserver6client23ServerStatusHttpContextE}}%
\pysigstartmultiline
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ServerStatusHttpContext:classnvidia_1_1inferenceserver_1_1client_1_1ServerStatusHttpContext}}\pysigline{\sphinxbfcode{\sphinxupquote{class }}\sphinxbfcode{\sphinxupquote{ServerStatusHttpContext}} : \sphinxbfcode{\sphinxupquote{public}} nvidia::inferenceserver::client::{\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ServerStatusContext:_CPPv4N6nvidia15inferenceserver6client19ServerStatusContextE}]{\sphinxcrossref{ServerStatusContext}}}}%
\pysigstopmultiline
{\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ServerStatusHttpContext:classnvidia_1_1inferenceserver_1_1client_1_1ServerStatusHttpContext}]{\sphinxcrossref{\DUrole{std,std-ref}{ServerStatusHttpContext}}}} is the HTTP instantiation of {\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ServerStatusContext:classnvidia_1_1inferenceserver_1_1client_1_1ServerStatusContext}]{\sphinxcrossref{\DUrole{std,std-ref}{ServerStatusContext}}}}. 
\subsubsection*{Public Functions}
\index{nvidia::inferenceserver::client::ServerStatusHttpContext::GetServerStatus (C++ function)@\spxentry{nvidia::inferenceserver::client::ServerStatusHttpContext::GetServerStatus}\spxextra{C++ function}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ServerStatusHttpContext:_CPPv4N6nvidia15inferenceserver6client23ServerStatusHttpContext15GetServerStatusEP12ServerStatus}}%
\pysigstartmultiline
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ServerStatusHttpContext:classnvidia_1_1inferenceserver_1_1client_1_1ServerStatusHttpContext_1a587d9241ae9b011307592b4ce0a312d3}}\pysiglinewithargsret{{\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1Error:_CPPv4N6nvidia15inferenceserver6client5ErrorE}]{\sphinxcrossref{Error}}} \sphinxbfcode{\sphinxupquote{GetServerStatus}}}{ServerStatus *\sphinxstyleemphasis{status}}{}%
\pysigstopmultiline
Contact the inference server and get status. 

\begin{description}
\item[{\sphinxstylestrong{Return}}] \leavevmode
{\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1Error:classnvidia_1_1inferenceserver_1_1client_1_1Error}]{\sphinxcrossref{\DUrole{std,std-ref}{Error}}}} object indicating success or failure. 

\item[{\sphinxstylestrong{Parameters}}] \leavevmode\begin{itemize}
\item {} 
\sphinxcode{\sphinxupquote{status}}: Returns the status. 

\end{itemize}

\end{description}


\end{fulllineitems}

\subsubsection*{Public Static Functions}
\index{nvidia::inferenceserver::client::ServerStatusHttpContext::Create (C++ function)@\spxentry{nvidia::inferenceserver::client::ServerStatusHttpContext::Create}\spxextra{C++ function}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ServerStatusHttpContext:_CPPv4N6nvidia15inferenceserver6client23ServerStatusHttpContext6CreateEPNSt10unique_ptrI19ServerStatusContextEERKNSt6stringEb}}%
\pysigstartmultiline
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ServerStatusHttpContext:classnvidia_1_1inferenceserver_1_1client_1_1ServerStatusHttpContext_1ab4b5945eb886c8a4e643f249d1bee2b0}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{static}} {\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1Error:_CPPv4N6nvidia15inferenceserver6client5ErrorE}]{\sphinxcrossref{Error}}} \sphinxbfcode{\sphinxupquote{Create}}}{std::unique\_ptr\textless{}{\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ServerStatusContext:_CPPv4N6nvidia15inferenceserver6client19ServerStatusContextE}]{\sphinxcrossref{ServerStatusContext}}}\textgreater{} *\sphinxstyleemphasis{ctx}, \sphinxbfcode{\sphinxupquote{const}} std::string \&\sphinxstyleemphasis{server\_url}, bool \sphinxstyleemphasis{verbose} = false}{}%
\pysigstopmultiline
Create a context that returns information about an inference server and all models on the server using HTTP protocol. 

\begin{description}
\item[{\sphinxstylestrong{Return}}] \leavevmode
{\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1Error:classnvidia_1_1inferenceserver_1_1client_1_1Error}]{\sphinxcrossref{\DUrole{std,std-ref}{Error}}}} object indicating success or failure. 

\item[{\sphinxstylestrong{Parameters}}] \leavevmode\begin{itemize}
\item {} 
\sphinxcode{\sphinxupquote{ctx}}: Returns a new {\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ServerStatusHttpContext:classnvidia_1_1inferenceserver_1_1client_1_1ServerStatusHttpContext}]{\sphinxcrossref{\DUrole{std,std-ref}{ServerStatusHttpContext}}}} object. 

\item {} 
\sphinxcode{\sphinxupquote{server\_url}}: The inference server name and port. 

\item {} 
\sphinxcode{\sphinxupquote{verbose}}: If true generate verbose output when contacting the inference server. 

\end{itemize}

\end{description}


\end{fulllineitems}

\index{nvidia::inferenceserver::client::ServerStatusHttpContext::Create (C++ function)@\spxentry{nvidia::inferenceserver::client::ServerStatusHttpContext::Create}\spxextra{C++ function}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ServerStatusHttpContext:_CPPv4N6nvidia15inferenceserver6client23ServerStatusHttpContext6CreateEPNSt10unique_ptrI19ServerStatusContextEERKNSt6stringERKNSt6stringEb}}%
\pysigstartmultiline
\phantomsection\label{\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ServerStatusHttpContext:classnvidia_1_1inferenceserver_1_1client_1_1ServerStatusHttpContext_1ac0cb2c45a1d73ab6f45c1397f14ba6f9}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{static}} {\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1Error:_CPPv4N6nvidia15inferenceserver6client5ErrorE}]{\sphinxcrossref{Error}}} \sphinxbfcode{\sphinxupquote{Create}}}{std::unique\_ptr\textless{}{\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ServerStatusContext:_CPPv4N6nvidia15inferenceserver6client19ServerStatusContextE}]{\sphinxcrossref{ServerStatusContext}}}\textgreater{} *\sphinxstyleemphasis{ctx}, \sphinxbfcode{\sphinxupquote{const}} std::string \&\sphinxstyleemphasis{server\_url}, \sphinxbfcode{\sphinxupquote{const}} std::string \&\sphinxstyleemphasis{model\_name}, bool \sphinxstyleemphasis{verbose} = false}{}%
\pysigstopmultiline
Create a context that returns information about an inference server and one model on the sever using HTTP protocol. 

\begin{description}
\item[{\sphinxstylestrong{Return}}] \leavevmode
{\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1Error:classnvidia_1_1inferenceserver_1_1client_1_1Error}]{\sphinxcrossref{\DUrole{std,std-ref}{Error}}}} object indicating success or failure. 

\item[{\sphinxstylestrong{Parameters}}] \leavevmode\begin{itemize}
\item {} 
\sphinxcode{\sphinxupquote{ctx}}: Returns a new {\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ServerStatusHttpContext:classnvidia_1_1inferenceserver_1_1client_1_1ServerStatusHttpContext}]{\sphinxcrossref{\DUrole{std,std-ref}{ServerStatusHttpContext}}}} object. 

\item {} 
\sphinxcode{\sphinxupquote{server\_url}}: The inference server name and port. 

\item {} 
\sphinxcode{\sphinxupquote{model\_name}}: The name of the model to get status for. 

\item {} 
\sphinxcode{\sphinxupquote{verbose}}: If true generate verbose output when contacting the inference server. 

\end{itemize}

\end{description}


\end{fulllineitems}


\end{fulllineitems}



\subsection{Functions}
\label{\detokenize{cpp_api/cpp_api_root:functions}}

\subsubsection{Function CustomErrorString}
\label{\detokenize{cpp_api/function_custom_8h_1aedfdcb69f8eaab594173b197b10dc804:function-customerrorstring}}\label{\detokenize{cpp_api/function_custom_8h_1aedfdcb69f8eaab594173b197b10dc804:exhale-function-custom-8h-1aedfdcb69f8eaab594173b197b10dc804}}\label{\detokenize{cpp_api/function_custom_8h_1aedfdcb69f8eaab594173b197b10dc804::doc}}\begin{itemize}
\item {} 
Defined in {\hyperref[\detokenize{cpp_api/file_src_servables_custom_custom.h:file-src-servables-custom-custom-h}]{\sphinxcrossref{\DUrole{std,std-ref}{File custom.h}}}}

\end{itemize}


\paragraph{Function Documentation}
\label{\detokenize{cpp_api/function_custom_8h_1aedfdcb69f8eaab594173b197b10dc804:function-documentation}}\index{CustomErrorString (C++ function)@\spxentry{CustomErrorString}\spxextra{C++ function}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/function_custom_8h_1aedfdcb69f8eaab594173b197b10dc804:_CPPv417CustomErrorStringPvi}}%
\pysigstartmultiline
\phantomsection\label{\detokenize{cpp_api/function_custom_8h_1aedfdcb69f8eaab594173b197b10dc804:custom_8h_1aedfdcb69f8eaab594173b197b10dc804}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{const}} char *\sphinxbfcode{\sphinxupquote{CustomErrorString}}}{void *\sphinxstyleemphasis{custom\_context}, int \sphinxstyleemphasis{errcode}}{}%
\pysigstopmultiline
Get the string for an error code. 

\begin{description}
\item[{\sphinxstylestrong{Return}}] \leavevmode
The error code string, or nullptr if the error code has no string representation. 

\item[{\sphinxstylestrong{Parameters}}] \leavevmode\begin{itemize}
\item {} 
\sphinxcode{\sphinxupquote{custom\_context}}: The custom state associated with the error code. Can be nullptr if no custom state. 

\item {} 
\sphinxcode{\sphinxupquote{errcode}}: The error code. 

\end{itemize}

\end{description}


\end{fulllineitems}



\subsubsection{Function CustomExecute}
\label{\detokenize{cpp_api/function_custom_8h_1afc75c0a4e1a169562586c2d4ff09ba40:function-customexecute}}\label{\detokenize{cpp_api/function_custom_8h_1afc75c0a4e1a169562586c2d4ff09ba40:exhale-function-custom-8h-1afc75c0a4e1a169562586c2d4ff09ba40}}\label{\detokenize{cpp_api/function_custom_8h_1afc75c0a4e1a169562586c2d4ff09ba40::doc}}\begin{itemize}
\item {} 
Defined in {\hyperref[\detokenize{cpp_api/file_src_servables_custom_custom.h:file-src-servables-custom-custom-h}]{\sphinxcrossref{\DUrole{std,std-ref}{File custom.h}}}}

\end{itemize}


\paragraph{Function Documentation}
\label{\detokenize{cpp_api/function_custom_8h_1afc75c0a4e1a169562586c2d4ff09ba40:function-documentation}}\index{CustomExecute (C++ function)@\spxentry{CustomExecute}\spxextra{C++ function}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/function_custom_8h_1afc75c0a4e1a169562586c2d4ff09ba40:_CPPv413CustomExecutePv8uint32_tP13CustomPayload22CustomGetNextInputFn_t19CustomGetOutputFn_t}}%
\pysigstartmultiline
\phantomsection\label{\detokenize{cpp_api/function_custom_8h_1afc75c0a4e1a169562586c2d4ff09ba40:custom_8h_1afc75c0a4e1a169562586c2d4ff09ba40}}\pysiglinewithargsret{int \sphinxbfcode{\sphinxupquote{CustomExecute}}}{void *\sphinxstyleemphasis{custom\_context}, uint32\_t \sphinxstyleemphasis{payload\_cnt}, {\hyperref[\detokenize{cpp_api/typedef_custom_8h_1a99a7b9dcc756702784a869d9afb59df4:_CPPv413CustomPayload}]{\sphinxcrossref{CustomPayload}}} *\sphinxstyleemphasis{payloads}, {\hyperref[\detokenize{cpp_api/typedef_custom_8h_1aee3723f1a17ff4664691655e056a309c:_CPPv422CustomGetNextInputFn_t}]{\sphinxcrossref{CustomGetNextInputFn\_t}}} \sphinxstyleemphasis{input\_fn}, {\hyperref[\detokenize{cpp_api/typedef_custom_8h_1a71cb419d6e7feee902115438973d02f8:_CPPv419CustomGetOutputFn_t}]{\sphinxcrossref{CustomGetOutputFn\_t}}} \sphinxstyleemphasis{output\_fn}}{}%
\pysigstopmultiline
Execute the custom model. 

\begin{description}
\item[{\sphinxstylestrong{Return}}] \leavevmode
An error code. Zero indicates success, all other values indicate failure. Use CustomErrorString to get the error string for an error code. 

\item[{\sphinxstylestrong{Parameters}}] \leavevmode\begin{itemize}
\item {} 
\sphinxcode{\sphinxupquote{custom\_context}}: The custom state associated with the context that should execute. Can be nullptr if no custom state. 

\item {} 
\sphinxcode{\sphinxupquote{payload\_cnt}}: The number of payloads to execute. 

\item {} 
\sphinxcode{\sphinxupquote{payloads}}: The payloads to execute. 

\item {} 
\sphinxcode{\sphinxupquote{input\_fn}}: The callback function to get tensor input (see CustomGetNextInputFn\_t). 

\item {} 
\sphinxcode{\sphinxupquote{output\_fn}}: The callback function to get buffer for tensor output (see CustomGetOutputFn\_t). 

\end{itemize}

\end{description}


\end{fulllineitems}



\subsubsection{Function CustomFinalize}
\label{\detokenize{cpp_api/function_custom_8h_1a9cd799dfef77db82babc6ddd4a4be707:function-customfinalize}}\label{\detokenize{cpp_api/function_custom_8h_1a9cd799dfef77db82babc6ddd4a4be707:exhale-function-custom-8h-1a9cd799dfef77db82babc6ddd4a4be707}}\label{\detokenize{cpp_api/function_custom_8h_1a9cd799dfef77db82babc6ddd4a4be707::doc}}\begin{itemize}
\item {} 
Defined in {\hyperref[\detokenize{cpp_api/file_src_servables_custom_custom.h:file-src-servables-custom-custom-h}]{\sphinxcrossref{\DUrole{std,std-ref}{File custom.h}}}}

\end{itemize}


\paragraph{Function Documentation}
\label{\detokenize{cpp_api/function_custom_8h_1a9cd799dfef77db82babc6ddd4a4be707:function-documentation}}\index{CustomFinalize (C++ function)@\spxentry{CustomFinalize}\spxextra{C++ function}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/function_custom_8h_1a9cd799dfef77db82babc6ddd4a4be707:_CPPv414CustomFinalizePv}}%
\pysigstartmultiline
\phantomsection\label{\detokenize{cpp_api/function_custom_8h_1a9cd799dfef77db82babc6ddd4a4be707:custom_8h_1a9cd799dfef77db82babc6ddd4a4be707}}\pysiglinewithargsret{int \sphinxbfcode{\sphinxupquote{CustomFinalize}}}{void *\sphinxstyleemphasis{custom\_context}}{}%
\pysigstopmultiline
Finalize a custom context. 

All state associated with the context should be freed.

\begin{description}
\item[{\sphinxstylestrong{Return}}] \leavevmode
An error code. Zero indicates success, all other values indicate failure. Use CustomErrorString to get the error string for an error code. 

\item[{\sphinxstylestrong{Parameters}}] \leavevmode\begin{itemize}
\item {} 
\sphinxcode{\sphinxupquote{custom\_context}}: The custom state associated with context that should be freed. Can be nullptr if no custom state. 

\end{itemize}

\end{description}


\end{fulllineitems}



\subsubsection{Function CustomInitialize}
\label{\detokenize{cpp_api/function_custom_8h_1ad384f6af9bfb11b82b2b4e411f799b6f:function-custominitialize}}\label{\detokenize{cpp_api/function_custom_8h_1ad384f6af9bfb11b82b2b4e411f799b6f:exhale-function-custom-8h-1ad384f6af9bfb11b82b2b4e411f799b6f}}\label{\detokenize{cpp_api/function_custom_8h_1ad384f6af9bfb11b82b2b4e411f799b6f::doc}}\begin{itemize}
\item {} 
Defined in {\hyperref[\detokenize{cpp_api/file_src_servables_custom_custom.h:file-src-servables-custom-custom-h}]{\sphinxcrossref{\DUrole{std,std-ref}{File custom.h}}}}

\end{itemize}


\paragraph{Function Documentation}
\label{\detokenize{cpp_api/function_custom_8h_1ad384f6af9bfb11b82b2b4e411f799b6f:function-documentation}}\index{CustomInitialize (C++ function)@\spxentry{CustomInitialize}\spxextra{C++ function}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/function_custom_8h_1ad384f6af9bfb11b82b2b4e411f799b6f:_CPPv416CustomInitializePKc6size_tiPPv}}%
\pysigstartmultiline
\phantomsection\label{\detokenize{cpp_api/function_custom_8h_1ad384f6af9bfb11b82b2b4e411f799b6f:custom_8h_1ad384f6af9bfb11b82b2b4e411f799b6f}}\pysiglinewithargsret{int \sphinxbfcode{\sphinxupquote{CustomInitialize}}}{\sphinxbfcode{\sphinxupquote{const}} char *\sphinxstyleemphasis{serialized\_model\_config}, size\_t \sphinxstyleemphasis{serialized\_model\_config\_size}, int \sphinxstyleemphasis{gpu\_device\_id}, void **\sphinxstyleemphasis{custom\_context}}{}%
\pysigstopmultiline
Initialize the custom shared library for a given model configuration and get the associated custom context. 

\begin{description}
\item[{\sphinxstylestrong{Return}}] \leavevmode
An error code. Zero indicates success, all other values indicate failure. Use CustomErrorString to get the error string for an error code. 

\item[{\sphinxstylestrong{Parameters}}] \leavevmode\begin{itemize}
\item {} 
\sphinxcode{\sphinxupquote{serialized\_model\_config}}: Serialized representation of the model configuration to use for initialization. This serialization is owned by the caller and so must be copied if a persistent copy of required by the shared library. 

\item {} 
\sphinxcode{\sphinxupquote{serialized\_model\_config\_size}}: The size of serialized\_model\_config, in bytes. 

\item {} 
\sphinxcode{\sphinxupquote{gpu\_device\_id}}: The GPU device ID to initialize for, or CUSTOM\_NO\_GPU\_DEVICE if should initialize for CPU. 

\item {} 
\sphinxcode{\sphinxupquote{custom\_context}}: Returns the opaque handle to the custom state associated with this initialization. Returns nullptr if no context associated with the initialization. 

\end{itemize}

\end{description}


\end{fulllineitems}



\subsubsection{Function nvidia::inferenceserver::client::operator\textless{}\textless{}}
\label{\detokenize{cpp_api/function_request_8h_1a19d352bb13848c14b5f03274b25ecee3:function-nvidia-inferenceserver-client-operator}}\label{\detokenize{cpp_api/function_request_8h_1a19d352bb13848c14b5f03274b25ecee3:exhale-function-request-8h-1a19d352bb13848c14b5f03274b25ecee3}}\label{\detokenize{cpp_api/function_request_8h_1a19d352bb13848c14b5f03274b25ecee3::doc}}\begin{itemize}
\item {} 
Defined in {\hyperref[\detokenize{cpp_api/file_src_clients_c++_request.h:file-src-clients-c-request-h}]{\sphinxcrossref{\DUrole{std,std-ref}{File request.h}}}}

\end{itemize}


\paragraph{Function Documentation}
\label{\detokenize{cpp_api/function_request_8h_1a19d352bb13848c14b5f03274b25ecee3:function-documentation}}\index{nvidia::inferenceserver::client::operator\textless{}\textless{} (C++ function)@\spxentry{nvidia::inferenceserver::client::operator\textless{}\textless{}}\spxextra{C++ function}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/function_request_8h_1a19d352bb13848c14b5f03274b25ecee3:_CPPv4N6nvidia15inferenceserver6clientlsERNSt7ostreamERK5Error}}%
\pysigstartmultiline
\phantomsection\label{\detokenize{cpp_api/function_request_8h_1a19d352bb13848c14b5f03274b25ecee3:request_8h_1a19d352bb13848c14b5f03274b25ecee3}}\pysiglinewithargsret{std::ostream \&\sphinxcode{\sphinxupquote{nvidia::inferenceserver::client\sphinxcode{\sphinxupquote{::}}}}\sphinxbfcode{\sphinxupquote{operator\textless{}\textless{}}}}{std::ostream\&, \sphinxbfcode{\sphinxupquote{const}} {\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1Error:_CPPv4N6nvidia15inferenceserver6client5ErrorE}]{\sphinxcrossref{Error}}}\&}{}%
\pysigstopmultiline
\end{fulllineitems}



\subsection{Defines}
\label{\detokenize{cpp_api/cpp_api_root:defines}}

\subsubsection{Define CUSTOM\_NO\_GPU\_DEVICE}
\label{\detokenize{cpp_api/define_custom_8h_1aaf537da73bb55fda05fb4e39defe1949:define-custom-no-gpu-device}}\label{\detokenize{cpp_api/define_custom_8h_1aaf537da73bb55fda05fb4e39defe1949:exhale-define-custom-8h-1aaf537da73bb55fda05fb4e39defe1949}}\label{\detokenize{cpp_api/define_custom_8h_1aaf537da73bb55fda05fb4e39defe1949::doc}}\begin{itemize}
\item {} 
Defined in {\hyperref[\detokenize{cpp_api/file_src_servables_custom_custom.h:file-src-servables-custom-custom-h}]{\sphinxcrossref{\DUrole{std,std-ref}{File custom.h}}}}

\end{itemize}


\paragraph{Define Documentation}
\label{\detokenize{cpp_api/define_custom_8h_1aaf537da73bb55fda05fb4e39defe1949:define-documentation}}\index{CUSTOM\_NO\_GPU\_DEVICE (C macro)@\spxentry{CUSTOM\_NO\_GPU\_DEVICE}\spxextra{C macro}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/define_custom_8h_1aaf537da73bb55fda05fb4e39defe1949:c.CUSTOM_NO_GPU_DEVICE}}\pysigline{\phantomsection\label{\detokenize{cpp_api/define_custom_8h_1aaf537da73bb55fda05fb4e39defe1949:custom_8h_1aaf537da73bb55fda05fb4e39defe1949}}\sphinxbfcode{\sphinxupquote{CUSTOM\_NO\_GPU\_DEVICE}}}
GPU device number that indicates that no GPU is available for a context. 

In CustomInitialize this value is used for ‘gpu\_device\_id’ to indicate that the model must execute on the CPU. 

\end{fulllineitems}



\subsection{Typedefs}
\label{\detokenize{cpp_api/cpp_api_root:typedefs}}

\subsubsection{Typedef CustomErrorStringFn\_t}
\label{\detokenize{cpp_api/typedef_custom_8h_1aa6616d50105d3441ba277c6dbce66286:typedef-customerrorstringfn-t}}\label{\detokenize{cpp_api/typedef_custom_8h_1aa6616d50105d3441ba277c6dbce66286:exhale-typedef-custom-8h-1aa6616d50105d3441ba277c6dbce66286}}\label{\detokenize{cpp_api/typedef_custom_8h_1aa6616d50105d3441ba277c6dbce66286::doc}}\begin{itemize}
\item {} 
Defined in {\hyperref[\detokenize{cpp_api/file_src_servables_custom_custom.h:file-src-servables-custom-custom-h}]{\sphinxcrossref{\DUrole{std,std-ref}{File custom.h}}}}

\end{itemize}


\paragraph{Typedef Documentation}
\label{\detokenize{cpp_api/typedef_custom_8h_1aa6616d50105d3441ba277c6dbce66286:typedef-documentation}}\index{CustomErrorStringFn\_t (C++ type)@\spxentry{CustomErrorStringFn\_t}\spxextra{C++ type}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/typedef_custom_8h_1aa6616d50105d3441ba277c6dbce66286:_CPPv421CustomErrorStringFn_t}}%
\pysigstartmultiline
\pysiglinewithargsret{\phantomsection\label{\detokenize{cpp_api/typedef_custom_8h_1aa6616d50105d3441ba277c6dbce66286:custom_8h_1aa6616d50105d3441ba277c6dbce66286}}\sphinxbfcode{\sphinxupquote{typedef }}char *(*\sphinxbfcode{\sphinxupquote{CustomErrorStringFn\_t}})}{void *, int}{}%
\pysigstopmultiline
Type for the CustomErrorString function. 

\end{fulllineitems}



\subsubsection{Typedef CustomExecuteFn\_t}
\label{\detokenize{cpp_api/typedef_custom_8h_1ac0c3508cd00e9db56912ff40deecf9d2:typedef-customexecutefn-t}}\label{\detokenize{cpp_api/typedef_custom_8h_1ac0c3508cd00e9db56912ff40deecf9d2:exhale-typedef-custom-8h-1ac0c3508cd00e9db56912ff40deecf9d2}}\label{\detokenize{cpp_api/typedef_custom_8h_1ac0c3508cd00e9db56912ff40deecf9d2::doc}}\begin{itemize}
\item {} 
Defined in {\hyperref[\detokenize{cpp_api/file_src_servables_custom_custom.h:file-src-servables-custom-custom-h}]{\sphinxcrossref{\DUrole{std,std-ref}{File custom.h}}}}

\end{itemize}


\paragraph{Typedef Documentation}
\label{\detokenize{cpp_api/typedef_custom_8h_1ac0c3508cd00e9db56912ff40deecf9d2:typedef-documentation}}\index{CustomExecuteFn\_t (C++ type)@\spxentry{CustomExecuteFn\_t}\spxextra{C++ type}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/typedef_custom_8h_1ac0c3508cd00e9db56912ff40deecf9d2:_CPPv417CustomExecuteFn_t}}%
\pysigstartmultiline
\pysiglinewithargsret{\phantomsection\label{\detokenize{cpp_api/typedef_custom_8h_1ac0c3508cd00e9db56912ff40deecf9d2:custom_8h_1ac0c3508cd00e9db56912ff40deecf9d2}}\sphinxbfcode{\sphinxupquote{typedef }}int (*\sphinxbfcode{\sphinxupquote{CustomExecuteFn\_t}})}{void *, uint32\_t, {\hyperref[\detokenize{cpp_api/typedef_custom_8h_1a99a7b9dcc756702784a869d9afb59df4:_CPPv413CustomPayload}]{\sphinxcrossref{CustomPayload}}} *, {\hyperref[\detokenize{cpp_api/typedef_custom_8h_1aee3723f1a17ff4664691655e056a309c:_CPPv422CustomGetNextInputFn_t}]{\sphinxcrossref{CustomGetNextInputFn\_t}}}, {\hyperref[\detokenize{cpp_api/typedef_custom_8h_1a71cb419d6e7feee902115438973d02f8:_CPPv419CustomGetOutputFn_t}]{\sphinxcrossref{CustomGetOutputFn\_t}}}}{}%
\pysigstopmultiline
Type for the CustomExecute function. 

\end{fulllineitems}



\subsubsection{Typedef CustomFinalizeFn\_t}
\label{\detokenize{cpp_api/typedef_custom_8h_1aa32f51734df1f217cc7c2b07f31f8f3f:typedef-customfinalizefn-t}}\label{\detokenize{cpp_api/typedef_custom_8h_1aa32f51734df1f217cc7c2b07f31f8f3f:exhale-typedef-custom-8h-1aa32f51734df1f217cc7c2b07f31f8f3f}}\label{\detokenize{cpp_api/typedef_custom_8h_1aa32f51734df1f217cc7c2b07f31f8f3f::doc}}\begin{itemize}
\item {} 
Defined in {\hyperref[\detokenize{cpp_api/file_src_servables_custom_custom.h:file-src-servables-custom-custom-h}]{\sphinxcrossref{\DUrole{std,std-ref}{File custom.h}}}}

\end{itemize}


\paragraph{Typedef Documentation}
\label{\detokenize{cpp_api/typedef_custom_8h_1aa32f51734df1f217cc7c2b07f31f8f3f:typedef-documentation}}\index{CustomFinalizeFn\_t (C++ type)@\spxentry{CustomFinalizeFn\_t}\spxextra{C++ type}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/typedef_custom_8h_1aa32f51734df1f217cc7c2b07f31f8f3f:_CPPv418CustomFinalizeFn_t}}%
\pysigstartmultiline
\pysiglinewithargsret{\phantomsection\label{\detokenize{cpp_api/typedef_custom_8h_1aa32f51734df1f217cc7c2b07f31f8f3f:custom_8h_1aa32f51734df1f217cc7c2b07f31f8f3f}}\sphinxbfcode{\sphinxupquote{typedef }}int (*\sphinxbfcode{\sphinxupquote{CustomFinalizeFn\_t}})}{void *}{}%
\pysigstopmultiline
Type for the CustomFinalize function. 

\end{fulllineitems}



\subsubsection{Typedef CustomGetNextInputFn\_t}
\label{\detokenize{cpp_api/typedef_custom_8h_1aee3723f1a17ff4664691655e056a309c:typedef-customgetnextinputfn-t}}\label{\detokenize{cpp_api/typedef_custom_8h_1aee3723f1a17ff4664691655e056a309c:exhale-typedef-custom-8h-1aee3723f1a17ff4664691655e056a309c}}\label{\detokenize{cpp_api/typedef_custom_8h_1aee3723f1a17ff4664691655e056a309c::doc}}\begin{itemize}
\item {} 
Defined in {\hyperref[\detokenize{cpp_api/file_src_servables_custom_custom.h:file-src-servables-custom-custom-h}]{\sphinxcrossref{\DUrole{std,std-ref}{File custom.h}}}}

\end{itemize}


\paragraph{Typedef Documentation}
\label{\detokenize{cpp_api/typedef_custom_8h_1aee3723f1a17ff4664691655e056a309c:typedef-documentation}}\index{CustomGetNextInputFn\_t (C++ type)@\spxentry{CustomGetNextInputFn\_t}\spxextra{C++ type}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/typedef_custom_8h_1aee3723f1a17ff4664691655e056a309c:_CPPv422CustomGetNextInputFn_t}}%
\pysigstartmultiline
\pysiglinewithargsret{\phantomsection\label{\detokenize{cpp_api/typedef_custom_8h_1aee3723f1a17ff4664691655e056a309c:custom_8h_1aee3723f1a17ff4664691655e056a309c}}\sphinxbfcode{\sphinxupquote{typedef }}bool (*\sphinxbfcode{\sphinxupquote{CustomGetNextInputFn\_t}})}{void *input\_context, \sphinxbfcode{\sphinxupquote{const}} char *name, \sphinxbfcode{\sphinxupquote{const}} void **content, uint64\_t *content\_byte\_size}{}%
\pysigstopmultiline
Type for the CustomGetNextInput callback function. 

This callback function is provided in the call to ComputeExecute and is used to get the value of the input tensors. Each call to this function returns a contiguous block of the input tensor value. The entire tensor value may be in multiple non-contiguous blocks and so this function must be called multiple times until ‘content’ returns nullptr.

\begin{description}
\item[{\sphinxstylestrong{Return}}] \leavevmode
false if error, true if success. 

\item[{\sphinxstylestrong{Parameters}}] \leavevmode\begin{itemize}
\item {} 
\sphinxcode{\sphinxupquote{input\_context}}: The input context provided in call to CustomExecute. 

\item {} 
\sphinxcode{\sphinxupquote{name}}: The name of the input tensor. 

\item {} 
\sphinxcode{\sphinxupquote{content}}: Returns a pointer to the next contiguous block of content for the named input. Returns nullptr if there is no more content for the input. 

\item {} 
\sphinxcode{\sphinxupquote{content\_byte\_size}}: Acts as both input and output. On input gives the maximum size expected for ‘content’. Returns the actual size, in bytes, of ‘content’. 

\end{itemize}

\end{description}


\end{fulllineitems}



\subsubsection{Typedef CustomGetOutputFn\_t}
\label{\detokenize{cpp_api/typedef_custom_8h_1a71cb419d6e7feee902115438973d02f8:typedef-customgetoutputfn-t}}\label{\detokenize{cpp_api/typedef_custom_8h_1a71cb419d6e7feee902115438973d02f8:exhale-typedef-custom-8h-1a71cb419d6e7feee902115438973d02f8}}\label{\detokenize{cpp_api/typedef_custom_8h_1a71cb419d6e7feee902115438973d02f8::doc}}\begin{itemize}
\item {} 
Defined in {\hyperref[\detokenize{cpp_api/file_src_servables_custom_custom.h:file-src-servables-custom-custom-h}]{\sphinxcrossref{\DUrole{std,std-ref}{File custom.h}}}}

\end{itemize}


\paragraph{Typedef Documentation}
\label{\detokenize{cpp_api/typedef_custom_8h_1a71cb419d6e7feee902115438973d02f8:typedef-documentation}}\index{CustomGetOutputFn\_t (C++ type)@\spxentry{CustomGetOutputFn\_t}\spxextra{C++ type}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/typedef_custom_8h_1a71cb419d6e7feee902115438973d02f8:_CPPv419CustomGetOutputFn_t}}%
\pysigstartmultiline
\pysiglinewithargsret{\phantomsection\label{\detokenize{cpp_api/typedef_custom_8h_1a71cb419d6e7feee902115438973d02f8:custom_8h_1a71cb419d6e7feee902115438973d02f8}}\sphinxbfcode{\sphinxupquote{typedef }}bool (*\sphinxbfcode{\sphinxupquote{CustomGetOutputFn\_t}})}{void *output\_context, \sphinxbfcode{\sphinxupquote{const}} char *name, size\_t shape\_dim\_cnt, int64\_t *shape\_dims, uint64\_t content\_byte\_size, void **content}{}%
\pysigstopmultiline
Type for the CustomGetOutput callback function. 

This callback function is provided in the call to ComputeExecute and is used to report the shape of an output and to get the buffers to store the output tensor values.

\begin{description}
\item[{\sphinxstylestrong{Return}}] \leavevmode
false if error, true if success. 

\item[{\sphinxstylestrong{Parameters}}] \leavevmode\begin{itemize}
\item {} 
\sphinxcode{\sphinxupquote{output\_context}}: The output context provided in call to CustomExecute. 

\item {} 
\sphinxcode{\sphinxupquote{name}}: The name of the output tensor. 

\item {} 
\sphinxcode{\sphinxupquote{shape\_dim\_cnt}}: The number of dimensions in the output shape. 

\item {} 
\sphinxcode{\sphinxupquote{shape\_dims}}: The dimensions of the output shape. 

\item {} 
\sphinxcode{\sphinxupquote{content\_byte\_size}}: The size, in bytes, of the output tensor. 

\item {} 
\sphinxcode{\sphinxupquote{content}}: Returns a pointer to a buffer where the output for the tensor should be copied. If nullptr and function returns true (no error), then the output should not be written and the backend should continue to the next output. If non-nullptr, the size of the buffer will be large enough to hold ‘content\_byte\_size’ bytes. 

\end{itemize}

\end{description}


\end{fulllineitems}



\subsubsection{Typedef CustomInitializeFn\_t}
\label{\detokenize{cpp_api/typedef_custom_8h_1a11fea108abb0b28f86a96e6b7cd92a20:typedef-custominitializefn-t}}\label{\detokenize{cpp_api/typedef_custom_8h_1a11fea108abb0b28f86a96e6b7cd92a20:exhale-typedef-custom-8h-1a11fea108abb0b28f86a96e6b7cd92a20}}\label{\detokenize{cpp_api/typedef_custom_8h_1a11fea108abb0b28f86a96e6b7cd92a20::doc}}\begin{itemize}
\item {} 
Defined in {\hyperref[\detokenize{cpp_api/file_src_servables_custom_custom.h:file-src-servables-custom-custom-h}]{\sphinxcrossref{\DUrole{std,std-ref}{File custom.h}}}}

\end{itemize}


\paragraph{Typedef Documentation}
\label{\detokenize{cpp_api/typedef_custom_8h_1a11fea108abb0b28f86a96e6b7cd92a20:typedef-documentation}}\index{CustomInitializeFn\_t (C++ type)@\spxentry{CustomInitializeFn\_t}\spxextra{C++ type}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/typedef_custom_8h_1a11fea108abb0b28f86a96e6b7cd92a20:_CPPv420CustomInitializeFn_t}}%
\pysigstartmultiline
\pysiglinewithargsret{\phantomsection\label{\detokenize{cpp_api/typedef_custom_8h_1a11fea108abb0b28f86a96e6b7cd92a20:custom_8h_1a11fea108abb0b28f86a96e6b7cd92a20}}\sphinxbfcode{\sphinxupquote{typedef }}int (*\sphinxbfcode{\sphinxupquote{CustomInitializeFn\_t}})}{\sphinxbfcode{\sphinxupquote{const}} char *, size\_t, int, void **}{}%
\pysigstopmultiline
Type for the CustomInitialize function. 

\end{fulllineitems}



\subsubsection{Typedef CustomPayload}
\label{\detokenize{cpp_api/typedef_custom_8h_1a99a7b9dcc756702784a869d9afb59df4:typedef-custompayload}}\label{\detokenize{cpp_api/typedef_custom_8h_1a99a7b9dcc756702784a869d9afb59df4:exhale-typedef-custom-8h-1a99a7b9dcc756702784a869d9afb59df4}}\label{\detokenize{cpp_api/typedef_custom_8h_1a99a7b9dcc756702784a869d9afb59df4::doc}}\begin{itemize}
\item {} 
Defined in {\hyperref[\detokenize{cpp_api/file_src_servables_custom_custom.h:file-src-servables-custom-custom-h}]{\sphinxcrossref{\DUrole{std,std-ref}{File custom.h}}}}

\end{itemize}


\paragraph{Typedef Documentation}
\label{\detokenize{cpp_api/typedef_custom_8h_1a99a7b9dcc756702784a869d9afb59df4:typedef-documentation}}\index{CustomPayload (C++ type)@\spxentry{CustomPayload}\spxextra{C++ type}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{cpp_api/typedef_custom_8h_1a99a7b9dcc756702784a869d9afb59df4:_CPPv413CustomPayload}}%
\pysigstartmultiline
\pysigline{\phantomsection\label{\detokenize{cpp_api/typedef_custom_8h_1a99a7b9dcc756702784a869d9afb59df4:custom_8h_1a99a7b9dcc756702784a869d9afb59df4}}\sphinxbfcode{\sphinxupquote{typedef }}\sphinxbfcode{\sphinxupquote{struct}} {\hyperref[\detokenize{cpp_api/structcustom__payload__struct:_CPPv421custom_payload_struct}]{\sphinxcrossref{custom\_payload\_struct}}} \sphinxbfcode{\sphinxupquote{CustomPayload}}}%
\pysigstopmultiline
\end{fulllineitems}



\subsection{Directories}
\label{\detokenize{cpp_api/cpp_api_root:directories}}

\subsubsection{Directory src}
\label{\detokenize{cpp_api/dir_src:directory-src}}\label{\detokenize{cpp_api/dir_src:dir-src}}\label{\detokenize{cpp_api/dir_src::doc}}
\sphinxstyleemphasis{Directory path:} \sphinxcode{\sphinxupquote{src}}


\paragraph{Subdirectories}
\label{\detokenize{cpp_api/dir_src:subdirectories}}\begin{itemize}
\item {} 
{\hyperref[\detokenize{cpp_api/dir_src_clients:dir-src-clients}]{\sphinxcrossref{\DUrole{std,std-ref}{Directory clients}}}}

\item {} 
{\hyperref[\detokenize{cpp_api/dir_src_servables:dir-src-servables}]{\sphinxcrossref{\DUrole{std,std-ref}{Directory servables}}}}

\end{itemize}


\subsubsection{Directory clients}
\label{\detokenize{cpp_api/dir_src_clients:directory-clients}}\label{\detokenize{cpp_api/dir_src_clients:dir-src-clients}}\label{\detokenize{cpp_api/dir_src_clients::doc}}
↰ {\hyperref[\detokenize{cpp_api/dir_src:dir-src}]{\sphinxcrossref{\DUrole{std,std-ref}{Parent directory}}}} (\sphinxcode{\sphinxupquote{src}})

\sphinxstyleemphasis{Directory path:} \sphinxcode{\sphinxupquote{src/clients}}


\paragraph{Subdirectories}
\label{\detokenize{cpp_api/dir_src_clients:subdirectories}}\begin{itemize}
\item {} 
{\hyperref[\detokenize{cpp_api/dir_src_clients_c++:dir-src-clients-c}]{\sphinxcrossref{\DUrole{std,std-ref}{Directory c++}}}}

\end{itemize}


\subsubsection{Directory c++}
\label{\detokenize{cpp_api/dir_src_clients_c++:directory-c}}\label{\detokenize{cpp_api/dir_src_clients_c++:dir-src-clients-c}}\label{\detokenize{cpp_api/dir_src_clients_c++::doc}}
↰ {\hyperref[\detokenize{cpp_api/dir_src_clients:dir-src-clients}]{\sphinxcrossref{\DUrole{std,std-ref}{Parent directory}}}} (\sphinxcode{\sphinxupquote{src/clients}})

\sphinxstyleemphasis{Directory path:} \sphinxcode{\sphinxupquote{src/clients/c++}}


\paragraph{Files}
\label{\detokenize{cpp_api/dir_src_clients_c++:files}}\begin{itemize}
\item {} 
{\hyperref[\detokenize{cpp_api/file_src_clients_c++_request.h:file-src-clients-c-request-h}]{\sphinxcrossref{\DUrole{std,std-ref}{File request.h}}}}

\end{itemize}


\subsubsection{Directory servables}
\label{\detokenize{cpp_api/dir_src_servables:directory-servables}}\label{\detokenize{cpp_api/dir_src_servables:dir-src-servables}}\label{\detokenize{cpp_api/dir_src_servables::doc}}
↰ {\hyperref[\detokenize{cpp_api/dir_src:dir-src}]{\sphinxcrossref{\DUrole{std,std-ref}{Parent directory}}}} (\sphinxcode{\sphinxupquote{src}})

\sphinxstyleemphasis{Directory path:} \sphinxcode{\sphinxupquote{src/servables}}


\paragraph{Subdirectories}
\label{\detokenize{cpp_api/dir_src_servables:subdirectories}}\begin{itemize}
\item {} 
{\hyperref[\detokenize{cpp_api/dir_src_servables_custom:dir-src-servables-custom}]{\sphinxcrossref{\DUrole{std,std-ref}{Directory custom}}}}

\end{itemize}


\subsubsection{Directory custom}
\label{\detokenize{cpp_api/dir_src_servables_custom:directory-custom}}\label{\detokenize{cpp_api/dir_src_servables_custom:dir-src-servables-custom}}\label{\detokenize{cpp_api/dir_src_servables_custom::doc}}
↰ {\hyperref[\detokenize{cpp_api/dir_src_servables:dir-src-servables}]{\sphinxcrossref{\DUrole{std,std-ref}{Parent directory}}}} (\sphinxcode{\sphinxupquote{src/servables}})

\sphinxstyleemphasis{Directory path:} \sphinxcode{\sphinxupquote{src/servables/custom}}


\paragraph{Files}
\label{\detokenize{cpp_api/dir_src_servables_custom:files}}\begin{itemize}
\item {} 
{\hyperref[\detokenize{cpp_api/file_src_servables_custom_custom.h:file-src-servables-custom-custom-h}]{\sphinxcrossref{\DUrole{std,std-ref}{File custom.h}}}}

\end{itemize}


\subsection{Files}
\label{\detokenize{cpp_api/cpp_api_root:files}}

\subsubsection{File custom.h}
\label{\detokenize{cpp_api/file_src_servables_custom_custom.h:file-custom-h}}\label{\detokenize{cpp_api/file_src_servables_custom_custom.h:file-src-servables-custom-custom-h}}\label{\detokenize{cpp_api/file_src_servables_custom_custom.h::doc}}
↰ {\hyperref[\detokenize{cpp_api/dir_src_servables_custom:dir-src-servables-custom}]{\sphinxcrossref{\DUrole{std,std-ref}{Parent directory}}}} (\sphinxcode{\sphinxupquote{src/servables/custom}})

\begin{sphinxShadowBox}
\sphinxstyletopictitle{Contents}
\begin{itemize}
\item {} 
\phantomsection\label{\detokenize{cpp_api/file_src_servables_custom_custom.h:id1}}{\hyperref[\detokenize{cpp_api/file_src_servables_custom_custom.h:definition-src-servables-custom-custom-h}]{\sphinxcrossref{Definition (\sphinxcode{\sphinxupquote{src/servables/custom/custom.h}})}}}

\item {} 
\phantomsection\label{\detokenize{cpp_api/file_src_servables_custom_custom.h:id2}}{\hyperref[\detokenize{cpp_api/file_src_servables_custom_custom.h:includes}]{\sphinxcrossref{Includes}}}

\item {} 
\phantomsection\label{\detokenize{cpp_api/file_src_servables_custom_custom.h:id3}}{\hyperref[\detokenize{cpp_api/file_src_servables_custom_custom.h:classes}]{\sphinxcrossref{Classes}}}

\item {} 
\phantomsection\label{\detokenize{cpp_api/file_src_servables_custom_custom.h:id4}}{\hyperref[\detokenize{cpp_api/file_src_servables_custom_custom.h:functions}]{\sphinxcrossref{Functions}}}

\item {} 
\phantomsection\label{\detokenize{cpp_api/file_src_servables_custom_custom.h:id5}}{\hyperref[\detokenize{cpp_api/file_src_servables_custom_custom.h:defines}]{\sphinxcrossref{Defines}}}

\item {} 
\phantomsection\label{\detokenize{cpp_api/file_src_servables_custom_custom.h:id6}}{\hyperref[\detokenize{cpp_api/file_src_servables_custom_custom.h:typedefs}]{\sphinxcrossref{Typedefs}}}

\end{itemize}
\end{sphinxShadowBox}


\paragraph{Definition (\sphinxstyleliteralintitle{\sphinxupquote{src/servables/custom/custom.h}})}
\label{\detokenize{cpp_api/file_src_servables_custom_custom.h:definition-src-servables-custom-custom-h}}

\subparagraph{Program Listing for File custom.h}
\label{\detokenize{cpp_api/program_listing_file_src_servables_custom_custom.h:program-listing-for-file-custom-h}}\label{\detokenize{cpp_api/program_listing_file_src_servables_custom_custom.h:program-listing-file-src-servables-custom-custom-h}}\label{\detokenize{cpp_api/program_listing_file_src_servables_custom_custom.h::doc}}
↰ {\hyperref[\detokenize{cpp_api/file_src_servables_custom_custom.h:file-src-servables-custom-custom-h}]{\sphinxcrossref{\DUrole{std,std-ref}{Return to documentation for file}}}} (\sphinxcode{\sphinxupquote{src/servables/custom/custom.h}})

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{// Copyright (c) 2018\PYGZhy{}2019, NVIDIA CORPORATION. All rights reserved.}
\PYG{c+c1}{//}
\PYG{c+c1}{// Redistribution and use in source and binary forms, with or without}
\PYG{c+c1}{// modification, are permitted provided that the following conditions}
\PYG{c+c1}{// are met:}
\PYG{c+c1}{//  * Redistributions of source code must retain the above copyright}
\PYG{c+c1}{//    notice, this list of conditions and the following disclaimer.}
\PYG{c+c1}{//  * Redistributions in binary form must reproduce the above copyright}
\PYG{c+c1}{//    notice, this list of conditions and the following disclaimer in the}
\PYG{c+c1}{//    documentation and/or other materials provided with the distribution.}
\PYG{c+c1}{//  * Neither the name of NVIDIA CORPORATION nor the names of its}
\PYG{c+c1}{//    contributors may be used to endorse or promote products derived}
\PYG{c+c1}{//    from this software without specific prior written permission.}
\PYG{c+c1}{//}
\PYG{c+c1}{// THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS {}`{}`AS IS\PYGZsq{}\PYGZsq{} AND ANY}
\PYG{c+c1}{// EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE}
\PYG{c+c1}{// IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR}
\PYG{c+c1}{// PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR}
\PYG{c+c1}{// CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,}
\PYG{c+c1}{// EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,}
\PYG{c+c1}{// PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR}
\PYG{c+c1}{// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY}
\PYG{c+c1}{// OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT}
\PYG{c+c1}{// (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE}
\PYG{c+c1}{// OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.}
\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{pragma once}

\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{include} \PYG{c+cpf}{\PYGZlt{}stddef.h\PYGZgt{}}
\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{include} \PYG{c+cpf}{\PYGZlt{}stdint.h\PYGZgt{}}

\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{ifdef \PYGZus{}\PYGZus{}cplusplus}
\PYG{k}{extern} \PYG{l+s}{\PYGZdq{}}\PYG{l+s}{C}\PYG{l+s}{\PYGZdq{}} \PYG{p}{\PYGZob{}}
\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{endif}

\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{define CUSTOM\PYGZus{}NO\PYGZus{}GPU\PYGZus{}DEVICE \PYGZhy{}1}

\PYG{c+c1}{// A payload represents the input tensors and the required output}
\PYG{c+c1}{// needed for execution in the backend.}
\PYG{k}{typedef} \PYG{k}{struct} \PYG{n}{custom\PYGZus{}payload\PYGZus{}struct} \PYG{p}{\PYGZob{}}
  \PYG{c+c1}{// The size of the batch represented by this payload.}
  \PYG{k+kt}{uint32\PYGZus{}t} \PYG{n}{batch\PYGZus{}size}\PYG{p}{;}

  \PYG{c+c1}{// The number of inputs included in this payload.}
  \PYG{k+kt}{uint32\PYGZus{}t} \PYG{n}{input\PYGZus{}cnt}\PYG{p}{;}

  \PYG{c+c1}{// The \PYGZsq{}input\PYGZus{}cnt\PYGZsq{} names of the inputs included in this payload.}
  \PYG{k}{const} \PYG{k+kt}{char}\PYG{o}{*}\PYG{o}{*} \PYG{n}{input\PYGZus{}names}\PYG{p}{;}

  \PYG{c+c1}{// For each of the \PYGZsq{}input\PYGZus{}cnt\PYGZsq{} inputs, the number of dimensions in}
  \PYG{c+c1}{// the input\PYGZsq{}s shape, not including the batch dimension.}
  \PYG{k}{const} \PYG{k+kt}{size\PYGZus{}t}\PYG{o}{*} \PYG{n}{input\PYGZus{}shape\PYGZus{}dim\PYGZus{}cnts}\PYG{p}{;}

  \PYG{c+c1}{// For each of the \PYGZsq{}input\PYGZus{}cnt\PYGZsq{} inputs, the shape of the input, not}
  \PYG{c+c1}{// including the batch dimension.}
  \PYG{k}{const} \PYG{k+kt}{int64\PYGZus{}t}\PYG{o}{*}\PYG{o}{*} \PYG{n}{input\PYGZus{}shape\PYGZus{}dims}\PYG{p}{;}

  \PYG{c+c1}{// The number of outputs that must be computed for this payload. Can}
  \PYG{c+c1}{// be 0 to indicate that no outputs are required from the backend.}
  \PYG{k+kt}{uint32\PYGZus{}t} \PYG{n}{output\PYGZus{}cnt}\PYG{p}{;}

  \PYG{c+c1}{// The \PYGZsq{}output\PYGZus{}cnt\PYGZsq{} names of the outputs that must be computed for}
  \PYG{c+c1}{// this payload. Each name must be one of the names from the model}
  \PYG{c+c1}{// configuration, but all outputs do not need to be computed.}
  \PYG{k}{const} \PYG{k+kt}{char}\PYG{o}{*}\PYG{o}{*} \PYG{n}{required\PYGZus{}output\PYGZus{}names}\PYG{p}{;}

  \PYG{c+c1}{// The context to use with CustomGetNextInput callback function to}
  \PYG{c+c1}{// get the input tensor values for this payload.}
  \PYG{k+kt}{void}\PYG{o}{*} \PYG{n}{input\PYGZus{}context}\PYG{p}{;}

  \PYG{c+c1}{// The context to use with CustomGetOutput callback function to get}
  \PYG{c+c1}{// the buffer for output tensor values for this payload.}
  \PYG{k+kt}{void}\PYG{o}{*} \PYG{n}{output\PYGZus{}context}\PYG{p}{;}

  \PYG{c+c1}{// The error code indicating success or failure from execution. A}
  \PYG{c+c1}{// value of 0 (zero) indicates success, all other values indicate}
  \PYG{c+c1}{// failure and are backend defined.}
  \PYG{k+kt}{int} \PYG{n}{error\PYGZus{}code}\PYG{p}{;}
\PYG{p}{\PYGZcb{}} \PYG{n}{CustomPayload}\PYG{p}{;}

\PYG{k}{typedef} \PYG{n+nf}{bool} \PYG{p}{(}\PYG{o}{*}\PYG{n}{CustomGetNextInputFn\PYGZus{}t}\PYG{p}{)}\PYG{p}{(}
    \PYG{k+kt}{void}\PYG{o}{*} \PYG{n}{input\PYGZus{}context}\PYG{p}{,} \PYG{k}{const} \PYG{k+kt}{char}\PYG{o}{*} \PYG{n}{name}\PYG{p}{,} \PYG{k}{const} \PYG{k+kt}{void}\PYG{o}{*}\PYG{o}{*} \PYG{n}{content}\PYG{p}{,}
    \PYG{k+kt}{uint64\PYGZus{}t}\PYG{o}{*} \PYG{n}{content\PYGZus{}byte\PYGZus{}size}\PYG{p}{)}\PYG{p}{;}

\PYG{k}{typedef} \PYG{n+nf}{bool} \PYG{p}{(}\PYG{o}{*}\PYG{n}{CustomGetOutputFn\PYGZus{}t}\PYG{p}{)}\PYG{p}{(}
    \PYG{k+kt}{void}\PYG{o}{*} \PYG{n}{output\PYGZus{}context}\PYG{p}{,} \PYG{k}{const} \PYG{k+kt}{char}\PYG{o}{*} \PYG{n}{name}\PYG{p}{,} \PYG{k+kt}{size\PYGZus{}t} \PYG{n}{shape\PYGZus{}dim\PYGZus{}cnt}\PYG{p}{,}
    \PYG{k+kt}{int64\PYGZus{}t}\PYG{o}{*} \PYG{n}{shape\PYGZus{}dims}\PYG{p}{,} \PYG{k+kt}{uint64\PYGZus{}t} \PYG{n}{content\PYGZus{}byte\PYGZus{}size}\PYG{p}{,} \PYG{k+kt}{void}\PYG{o}{*}\PYG{o}{*} \PYG{n}{content}\PYG{p}{)}\PYG{p}{;}

\PYG{k}{typedef} \PYG{n+nf}{int} \PYG{p}{(}\PYG{o}{*}\PYG{n}{CustomInitializeFn\PYGZus{}t}\PYG{p}{)}\PYG{p}{(}\PYG{k}{const} \PYG{k+kt}{char}\PYG{o}{*}\PYG{p}{,} \PYG{k+kt}{size\PYGZus{}t}\PYG{p}{,} \PYG{k+kt}{int}\PYG{p}{,} \PYG{k+kt}{void}\PYG{o}{*}\PYG{o}{*}\PYG{p}{)}\PYG{p}{;}

\PYG{k}{typedef} \PYG{n+nf}{int} \PYG{p}{(}\PYG{o}{*}\PYG{n}{CustomFinalizeFn\PYGZus{}t}\PYG{p}{)}\PYG{p}{(}\PYG{k+kt}{void}\PYG{o}{*}\PYG{p}{)}\PYG{p}{;}

\PYG{k}{typedef} \PYG{k+kt}{char}\PYG{o}{*} \PYG{p}{(}\PYG{o}{*}\PYG{n}{CustomErrorStringFn\PYGZus{}t}\PYG{p}{)}\PYG{p}{(}\PYG{k+kt}{void}\PYG{o}{*}\PYG{p}{,} \PYG{k+kt}{int}\PYG{p}{)}\PYG{p}{;}

\PYG{k}{typedef} \PYG{n+nf}{int} \PYG{p}{(}\PYG{o}{*}\PYG{n}{CustomExecuteFn\PYGZus{}t}\PYG{p}{)}\PYG{p}{(}
    \PYG{k+kt}{void}\PYG{o}{*}\PYG{p}{,} \PYG{k+kt}{uint32\PYGZus{}t}\PYG{p}{,} \PYG{n}{CustomPayload}\PYG{o}{*}\PYG{p}{,} \PYG{n}{CustomGetNextInputFn\PYGZus{}t}\PYG{p}{,}
    \PYG{n}{CustomGetOutputFn\PYGZus{}t}\PYG{p}{)}\PYG{p}{;}

\PYG{k+kt}{int} \PYG{n+nf}{CustomInitialize}\PYG{p}{(}
    \PYG{k}{const} \PYG{k+kt}{char}\PYG{o}{*} \PYG{n}{serialized\PYGZus{}model\PYGZus{}config}\PYG{p}{,} \PYG{k+kt}{size\PYGZus{}t} \PYG{n}{serialized\PYGZus{}model\PYGZus{}config\PYGZus{}size}\PYG{p}{,}
    \PYG{k+kt}{int} \PYG{n}{gpu\PYGZus{}device\PYGZus{}id}\PYG{p}{,} \PYG{k+kt}{void}\PYG{o}{*}\PYG{o}{*} \PYG{n}{custom\PYGZus{}context}\PYG{p}{)}\PYG{p}{;}

\PYG{k+kt}{int} \PYG{n+nf}{CustomFinalize}\PYG{p}{(}\PYG{k+kt}{void}\PYG{o}{*} \PYG{n}{custom\PYGZus{}context}\PYG{p}{)}\PYG{p}{;}

\PYG{k}{const} \PYG{k+kt}{char}\PYG{o}{*} \PYG{n+nf}{CustomErrorString}\PYG{p}{(}\PYG{k+kt}{void}\PYG{o}{*} \PYG{n}{custom\PYGZus{}context}\PYG{p}{,} \PYG{k+kt}{int} \PYG{n}{errcode}\PYG{p}{)}\PYG{p}{;}

\PYG{k+kt}{int} \PYG{n+nf}{CustomExecute}\PYG{p}{(}
    \PYG{k+kt}{void}\PYG{o}{*} \PYG{n}{custom\PYGZus{}context}\PYG{p}{,} \PYG{k+kt}{uint32\PYGZus{}t} \PYG{n}{payload\PYGZus{}cnt}\PYG{p}{,} \PYG{n}{CustomPayload}\PYG{o}{*} \PYG{n}{payloads}\PYG{p}{,}
    \PYG{n}{CustomGetNextInputFn\PYGZus{}t} \PYG{n}{input\PYGZus{}fn}\PYG{p}{,} \PYG{n}{CustomGetOutputFn\PYGZus{}t} \PYG{n}{output\PYGZus{}fn}\PYG{p}{)}\PYG{p}{;}

\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{ifdef \PYGZus{}\PYGZus{}cplusplus}
\PYG{p}{\PYGZcb{}}
\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{endif}
\end{sphinxVerbatim}


\paragraph{Includes}
\label{\detokenize{cpp_api/file_src_servables_custom_custom.h:includes}}\begin{itemize}
\item {} 
\sphinxcode{\sphinxupquote{stddef.h}}

\item {} 
\sphinxcode{\sphinxupquote{stdint.h}}

\end{itemize}


\paragraph{Classes}
\label{\detokenize{cpp_api/file_src_servables_custom_custom.h:classes}}\begin{itemize}
\item {} 
{\hyperref[\detokenize{cpp_api/structcustom__payload__struct:exhale-struct-structcustom-payload-struct}]{\sphinxcrossref{\DUrole{std,std-ref}{Struct custom\_payload\_struct}}}}

\end{itemize}


\paragraph{Functions}
\label{\detokenize{cpp_api/file_src_servables_custom_custom.h:functions}}\begin{itemize}
\item {} 
{\hyperref[\detokenize{cpp_api/function_custom_8h_1aedfdcb69f8eaab594173b197b10dc804:exhale-function-custom-8h-1aedfdcb69f8eaab594173b197b10dc804}]{\sphinxcrossref{\DUrole{std,std-ref}{Function CustomErrorString}}}}

\item {} 
{\hyperref[\detokenize{cpp_api/function_custom_8h_1afc75c0a4e1a169562586c2d4ff09ba40:exhale-function-custom-8h-1afc75c0a4e1a169562586c2d4ff09ba40}]{\sphinxcrossref{\DUrole{std,std-ref}{Function CustomExecute}}}}

\item {} 
{\hyperref[\detokenize{cpp_api/function_custom_8h_1a9cd799dfef77db82babc6ddd4a4be707:exhale-function-custom-8h-1a9cd799dfef77db82babc6ddd4a4be707}]{\sphinxcrossref{\DUrole{std,std-ref}{Function CustomFinalize}}}}

\item {} 
{\hyperref[\detokenize{cpp_api/function_custom_8h_1ad384f6af9bfb11b82b2b4e411f799b6f:exhale-function-custom-8h-1ad384f6af9bfb11b82b2b4e411f799b6f}]{\sphinxcrossref{\DUrole{std,std-ref}{Function CustomInitialize}}}}

\end{itemize}


\paragraph{Defines}
\label{\detokenize{cpp_api/file_src_servables_custom_custom.h:defines}}\begin{itemize}
\item {} 
{\hyperref[\detokenize{cpp_api/define_custom_8h_1aaf537da73bb55fda05fb4e39defe1949:exhale-define-custom-8h-1aaf537da73bb55fda05fb4e39defe1949}]{\sphinxcrossref{\DUrole{std,std-ref}{Define CUSTOM\_NO\_GPU\_DEVICE}}}}

\end{itemize}


\paragraph{Typedefs}
\label{\detokenize{cpp_api/file_src_servables_custom_custom.h:typedefs}}\begin{itemize}
\item {} 
{\hyperref[\detokenize{cpp_api/typedef_custom_8h_1aa6616d50105d3441ba277c6dbce66286:exhale-typedef-custom-8h-1aa6616d50105d3441ba277c6dbce66286}]{\sphinxcrossref{\DUrole{std,std-ref}{Typedef CustomErrorStringFn\_t}}}}

\item {} 
{\hyperref[\detokenize{cpp_api/typedef_custom_8h_1ac0c3508cd00e9db56912ff40deecf9d2:exhale-typedef-custom-8h-1ac0c3508cd00e9db56912ff40deecf9d2}]{\sphinxcrossref{\DUrole{std,std-ref}{Typedef CustomExecuteFn\_t}}}}

\item {} 
{\hyperref[\detokenize{cpp_api/typedef_custom_8h_1aa32f51734df1f217cc7c2b07f31f8f3f:exhale-typedef-custom-8h-1aa32f51734df1f217cc7c2b07f31f8f3f}]{\sphinxcrossref{\DUrole{std,std-ref}{Typedef CustomFinalizeFn\_t}}}}

\item {} 
{\hyperref[\detokenize{cpp_api/typedef_custom_8h_1aee3723f1a17ff4664691655e056a309c:exhale-typedef-custom-8h-1aee3723f1a17ff4664691655e056a309c}]{\sphinxcrossref{\DUrole{std,std-ref}{Typedef CustomGetNextInputFn\_t}}}}

\item {} 
{\hyperref[\detokenize{cpp_api/typedef_custom_8h_1a71cb419d6e7feee902115438973d02f8:exhale-typedef-custom-8h-1a71cb419d6e7feee902115438973d02f8}]{\sphinxcrossref{\DUrole{std,std-ref}{Typedef CustomGetOutputFn\_t}}}}

\item {} 
{\hyperref[\detokenize{cpp_api/typedef_custom_8h_1a11fea108abb0b28f86a96e6b7cd92a20:exhale-typedef-custom-8h-1a11fea108abb0b28f86a96e6b7cd92a20}]{\sphinxcrossref{\DUrole{std,std-ref}{Typedef CustomInitializeFn\_t}}}}

\item {} 
{\hyperref[\detokenize{cpp_api/typedef_custom_8h_1a99a7b9dcc756702784a869d9afb59df4:exhale-typedef-custom-8h-1a99a7b9dcc756702784a869d9afb59df4}]{\sphinxcrossref{\DUrole{std,std-ref}{Typedef CustomPayload}}}}

\end{itemize}


\subsubsection{File request.h}
\label{\detokenize{cpp_api/file_src_clients_c++_request.h:file-request-h}}\label{\detokenize{cpp_api/file_src_clients_c++_request.h:file-src-clients-c-request-h}}\label{\detokenize{cpp_api/file_src_clients_c++_request.h::doc}}
↰ {\hyperref[\detokenize{cpp_api/dir_src_clients_c++:dir-src-clients-c}]{\sphinxcrossref{\DUrole{std,std-ref}{Parent directory}}}} (\sphinxcode{\sphinxupquote{src/clients/c++}})

\begin{sphinxShadowBox}
\sphinxstyletopictitle{Contents}
\begin{itemize}
\item {} 
\phantomsection\label{\detokenize{cpp_api/file_src_clients_c++_request.h:id1}}{\hyperref[\detokenize{cpp_api/file_src_clients_c++_request.h:definition-src-clients-c-request-h}]{\sphinxcrossref{Definition (\sphinxcode{\sphinxupquote{src/clients/c++/request.h}})}}}

\item {} 
\phantomsection\label{\detokenize{cpp_api/file_src_clients_c++_request.h:id2}}{\hyperref[\detokenize{cpp_api/file_src_clients_c++_request.h:includes}]{\sphinxcrossref{Includes}}}

\item {} 
\phantomsection\label{\detokenize{cpp_api/file_src_clients_c++_request.h:id3}}{\hyperref[\detokenize{cpp_api/file_src_clients_c++_request.h:namespaces}]{\sphinxcrossref{Namespaces}}}

\item {} 
\phantomsection\label{\detokenize{cpp_api/file_src_clients_c++_request.h:id4}}{\hyperref[\detokenize{cpp_api/file_src_clients_c++_request.h:classes}]{\sphinxcrossref{Classes}}}

\item {} 
\phantomsection\label{\detokenize{cpp_api/file_src_clients_c++_request.h:id5}}{\hyperref[\detokenize{cpp_api/file_src_clients_c++_request.h:functions}]{\sphinxcrossref{Functions}}}

\end{itemize}
\end{sphinxShadowBox}


\paragraph{Definition (\sphinxstyleliteralintitle{\sphinxupquote{src/clients/c++/request.h}})}
\label{\detokenize{cpp_api/file_src_clients_c++_request.h:definition-src-clients-c-request-h}}

\subparagraph{Program Listing for File request.h}
\label{\detokenize{cpp_api/program_listing_file_src_clients_c++_request.h:program-listing-for-file-request-h}}\label{\detokenize{cpp_api/program_listing_file_src_clients_c++_request.h:program-listing-file-src-clients-c-request-h}}\label{\detokenize{cpp_api/program_listing_file_src_clients_c++_request.h::doc}}
↰ {\hyperref[\detokenize{cpp_api/file_src_clients_c++_request.h:file-src-clients-c-request-h}]{\sphinxcrossref{\DUrole{std,std-ref}{Return to documentation for file}}}} (\sphinxcode{\sphinxupquote{src/clients/c++/request.h}})

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{// Copyright (c) 2018\PYGZhy{}2019, NVIDIA CORPORATION. All rights reserved.}
\PYG{c+c1}{//}
\PYG{c+c1}{// Redistribution and use in source and binary forms, with or without}
\PYG{c+c1}{// modification, are permitted provided that the following conditions}
\PYG{c+c1}{// are met:}
\PYG{c+c1}{//  * Redistributions of source code must retain the above copyright}
\PYG{c+c1}{//    notice, this list of conditions and the following disclaimer.}
\PYG{c+c1}{//  * Redistributions in binary form must reproduce the above copyright}
\PYG{c+c1}{//    notice, this list of conditions and the following disclaimer in the}
\PYG{c+c1}{//    documentation and/or other materials provided with the distribution.}
\PYG{c+c1}{//  * Neither the name of NVIDIA CORPORATION nor the names of its}
\PYG{c+c1}{//    contributors may be used to endorse or promote products derived}
\PYG{c+c1}{//    from this software without specific prior written permission.}
\PYG{c+c1}{//}
\PYG{c+c1}{// THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS {}`{}`AS IS\PYGZsq{}\PYGZsq{} AND ANY}
\PYG{c+c1}{// EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE}
\PYG{c+c1}{// IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR}
\PYG{c+c1}{// PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR}
\PYG{c+c1}{// CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,}
\PYG{c+c1}{// EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,}
\PYG{c+c1}{// PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR}
\PYG{c+c1}{// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY}
\PYG{c+c1}{// OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT}
\PYG{c+c1}{// (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE}
\PYG{c+c1}{// OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.}
\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{pragma once}


\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{include} \PYG{c+cpf}{\PYGZlt{}curl/curl.h\PYGZgt{}}
\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{include} \PYG{c+cpf}{\PYGZlt{}grpcpp/grpcpp.h\PYGZgt{}}
\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{include} \PYG{c+cpf}{\PYGZlt{}condition\PYGZus{}variable\PYGZgt{}}
\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{include} \PYG{c+cpf}{\PYGZlt{}memory\PYGZgt{}}
\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{include} \PYG{c+cpf}{\PYGZlt{}mutex\PYGZgt{}}
\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{include} \PYG{c+cpf}{\PYGZlt{}string\PYGZgt{}}
\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{include} \PYG{c+cpf}{\PYGZlt{}thread\PYGZgt{}}
\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{include} \PYG{c+cpf}{\PYGZlt{}vector\PYGZgt{}}
\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{include} \PYG{c+cpf}{\PYGZdq{}src/core/api.pb.h\PYGZdq{}}
\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{include} \PYG{c+cpf}{\PYGZdq{}src/core/grpc\PYGZus{}service.grpc.pb.h\PYGZdq{}}
\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{include} \PYG{c+cpf}{\PYGZdq{}src/core/grpc\PYGZus{}service.pb.h\PYGZdq{}}
\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{include} \PYG{c+cpf}{\PYGZdq{}src/core/model\PYGZus{}config.h\PYGZdq{}}
\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{include} \PYG{c+cpf}{\PYGZdq{}src/core/model\PYGZus{}config.pb.h\PYGZdq{}}
\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{include} \PYG{c+cpf}{\PYGZdq{}src/core/request\PYGZus{}status.pb.h\PYGZdq{}}
\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{include} \PYG{c+cpf}{\PYGZdq{}src/core/server\PYGZus{}status.pb.h\PYGZdq{}}

\PYG{k}{namespace} \PYG{n}{nvidia} \PYG{p}{\PYGZob{}} \PYG{k}{namespace} \PYG{n}{inferenceserver} \PYG{p}{\PYGZob{}} \PYG{k}{namespace} \PYG{n}{client} \PYG{p}{\PYGZob{}}

\PYG{c+c1}{//==============================================================================}
\PYG{k}{class} \PYG{n+nc}{Error} \PYG{p}{\PYGZob{}}
 \PYG{k}{public}\PYG{o}{:}
  \PYG{k}{explicit} \PYG{n}{Error}\PYG{p}{(}\PYG{k}{const} \PYG{n}{RequestStatus}\PYG{o}{\PYGZam{}} \PYG{n}{status}\PYG{p}{)}\PYG{p}{;}

  \PYG{k}{explicit} \PYG{n+nf}{Error}\PYG{p}{(}\PYG{n}{RequestStatusCode} \PYG{n}{code} \PYG{o}{=} \PYG{n}{RequestStatusCode}\PYG{o}{:}\PYG{o}{:}\PYG{n}{SUCCESS}\PYG{p}{)}\PYG{p}{;}

  \PYG{k}{explicit} \PYG{n+nf}{Error}\PYG{p}{(}\PYG{n}{RequestStatusCode} \PYG{n}{code}\PYG{p}{,} \PYG{k}{const} \PYG{n}{std}\PYG{o}{:}\PYG{o}{:}\PYG{n}{string}\PYG{o}{\PYGZam{}} \PYG{n}{msg}\PYG{p}{)}\PYG{p}{;}

  \PYG{n}{RequestStatusCode} \PYG{n+nf}{Code}\PYG{p}{(}\PYG{p}{)} \PYG{k}{const} \PYG{p}{\PYGZob{}} \PYG{k}{return} \PYG{n}{code\PYGZus{}}\PYG{p}{;} \PYG{p}{\PYGZcb{}}

  \PYG{k}{const} \PYG{n}{std}\PYG{o}{:}\PYG{o}{:}\PYG{n}{string}\PYG{o}{\PYGZam{}} \PYG{n}{Message}\PYG{p}{(}\PYG{p}{)} \PYG{k}{const} \PYG{p}{\PYGZob{}} \PYG{k}{return} \PYG{n}{msg\PYGZus{}}\PYG{p}{;} \PYG{p}{\PYGZcb{}}

  \PYG{k}{const} \PYG{n}{std}\PYG{o}{:}\PYG{o}{:}\PYG{n}{string}\PYG{o}{\PYGZam{}} \PYG{n}{ServerId}\PYG{p}{(}\PYG{p}{)} \PYG{k}{const} \PYG{p}{\PYGZob{}} \PYG{k}{return} \PYG{n}{server\PYGZus{}id\PYGZus{}}\PYG{p}{;} \PYG{p}{\PYGZcb{}}

  \PYG{k+kt}{uint64\PYGZus{}t} \PYG{n}{RequestId}\PYG{p}{(}\PYG{p}{)} \PYG{k}{const} \PYG{p}{\PYGZob{}} \PYG{k}{return} \PYG{n}{request\PYGZus{}id\PYGZus{}}\PYG{p}{;} \PYG{p}{\PYGZcb{}}

  \PYG{k+kt}{bool} \PYG{n}{IsOk}\PYG{p}{(}\PYG{p}{)} \PYG{k}{const} \PYG{p}{\PYGZob{}} \PYG{k}{return} \PYG{n}{code\PYGZus{}} \PYG{o}{=}\PYG{o}{=} \PYG{n}{RequestStatusCode}\PYG{o}{:}\PYG{o}{:}\PYG{n}{SUCCESS}\PYG{p}{;} \PYG{p}{\PYGZcb{}}

  \PYG{k}{static} \PYG{k}{const} \PYG{n}{Error} \PYG{n}{Success}\PYG{p}{;}

 \PYG{k}{private}\PYG{o}{:}
  \PYG{k}{friend} \PYG{n}{std}\PYG{o}{:}\PYG{o}{:}\PYG{n}{ostream}\PYG{o}{\PYGZam{}} \PYG{k}{operator}\PYG{o}{\PYGZlt{}}\PYG{o}{\PYGZlt{}}\PYG{p}{(}\PYG{n}{std}\PYG{o}{:}\PYG{o}{:}\PYG{n}{ostream}\PYG{o}{\PYGZam{}}\PYG{p}{,} \PYG{k}{const} \PYG{n}{Error}\PYG{o}{\PYGZam{}}\PYG{p}{)}\PYG{p}{;}
  \PYG{n}{RequestStatusCode} \PYG{n}{code\PYGZus{}}\PYG{p}{;}
  \PYG{n}{std}\PYG{o}{:}\PYG{o}{:}\PYG{n}{string} \PYG{n}{msg\PYGZus{}}\PYG{p}{;}
  \PYG{n}{std}\PYG{o}{:}\PYG{o}{:}\PYG{n}{string} \PYG{n}{server\PYGZus{}id\PYGZus{}}\PYG{p}{;}
  \PYG{k+kt}{uint64\PYGZus{}t} \PYG{n}{request\PYGZus{}id\PYGZus{}}\PYG{p}{;}
\PYG{p}{\PYGZcb{}}\PYG{p}{;}

\PYG{c+c1}{//==============================================================================}
\PYG{k}{class} \PYG{n+nc}{ServerHealthContext} \PYG{p}{\PYGZob{}}
 \PYG{k}{public}\PYG{o}{:}
  \PYG{k}{virtual} \PYG{n}{Error} \PYG{n}{GetReady}\PYG{p}{(}\PYG{k+kt}{bool}\PYG{o}{*} \PYG{n}{ready}\PYG{p}{)} \PYG{o}{=} \PYG{l+m+mi}{0}\PYG{p}{;}

  \PYG{k}{virtual} \PYG{n}{Error} \PYG{n+nf}{GetLive}\PYG{p}{(}\PYG{k+kt}{bool}\PYG{o}{*} \PYG{n}{live}\PYG{p}{)} \PYG{o}{=} \PYG{l+m+mi}{0}\PYG{p}{;}

 \PYG{k}{protected}\PYG{o}{:}
  \PYG{n}{ServerHealthContext}\PYG{p}{(}\PYG{k+kt}{bool}\PYG{p}{)}\PYG{p}{;}

  \PYG{c+c1}{// If true print verbose output}
  \PYG{k}{const} \PYG{k+kt}{bool} \PYG{n}{verbose\PYGZus{}}\PYG{p}{;}
\PYG{p}{\PYGZcb{}}\PYG{p}{;}

\PYG{c+c1}{//==============================================================================}
\PYG{k}{class} \PYG{n+nc}{ServerStatusContext} \PYG{p}{\PYGZob{}}
 \PYG{k}{public}\PYG{o}{:}
  \PYG{k}{virtual} \PYG{n}{Error} \PYG{n}{GetServerStatus}\PYG{p}{(}\PYG{n}{ServerStatus}\PYG{o}{*} \PYG{n}{status}\PYG{p}{)} \PYG{o}{=} \PYG{l+m+mi}{0}\PYG{p}{;}

 \PYG{k}{protected}\PYG{o}{:}
  \PYG{n}{ServerStatusContext}\PYG{p}{(}\PYG{k+kt}{bool}\PYG{p}{)}\PYG{p}{;}

  \PYG{c+c1}{// If true print verbose output}
  \PYG{k}{const} \PYG{k+kt}{bool} \PYG{n}{verbose\PYGZus{}}\PYG{p}{;}
\PYG{p}{\PYGZcb{}}\PYG{p}{;}

\PYG{c+c1}{//==============================================================================}
\PYG{k}{class} \PYG{n+nc}{InferContext} \PYG{p}{\PYGZob{}}
 \PYG{k}{public}\PYG{o}{:}
  \PYG{c+c1}{//==============}
  \PYG{k}{class} \PYG{n+nc}{Input} \PYG{p}{\PYGZob{}}
   \PYG{k}{public}\PYG{o}{:}
    \PYG{k}{virtual} \PYG{o}{\PYGZti{}}\PYG{n}{Input}\PYG{p}{(}\PYG{p}{)}\PYG{p}{\PYGZob{}}\PYG{p}{\PYGZcb{}}\PYG{p}{;}

    \PYG{k}{virtual} \PYG{k}{const} \PYG{n}{std}\PYG{o}{:}\PYG{o}{:}\PYG{n}{string}\PYG{o}{\PYGZam{}} \PYG{n}{Name}\PYG{p}{(}\PYG{p}{)} \PYG{k}{const} \PYG{o}{=} \PYG{l+m+mi}{0}\PYG{p}{;}

    \PYG{k}{virtual} \PYG{k+kt}{int64\PYGZus{}t} \PYG{n+nf}{ByteSize}\PYG{p}{(}\PYG{p}{)} \PYG{k}{const} \PYG{o}{=} \PYG{l+m+mi}{0}\PYG{p}{;}

    \PYG{k}{virtual} \PYG{k+kt}{size\PYGZus{}t} \PYG{n+nf}{TotalByteSize}\PYG{p}{(}\PYG{p}{)} \PYG{k}{const} \PYG{o}{=} \PYG{l+m+mi}{0}\PYG{p}{;}

    \PYG{k}{virtual} \PYG{n}{DataType} \PYG{n+nf}{DType}\PYG{p}{(}\PYG{p}{)} \PYG{k}{const} \PYG{o}{=} \PYG{l+m+mi}{0}\PYG{p}{;}

    \PYG{k}{virtual} \PYG{n}{ModelInput}\PYG{o}{:}\PYG{o}{:}\PYG{n}{Format} \PYG{n}{Format}\PYG{p}{(}\PYG{p}{)} \PYG{k}{const} \PYG{o}{=} \PYG{l+m+mi}{0}\PYG{p}{;}

    \PYG{k}{virtual} \PYG{k}{const} \PYG{n}{DimsList}\PYG{o}{\PYGZam{}} \PYG{n}{Dims}\PYG{p}{(}\PYG{p}{)} \PYG{k}{const} \PYG{o}{=} \PYG{l+m+mi}{0}\PYG{p}{;}

    \PYG{k}{virtual} \PYG{n}{Error} \PYG{n+nf}{Reset}\PYG{p}{(}\PYG{p}{)} \PYG{o}{=} \PYG{l+m+mi}{0}\PYG{p}{;}

    \PYG{k}{virtual} \PYG{k}{const} \PYG{n}{std}\PYG{o}{:}\PYG{o}{:}\PYG{n}{vector}\PYG{o}{\PYGZlt{}}\PYG{k+kt}{int64\PYGZus{}t}\PYG{o}{\PYGZgt{}}\PYG{o}{\PYGZam{}} \PYG{n}{Shape}\PYG{p}{(}\PYG{p}{)} \PYG{k}{const} \PYG{o}{=} \PYG{l+m+mi}{0}\PYG{p}{;}

    \PYG{k}{virtual} \PYG{n}{Error} \PYG{n+nf}{SetShape}\PYG{p}{(}\PYG{k}{const} \PYG{n}{std}\PYG{o}{:}\PYG{o}{:}\PYG{n}{vector}\PYG{o}{\PYGZlt{}}\PYG{k+kt}{int64\PYGZus{}t}\PYG{o}{\PYGZgt{}}\PYG{o}{\PYGZam{}} \PYG{n}{dims}\PYG{p}{)} \PYG{o}{=} \PYG{l+m+mi}{0}\PYG{p}{;}

    \PYG{k}{virtual} \PYG{n}{Error} \PYG{n+nf}{SetRaw}\PYG{p}{(}\PYG{k}{const} \PYG{k+kt}{uint8\PYGZus{}t}\PYG{o}{*} \PYG{n}{input}\PYG{p}{,} \PYG{k+kt}{size\PYGZus{}t} \PYG{n}{input\PYGZus{}byte\PYGZus{}size}\PYG{p}{)} \PYG{o}{=} \PYG{l+m+mi}{0}\PYG{p}{;}

    \PYG{k}{virtual} \PYG{n}{Error} \PYG{n+nf}{SetRaw}\PYG{p}{(}\PYG{k}{const} \PYG{n}{std}\PYG{o}{:}\PYG{o}{:}\PYG{n}{vector}\PYG{o}{\PYGZlt{}}\PYG{k+kt}{uint8\PYGZus{}t}\PYG{o}{\PYGZgt{}}\PYG{o}{\PYGZam{}} \PYG{n}{input}\PYG{p}{)} \PYG{o}{=} \PYG{l+m+mi}{0}\PYG{p}{;}

    \PYG{k}{virtual} \PYG{n}{Error} \PYG{n+nf}{SetFromString}\PYG{p}{(}\PYG{k}{const} \PYG{n}{std}\PYG{o}{:}\PYG{o}{:}\PYG{n}{vector}\PYG{o}{\PYGZlt{}}\PYG{n}{std}\PYG{o}{:}\PYG{o}{:}\PYG{n}{string}\PYG{o}{\PYGZgt{}}\PYG{o}{\PYGZam{}} \PYG{n}{input}\PYG{p}{)} \PYG{o}{=} \PYG{l+m+mi}{0}\PYG{p}{;}
  \PYG{p}{\PYGZcb{}}\PYG{p}{;}

  \PYG{c+c1}{//==============}
  \PYG{k}{class} \PYG{n+nc}{Output} \PYG{p}{\PYGZob{}}
   \PYG{k}{public}\PYG{o}{:}
    \PYG{k}{virtual} \PYG{o}{\PYGZti{}}\PYG{n}{Output}\PYG{p}{(}\PYG{p}{)}\PYG{p}{\PYGZob{}}\PYG{p}{\PYGZcb{}}\PYG{p}{;}

    \PYG{k}{virtual} \PYG{k}{const} \PYG{n}{std}\PYG{o}{:}\PYG{o}{:}\PYG{n}{string}\PYG{o}{\PYGZam{}} \PYG{n}{Name}\PYG{p}{(}\PYG{p}{)} \PYG{k}{const} \PYG{o}{=} \PYG{l+m+mi}{0}\PYG{p}{;}

    \PYG{k}{virtual} \PYG{n}{DataType} \PYG{n+nf}{DType}\PYG{p}{(}\PYG{p}{)} \PYG{k}{const} \PYG{o}{=} \PYG{l+m+mi}{0}\PYG{p}{;}

    \PYG{k}{virtual} \PYG{k}{const} \PYG{n}{DimsList}\PYG{o}{\PYGZam{}} \PYG{n}{Dims}\PYG{p}{(}\PYG{p}{)} \PYG{k}{const} \PYG{o}{=} \PYG{l+m+mi}{0}\PYG{p}{;}
  \PYG{p}{\PYGZcb{}}\PYG{p}{;}

  \PYG{c+c1}{//==============}
  \PYG{k}{class} \PYG{n+nc}{Result} \PYG{p}{\PYGZob{}}
   \PYG{k}{public}\PYG{o}{:}
    \PYG{k}{virtual} \PYG{o}{\PYGZti{}}\PYG{n}{Result}\PYG{p}{(}\PYG{p}{)}\PYG{p}{\PYGZob{}}\PYG{p}{\PYGZcb{}}\PYG{p}{;}

    \PYG{k}{enum} \PYG{n}{ResultFormat} \PYG{p}{\PYGZob{}}
      \PYG{n}{RAW} \PYG{o}{=} \PYG{l+m+mi}{0}\PYG{p}{,}

      \PYG{n}{CLASS} \PYG{o}{=} \PYG{l+m+mi}{1}
    \PYG{p}{\PYGZcb{}}\PYG{p}{;}

    \PYG{k}{virtual} \PYG{k}{const} \PYG{n}{std}\PYG{o}{:}\PYG{o}{:}\PYG{n}{string}\PYG{o}{\PYGZam{}} \PYG{n}{ModelName}\PYG{p}{(}\PYG{p}{)} \PYG{k}{const} \PYG{o}{=} \PYG{l+m+mi}{0}\PYG{p}{;}

    \PYG{k}{virtual} \PYG{k+kt}{int64\PYGZus{}t} \PYG{n+nf}{ModelVersion}\PYG{p}{(}\PYG{p}{)} \PYG{k}{const} \PYG{o}{=} \PYG{l+m+mi}{0}\PYG{p}{;}

    \PYG{k}{virtual} \PYG{k}{const} \PYG{n}{std}\PYG{o}{:}\PYG{o}{:}\PYG{n}{shared\PYGZus{}ptr}\PYG{o}{\PYGZlt{}}\PYG{n}{Output}\PYG{o}{\PYGZgt{}} \PYG{n}{GetOutput}\PYG{p}{(}\PYG{p}{)} \PYG{k}{const} \PYG{o}{=} \PYG{l+m+mi}{0}\PYG{p}{;}

    \PYG{k}{virtual} \PYG{n}{Error} \PYG{n+nf}{GetRawShape}\PYG{p}{(}\PYG{n}{std}\PYG{o}{:}\PYG{o}{:}\PYG{n}{vector}\PYG{o}{\PYGZlt{}}\PYG{k+kt}{int64\PYGZus{}t}\PYG{o}{\PYGZgt{}}\PYG{o}{*} \PYG{n}{shape}\PYG{p}{)} \PYG{k}{const} \PYG{o}{=} \PYG{l+m+mi}{0}\PYG{p}{;}

    \PYG{k}{virtual} \PYG{n}{Error} \PYG{n+nf}{GetRaw}\PYG{p}{(}
        \PYG{k+kt}{size\PYGZus{}t} \PYG{n}{batch\PYGZus{}idx}\PYG{p}{,} \PYG{k}{const} \PYG{n}{std}\PYG{o}{:}\PYG{o}{:}\PYG{n}{vector}\PYG{o}{\PYGZlt{}}\PYG{k+kt}{uint8\PYGZus{}t}\PYG{o}{\PYGZgt{}}\PYG{o}{*}\PYG{o}{*} \PYG{n}{buf}\PYG{p}{)} \PYG{k}{const} \PYG{o}{=} \PYG{l+m+mi}{0}\PYG{p}{;}

    \PYG{k}{virtual} \PYG{n}{Error} \PYG{n+nf}{GetRawAtCursor}\PYG{p}{(}
        \PYG{k+kt}{size\PYGZus{}t} \PYG{n}{batch\PYGZus{}idx}\PYG{p}{,} \PYG{k}{const} \PYG{k+kt}{uint8\PYGZus{}t}\PYG{o}{*}\PYG{o}{*} \PYG{n}{buf}\PYG{p}{,} \PYG{k+kt}{size\PYGZus{}t} \PYG{n}{adv\PYGZus{}byte\PYGZus{}size}\PYG{p}{)} \PYG{o}{=} \PYG{l+m+mi}{0}\PYG{p}{;}

    \PYG{k}{template} \PYG{o}{\PYGZlt{}}\PYG{k}{typename} \PYG{n}{T}\PYG{o}{\PYGZgt{}}
    \PYG{n}{Error} \PYG{n}{GetRawAtCursor}\PYG{p}{(}\PYG{k+kt}{size\PYGZus{}t} \PYG{n}{batch\PYGZus{}idx}\PYG{p}{,} \PYG{n}{T}\PYG{o}{*} \PYG{n}{out}\PYG{p}{)}\PYG{p}{;}

    \PYG{k}{struct} \PYG{n}{ClassResult} \PYG{p}{\PYGZob{}}
      \PYG{k+kt}{size\PYGZus{}t} \PYG{n}{idx}\PYG{p}{;}
      \PYG{k+kt}{float} \PYG{n}{value}\PYG{p}{;}
      \PYG{n}{std}\PYG{o}{:}\PYG{o}{:}\PYG{n}{string} \PYG{n}{label}\PYG{p}{;}
    \PYG{p}{\PYGZcb{}}\PYG{p}{;}

    \PYG{k}{virtual} \PYG{n}{Error} \PYG{n+nf}{GetClassCount}\PYG{p}{(}\PYG{k+kt}{size\PYGZus{}t} \PYG{n}{batch\PYGZus{}idx}\PYG{p}{,} \PYG{k+kt}{size\PYGZus{}t}\PYG{o}{*} \PYG{n}{cnt}\PYG{p}{)} \PYG{k}{const} \PYG{o}{=} \PYG{l+m+mi}{0}\PYG{p}{;}

    \PYG{k}{virtual} \PYG{n}{Error} \PYG{n+nf}{GetClassAtCursor}\PYG{p}{(}\PYG{k+kt}{size\PYGZus{}t} \PYG{n}{batch\PYGZus{}idx}\PYG{p}{,} \PYG{n}{ClassResult}\PYG{o}{*} \PYG{n}{result}\PYG{p}{)} \PYG{o}{=} \PYG{l+m+mi}{0}\PYG{p}{;}

    \PYG{k}{virtual} \PYG{n}{Error} \PYG{n+nf}{ResetCursors}\PYG{p}{(}\PYG{p}{)} \PYG{o}{=} \PYG{l+m+mi}{0}\PYG{p}{;}

    \PYG{k}{virtual} \PYG{n}{Error} \PYG{n+nf}{ResetCursor}\PYG{p}{(}\PYG{k+kt}{size\PYGZus{}t} \PYG{n}{batch\PYGZus{}idx}\PYG{p}{)} \PYG{o}{=} \PYG{l+m+mi}{0}\PYG{p}{;}
  \PYG{p}{\PYGZcb{}}\PYG{p}{;}

  \PYG{c+c1}{//==============}
  \PYG{k}{class} \PYG{n+nc}{Options} \PYG{p}{\PYGZob{}}
   \PYG{k}{public}\PYG{o}{:}
    \PYG{k}{virtual} \PYG{o}{\PYGZti{}}\PYG{n}{Options}\PYG{p}{(}\PYG{p}{)}\PYG{p}{\PYGZob{}}\PYG{p}{\PYGZcb{}}\PYG{p}{;}

    \PYG{k}{static} \PYG{n}{Error} \PYG{n+nf}{Create}\PYG{p}{(}\PYG{n}{std}\PYG{o}{:}\PYG{o}{:}\PYG{n}{unique\PYGZus{}ptr}\PYG{o}{\PYGZlt{}}\PYG{n}{Options}\PYG{o}{\PYGZgt{}}\PYG{o}{*} \PYG{n}{options}\PYG{p}{)}\PYG{p}{;}

    \PYG{k}{virtual} \PYG{k+kt}{bool} \PYG{n+nf}{Flag}\PYG{p}{(}\PYG{n}{InferRequestHeader}\PYG{o}{:}\PYG{o}{:}\PYG{n}{Flag} \PYG{n}{flag}\PYG{p}{)} \PYG{k}{const} \PYG{o}{=} \PYG{l+m+mi}{0}\PYG{p}{;}

    \PYG{k}{virtual} \PYG{k+kt}{void} \PYG{n+nf}{SetFlag}\PYG{p}{(}\PYG{n}{InferRequestHeader}\PYG{o}{:}\PYG{o}{:}\PYG{n}{Flag} \PYG{n}{flag}\PYG{p}{,} \PYG{k+kt}{bool} \PYG{n}{value}\PYG{p}{)} \PYG{o}{=} \PYG{l+m+mi}{0}\PYG{p}{;}

    \PYG{k}{virtual} \PYG{k+kt}{uint32\PYGZus{}t} \PYG{n+nf}{Flags}\PYG{p}{(}\PYG{p}{)} \PYG{k}{const} \PYG{o}{=} \PYG{l+m+mi}{0}\PYG{p}{;}

    \PYG{k}{virtual} \PYG{k+kt}{void} \PYG{n+nf}{SetFlags}\PYG{p}{(}\PYG{k+kt}{uint32\PYGZus{}t} \PYG{n}{flags}\PYG{p}{)} \PYG{o}{=} \PYG{l+m+mi}{0}\PYG{p}{;}

    \PYG{k}{virtual} \PYG{k+kt}{size\PYGZus{}t} \PYG{n+nf}{BatchSize}\PYG{p}{(}\PYG{p}{)} \PYG{k}{const} \PYG{o}{=} \PYG{l+m+mi}{0}\PYG{p}{;}

    \PYG{k}{virtual} \PYG{k+kt}{void} \PYG{n+nf}{SetBatchSize}\PYG{p}{(}\PYG{k+kt}{size\PYGZus{}t} \PYG{n}{batch\PYGZus{}size}\PYG{p}{)} \PYG{o}{=} \PYG{l+m+mi}{0}\PYG{p}{;}

    \PYG{k}{virtual} \PYG{n}{Error} \PYG{n+nf}{AddRawResult}\PYG{p}{(}
        \PYG{k}{const} \PYG{n}{std}\PYG{o}{:}\PYG{o}{:}\PYG{n}{shared\PYGZus{}ptr}\PYG{o}{\PYGZlt{}}\PYG{n}{InferContext}\PYG{o}{:}\PYG{o}{:}\PYG{n}{Output}\PYG{o}{\PYGZgt{}}\PYG{o}{\PYGZam{}} \PYG{n}{output}\PYG{p}{)} \PYG{o}{=} \PYG{l+m+mi}{0}\PYG{p}{;}

    \PYG{k}{virtual} \PYG{n}{Error} \PYG{n+nf}{AddClassResult}\PYG{p}{(}
        \PYG{k}{const} \PYG{n}{std}\PYG{o}{:}\PYG{o}{:}\PYG{n}{shared\PYGZus{}ptr}\PYG{o}{\PYGZlt{}}\PYG{n}{InferContext}\PYG{o}{:}\PYG{o}{:}\PYG{n}{Output}\PYG{o}{\PYGZgt{}}\PYG{o}{\PYGZam{}} \PYG{n}{output}\PYG{p}{,} \PYG{k+kt}{uint64\PYGZus{}t} \PYG{n}{k}\PYG{p}{)} \PYG{o}{=} \PYG{l+m+mi}{0}\PYG{p}{;}
  \PYG{p}{\PYGZcb{}}\PYG{p}{;}

  \PYG{c+c1}{//==============}
  \PYG{k}{class} \PYG{n+nc}{Request} \PYG{p}{\PYGZob{}}
   \PYG{k}{public}\PYG{o}{:}
    \PYG{k}{virtual} \PYG{o}{\PYGZti{}}\PYG{n}{Request}\PYG{p}{(}\PYG{p}{)} \PYG{o}{=} \PYG{k}{default}\PYG{p}{;}

    \PYG{k}{virtual} \PYG{k+kt}{uint64\PYGZus{}t} \PYG{n+nf}{Id}\PYG{p}{(}\PYG{p}{)} \PYG{k}{const} \PYG{o}{=} \PYG{l+m+mi}{0}\PYG{p}{;}
  \PYG{p}{\PYGZcb{}}\PYG{p}{;}

  \PYG{c+c1}{//==============}
  \PYG{k}{struct} \PYG{n}{Stat} \PYG{p}{\PYGZob{}}
    \PYG{k+kt}{size\PYGZus{}t} \PYG{n}{completed\PYGZus{}request\PYGZus{}count}\PYG{p}{;}

    \PYG{k+kt}{uint64\PYGZus{}t} \PYG{n}{cumulative\PYGZus{}total\PYGZus{}request\PYGZus{}time\PYGZus{}ns}\PYG{p}{;}

    \PYG{k+kt}{uint64\PYGZus{}t} \PYG{n}{cumulative\PYGZus{}send\PYGZus{}time\PYGZus{}ns}\PYG{p}{;}

    \PYG{k+kt}{uint64\PYGZus{}t} \PYG{n}{cumulative\PYGZus{}receive\PYGZus{}time\PYGZus{}ns}\PYG{p}{;}

    \PYG{n}{Stat}\PYG{p}{(}\PYG{p}{)}
        \PYG{o}{:} \PYG{n}{completed\PYGZus{}request\PYGZus{}count}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{)}\PYG{p}{,} \PYG{n}{cumulative\PYGZus{}total\PYGZus{}request\PYGZus{}time\PYGZus{}ns}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{)}\PYG{p}{,}
          \PYG{n}{cumulative\PYGZus{}send\PYGZus{}time\PYGZus{}ns}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{)}\PYG{p}{,} \PYG{n}{cumulative\PYGZus{}receive\PYGZus{}time\PYGZus{}ns}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{)}
    \PYG{p}{\PYGZob{}}
    \PYG{p}{\PYGZcb{}}
  \PYG{p}{\PYGZcb{}}\PYG{p}{;}

  \PYG{c+c1}{//==============}
  \PYG{k}{class} \PYG{n+nc}{RequestTimers} \PYG{p}{\PYGZob{}}
   \PYG{k}{public}\PYG{o}{:}
    \PYG{k}{enum} \PYG{n}{Kind} \PYG{p}{\PYGZob{}}
      \PYG{n}{REQUEST\PYGZus{}START}\PYG{p}{,}
      \PYG{n}{REQUEST\PYGZus{}END}\PYG{p}{,}
      \PYG{n}{SEND\PYGZus{}START}\PYG{p}{,}
      \PYG{n}{SEND\PYGZus{}END}\PYG{p}{,}
      \PYG{n}{RECEIVE\PYGZus{}START}\PYG{p}{,}
      \PYG{n}{RECEIVE\PYGZus{}END}
    \PYG{p}{\PYGZcb{}}\PYG{p}{;}

    \PYG{n}{RequestTimers}\PYG{p}{(}\PYG{p}{)}\PYG{p}{;}

    \PYG{n}{Error} \PYG{n+nf}{Reset}\PYG{p}{(}\PYG{p}{)}\PYG{p}{;}

    \PYG{n}{Error} \PYG{n+nf}{Record}\PYG{p}{(}\PYG{n}{Kind} \PYG{n}{kind}\PYG{p}{)}\PYG{p}{;}

   \PYG{k}{private}\PYG{o}{:}
    \PYG{k}{friend} \PYG{k}{class} \PYG{n+nc}{InferContext}\PYG{p}{;}
    \PYG{k}{friend} \PYG{k}{class} \PYG{n+nc}{InferHttpContext}\PYG{p}{;}
    \PYG{k}{friend} \PYG{k}{class} \PYG{n+nc}{InferGrpcContext}\PYG{p}{;}
    \PYG{k}{friend} \PYG{k}{class} \PYG{n+nc}{InferGrpcStreamContext}\PYG{p}{;}
    \PYG{k}{struct} \PYG{n}{timespec} \PYG{n}{request\PYGZus{}start\PYGZus{}}\PYG{p}{;}
    \PYG{k}{struct} \PYG{n}{timespec} \PYG{n}{request\PYGZus{}end\PYGZus{}}\PYG{p}{;}
    \PYG{k}{struct} \PYG{n}{timespec} \PYG{n}{send\PYGZus{}start\PYGZus{}}\PYG{p}{;}
    \PYG{k}{struct} \PYG{n}{timespec} \PYG{n}{send\PYGZus{}end\PYGZus{}}\PYG{p}{;}
    \PYG{k}{struct} \PYG{n}{timespec} \PYG{n}{receive\PYGZus{}start\PYGZus{}}\PYG{p}{;}
    \PYG{k}{struct} \PYG{n}{timespec} \PYG{n}{receive\PYGZus{}end\PYGZus{}}\PYG{p}{;}
  \PYG{p}{\PYGZcb{}}\PYG{p}{;}

 \PYG{k}{public}\PYG{o}{:}
  \PYG{k}{using} \PYG{n}{ResultMap} \PYG{o}{=} \PYG{n}{std}\PYG{o}{:}\PYG{o}{:}\PYG{n}{map}\PYG{o}{\PYGZlt{}}\PYG{n}{std}\PYG{o}{:}\PYG{o}{:}\PYG{n}{string}\PYG{p}{,} \PYG{n}{std}\PYG{o}{:}\PYG{o}{:}\PYG{n}{unique\PYGZus{}ptr}\PYG{o}{\PYGZlt{}}\PYG{n}{Result}\PYG{o}{\PYGZgt{}}\PYG{o}{\PYGZgt{}}\PYG{p}{;}

  \PYG{k}{virtual} \PYG{o}{\PYGZti{}}\PYG{n}{InferContext}\PYG{p}{(}\PYG{p}{)} \PYG{o}{=} \PYG{k}{default}\PYG{p}{;}

  \PYG{k}{const} \PYG{n}{std}\PYG{o}{:}\PYG{o}{:}\PYG{n}{string}\PYG{o}{\PYGZam{}} \PYG{n}{ModelName}\PYG{p}{(}\PYG{p}{)} \PYG{k}{const} \PYG{p}{\PYGZob{}} \PYG{k}{return} \PYG{n}{model\PYGZus{}name\PYGZus{}}\PYG{p}{;} \PYG{p}{\PYGZcb{}}

  \PYG{k+kt}{int64\PYGZus{}t} \PYG{n}{ModelVersion}\PYG{p}{(}\PYG{p}{)} \PYG{k}{const} \PYG{p}{\PYGZob{}} \PYG{k}{return} \PYG{n}{model\PYGZus{}version\PYGZus{}}\PYG{p}{;} \PYG{p}{\PYGZcb{}}

  \PYG{k+kt}{uint64\PYGZus{}t} \PYG{n}{MaxBatchSize}\PYG{p}{(}\PYG{p}{)} \PYG{k}{const} \PYG{p}{\PYGZob{}} \PYG{k}{return} \PYG{n}{max\PYGZus{}batch\PYGZus{}size\PYGZus{}}\PYG{p}{;} \PYG{p}{\PYGZcb{}}

  \PYG{k}{const} \PYG{n}{std}\PYG{o}{:}\PYG{o}{:}\PYG{n}{vector}\PYG{o}{\PYGZlt{}}\PYG{n}{std}\PYG{o}{:}\PYG{o}{:}\PYG{n}{shared\PYGZus{}ptr}\PYG{o}{\PYGZlt{}}\PYG{n}{Input}\PYG{o}{\PYGZgt{}}\PYG{o}{\PYGZgt{}}\PYG{o}{\PYGZam{}} \PYG{n}{Inputs}\PYG{p}{(}\PYG{p}{)} \PYG{k}{const} \PYG{p}{\PYGZob{}} \PYG{k}{return} \PYG{n}{inputs\PYGZus{}}\PYG{p}{;} \PYG{p}{\PYGZcb{}}

  \PYG{k}{const} \PYG{n}{std}\PYG{o}{:}\PYG{o}{:}\PYG{n}{vector}\PYG{o}{\PYGZlt{}}\PYG{n}{std}\PYG{o}{:}\PYG{o}{:}\PYG{n}{shared\PYGZus{}ptr}\PYG{o}{\PYGZlt{}}\PYG{n}{Output}\PYG{o}{\PYGZgt{}}\PYG{o}{\PYGZgt{}}\PYG{o}{\PYGZam{}} \PYG{n}{Outputs}\PYG{p}{(}\PYG{p}{)} \PYG{k}{const}
  \PYG{p}{\PYGZob{}}
    \PYG{k}{return} \PYG{n}{outputs\PYGZus{}}\PYG{p}{;}
  \PYG{p}{\PYGZcb{}}

  \PYG{n}{Error} \PYG{n}{GetInput}\PYG{p}{(}\PYG{k}{const} \PYG{n}{std}\PYG{o}{:}\PYG{o}{:}\PYG{n}{string}\PYG{o}{\PYGZam{}} \PYG{n}{name}\PYG{p}{,} \PYG{n}{std}\PYG{o}{:}\PYG{o}{:}\PYG{n}{shared\PYGZus{}ptr}\PYG{o}{\PYGZlt{}}\PYG{n}{Input}\PYG{o}{\PYGZgt{}}\PYG{o}{*} \PYG{n}{input}\PYG{p}{)} \PYG{k}{const}\PYG{p}{;}

  \PYG{n}{Error} \PYG{n+nf}{GetOutput}\PYG{p}{(}
      \PYG{k}{const} \PYG{n}{std}\PYG{o}{:}\PYG{o}{:}\PYG{n}{string}\PYG{o}{\PYGZam{}} \PYG{n}{name}\PYG{p}{,} \PYG{n}{std}\PYG{o}{:}\PYG{o}{:}\PYG{n}{shared\PYGZus{}ptr}\PYG{o}{\PYGZlt{}}\PYG{n}{Output}\PYG{o}{\PYGZgt{}}\PYG{o}{*} \PYG{n}{output}\PYG{p}{)} \PYG{k}{const}\PYG{p}{;}

  \PYG{n}{Error} \PYG{n+nf}{SetRunOptions}\PYG{p}{(}\PYG{k}{const} \PYG{n}{Options}\PYG{o}{\PYGZam{}} \PYG{n}{options}\PYG{p}{)}\PYG{p}{;}

  \PYG{n}{Error} \PYG{n+nf}{GetStat}\PYG{p}{(}\PYG{n}{Stat}\PYG{o}{*} \PYG{n}{stat}\PYG{p}{)}\PYG{p}{;}

  \PYG{k}{virtual} \PYG{n}{Error} \PYG{n+nf}{Run}\PYG{p}{(}\PYG{n}{ResultMap}\PYG{o}{*} \PYG{n}{results}\PYG{p}{)} \PYG{o}{=} \PYG{l+m+mi}{0}\PYG{p}{;}

  \PYG{k}{virtual} \PYG{n}{Error} \PYG{n+nf}{AsyncRun}\PYG{p}{(}\PYG{n}{std}\PYG{o}{:}\PYG{o}{:}\PYG{n}{shared\PYGZus{}ptr}\PYG{o}{\PYGZlt{}}\PYG{n}{Request}\PYG{o}{\PYGZgt{}}\PYG{o}{*} \PYG{n}{async\PYGZus{}request}\PYG{p}{)} \PYG{o}{=} \PYG{l+m+mi}{0}\PYG{p}{;}

  \PYG{k}{virtual} \PYG{n}{Error} \PYG{n+nf}{GetAsyncRunResults}\PYG{p}{(}
      \PYG{n}{ResultMap}\PYG{o}{*} \PYG{n}{results}\PYG{p}{,} \PYG{k}{const} \PYG{n}{std}\PYG{o}{:}\PYG{o}{:}\PYG{n}{shared\PYGZus{}ptr}\PYG{o}{\PYGZlt{}}\PYG{n}{Request}\PYG{o}{\PYGZgt{}}\PYG{o}{\PYGZam{}} \PYG{n}{async\PYGZus{}request}\PYG{p}{,}
      \PYG{k+kt}{bool} \PYG{n}{wait}\PYG{p}{)} \PYG{o}{=} \PYG{l+m+mi}{0}\PYG{p}{;}

  \PYG{n}{Error} \PYG{n+nf}{GetReadyAsyncRequest}\PYG{p}{(}
      \PYG{n}{std}\PYG{o}{:}\PYG{o}{:}\PYG{n}{shared\PYGZus{}ptr}\PYG{o}{\PYGZlt{}}\PYG{n}{Request}\PYG{o}{\PYGZgt{}}\PYG{o}{*} \PYG{n}{async\PYGZus{}request}\PYG{p}{,} \PYG{k+kt}{bool} \PYG{n}{wait}\PYG{p}{)}\PYG{p}{;}

 \PYG{k}{protected}\PYG{o}{:}
  \PYG{n}{InferContext}\PYG{p}{(}\PYG{k}{const} \PYG{n}{std}\PYG{o}{:}\PYG{o}{:}\PYG{n}{string}\PYG{o}{\PYGZam{}}\PYG{p}{,} \PYG{k+kt}{int64\PYGZus{}t}\PYG{p}{,} \PYG{n}{CorrelationID}\PYG{p}{,} \PYG{k+kt}{bool}\PYG{p}{)}\PYG{p}{;}

  \PYG{c+c1}{// Function for worker thread to proceed the data transfer for all requests}
  \PYG{k}{virtual} \PYG{k+kt}{void} \PYG{n+nf}{AsyncTransfer}\PYG{p}{(}\PYG{p}{)} \PYG{o}{=} \PYG{l+m+mi}{0}\PYG{p}{;}

  \PYG{c+c1}{// Helper function called before inference to prepare \PYGZsq{}request\PYGZsq{}}
  \PYG{k}{virtual} \PYG{n}{Error} \PYG{n+nf}{PreRunProcessing}\PYG{p}{(}\PYG{n}{std}\PYG{o}{:}\PYG{o}{:}\PYG{n}{shared\PYGZus{}ptr}\PYG{o}{\PYGZlt{}}\PYG{n}{Request}\PYG{o}{\PYGZgt{}}\PYG{o}{\PYGZam{}} \PYG{n}{request}\PYG{p}{)} \PYG{o}{=} \PYG{l+m+mi}{0}\PYG{p}{;}

  \PYG{c+c1}{// Helper function called by GetAsyncRunResults() to check if the request}
  \PYG{c+c1}{// is ready. If the request is valid and wait == true,}
  \PYG{c+c1}{// the function will block until request is ready.}
  \PYG{n}{Error} \PYG{n+nf}{IsRequestReady}\PYG{p}{(}
      \PYG{k}{const} \PYG{n}{std}\PYG{o}{:}\PYG{o}{:}\PYG{n}{shared\PYGZus{}ptr}\PYG{o}{\PYGZlt{}}\PYG{n}{Request}\PYG{o}{\PYGZgt{}}\PYG{o}{\PYGZam{}} \PYG{n}{async\PYGZus{}request}\PYG{p}{,} \PYG{k+kt}{bool} \PYG{n}{wait}\PYG{p}{)}\PYG{p}{;}

  \PYG{c+c1}{// Update the context stat with the given timer}
  \PYG{n}{Error} \PYG{n+nf}{UpdateStat}\PYG{p}{(}\PYG{k}{const} \PYG{n}{RequestTimers}\PYG{o}{\PYGZam{}} \PYG{n}{timer}\PYG{p}{)}\PYG{p}{;}

  \PYG{k}{using} \PYG{n}{AsyncReqMap} \PYG{o}{=} \PYG{n}{std}\PYG{o}{:}\PYG{o}{:}\PYG{n}{map}\PYG{o}{\PYGZlt{}}\PYG{k+kt}{uintptr\PYGZus{}t}\PYG{p}{,} \PYG{n}{std}\PYG{o}{:}\PYG{o}{:}\PYG{n}{shared\PYGZus{}ptr}\PYG{o}{\PYGZlt{}}\PYG{n}{Request}\PYG{o}{\PYGZgt{}}\PYG{o}{\PYGZgt{}}\PYG{p}{;}

  \PYG{c+c1}{// map to record ongoing asynchronous requests with pointer to easy handle}
  \PYG{c+c1}{// as key}
  \PYG{n}{AsyncReqMap} \PYG{n}{ongoing\PYGZus{}async\PYGZus{}requests\PYGZus{}}\PYG{p}{;}

  \PYG{c+c1}{// Model name}
  \PYG{k}{const} \PYG{n}{std}\PYG{o}{:}\PYG{o}{:}\PYG{n}{string} \PYG{n}{model\PYGZus{}name\PYGZus{}}\PYG{p}{;}

  \PYG{c+c1}{// Model version}
  \PYG{k}{const} \PYG{k+kt}{int64\PYGZus{}t} \PYG{n}{model\PYGZus{}version\PYGZus{}}\PYG{p}{;}

  \PYG{c+c1}{// The correlation ID to use with all inference requests using this}
  \PYG{c+c1}{// context. A value of 0 (zero) indicates no correlation ID.}
  \PYG{k}{const} \PYG{n}{CorrelationID} \PYG{n}{correlation\PYGZus{}id\PYGZus{}}\PYG{p}{;}

  \PYG{c+c1}{// If true print verbose output}
  \PYG{k}{const} \PYG{k+kt}{bool} \PYG{n}{verbose\PYGZus{}}\PYG{p}{;}

  \PYG{c+c1}{// Maximum batch size supported by this context. A maximum batch}
  \PYG{c+c1}{// size indicates that the context does not support batching and so}
  \PYG{c+c1}{// only a single inference at a time can be performed.}
  \PYG{k+kt}{uint64\PYGZus{}t} \PYG{n}{max\PYGZus{}batch\PYGZus{}size\PYGZus{}}\PYG{p}{;}

  \PYG{c+c1}{// Requested batch size for inference request}
  \PYG{k+kt}{uint64\PYGZus{}t} \PYG{n}{batch\PYGZus{}size\PYGZus{}}\PYG{p}{;}

  \PYG{c+c1}{// Use to assign unique identifier for each asynchronous request}
  \PYG{k+kt}{uint64\PYGZus{}t} \PYG{n}{async\PYGZus{}request\PYGZus{}id\PYGZus{}}\PYG{p}{;}

  \PYG{c+c1}{// The inputs and outputs}
  \PYG{n}{std}\PYG{o}{:}\PYG{o}{:}\PYG{n}{vector}\PYG{o}{\PYGZlt{}}\PYG{n}{std}\PYG{o}{:}\PYG{o}{:}\PYG{n}{shared\PYGZus{}ptr}\PYG{o}{\PYGZlt{}}\PYG{n}{Input}\PYG{o}{\PYGZgt{}}\PYG{o}{\PYGZgt{}} \PYG{n}{inputs\PYGZus{}}\PYG{p}{;}
  \PYG{n}{std}\PYG{o}{:}\PYG{o}{:}\PYG{n}{vector}\PYG{o}{\PYGZlt{}}\PYG{n}{std}\PYG{o}{:}\PYG{o}{:}\PYG{n}{shared\PYGZus{}ptr}\PYG{o}{\PYGZlt{}}\PYG{n}{Output}\PYG{o}{\PYGZgt{}}\PYG{o}{\PYGZgt{}} \PYG{n}{outputs\PYGZus{}}\PYG{p}{;}

  \PYG{c+c1}{// Settings generated by current option}
  \PYG{c+c1}{// InferRequestHeader protobuf describing the request}
  \PYG{n}{InferRequestHeader} \PYG{n}{infer\PYGZus{}request\PYGZus{}}\PYG{p}{;}

  \PYG{c+c1}{// Standalone request context used for synchronous request}
  \PYG{n}{std}\PYG{o}{:}\PYG{o}{:}\PYG{n}{shared\PYGZus{}ptr}\PYG{o}{\PYGZlt{}}\PYG{n}{Request}\PYG{o}{\PYGZgt{}} \PYG{n}{sync\PYGZus{}request\PYGZus{}}\PYG{p}{;}

  \PYG{c+c1}{// The statistic of the current context}
  \PYG{n}{Stat} \PYG{n}{context\PYGZus{}stat\PYGZus{}}\PYG{p}{;}

  \PYG{c+c1}{// worker thread that will perform the asynchronous transfer}
  \PYG{n}{std}\PYG{o}{:}\PYG{o}{:}\PYG{k+kr}{thread} \PYG{n}{worker\PYGZus{}}\PYG{p}{;}

  \PYG{c+c1}{// Avoid race condition between main thread and worker thread}
  \PYG{n}{std}\PYG{o}{:}\PYG{o}{:}\PYG{n}{mutex} \PYG{n}{mutex\PYGZus{}}\PYG{p}{;}

  \PYG{c+c1}{// Condition variable used for waiting on asynchronous request}
  \PYG{n}{std}\PYG{o}{:}\PYG{o}{:}\PYG{n}{condition\PYGZus{}variable} \PYG{n}{cv\PYGZus{}}\PYG{p}{;}

  \PYG{c+c1}{// signal for worker thread to stop}
  \PYG{k+kt}{bool} \PYG{n}{exiting\PYGZus{}}\PYG{p}{;}
\PYG{p}{\PYGZcb{}}\PYG{p}{;}

\PYG{c+c1}{//==============================================================================}
\PYG{k}{class} \PYG{n+nc}{ProfileContext} \PYG{p}{\PYGZob{}}
 \PYG{k}{public}\PYG{o}{:}
  \PYG{n}{Error} \PYG{n}{StartProfile}\PYG{p}{(}\PYG{p}{)}\PYG{p}{;}

  \PYG{c+c1}{// \PYGZbs{}return Error object indicating success or failure.}
  \PYG{n}{Error} \PYG{n+nf}{StopProfile}\PYG{p}{(}\PYG{p}{)}\PYG{p}{;}

 \PYG{k}{protected}\PYG{o}{:}
  \PYG{n}{ProfileContext}\PYG{p}{(}\PYG{k+kt}{bool}\PYG{p}{)}\PYG{p}{;}
  \PYG{k}{virtual} \PYG{n}{Error} \PYG{n+nf}{SendCommand}\PYG{p}{(}\PYG{k}{const} \PYG{n}{std}\PYG{o}{:}\PYG{o}{:}\PYG{n}{string}\PYG{o}{\PYGZam{}} \PYG{n}{cmd\PYGZus{}str}\PYG{p}{)} \PYG{o}{=} \PYG{l+m+mi}{0}\PYG{p}{;}

  \PYG{c+c1}{// If true print verbose output}
  \PYG{k}{const} \PYG{k+kt}{bool} \PYG{n}{verbose\PYGZus{}}\PYG{p}{;}
\PYG{p}{\PYGZcb{}}\PYG{p}{;}

\PYG{c+c1}{//==============================================================================}
\PYG{k}{class} \PYG{n+nc}{ServerHealthHttpContext} \PYG{o}{:} \PYG{k}{public} \PYG{n}{ServerHealthContext} \PYG{p}{\PYGZob{}}
 \PYG{k}{public}\PYG{o}{:}
  \PYG{k}{static} \PYG{n}{Error} \PYG{n}{Create}\PYG{p}{(}
      \PYG{n}{std}\PYG{o}{:}\PYG{o}{:}\PYG{n}{unique\PYGZus{}ptr}\PYG{o}{\PYGZlt{}}\PYG{n}{ServerHealthContext}\PYG{o}{\PYGZgt{}}\PYG{o}{*} \PYG{n}{ctx}\PYG{p}{,} \PYG{k}{const} \PYG{n}{std}\PYG{o}{:}\PYG{o}{:}\PYG{n}{string}\PYG{o}{\PYGZam{}} \PYG{n}{server\PYGZus{}url}\PYG{p}{,}
      \PYG{k+kt}{bool} \PYG{n}{verbose} \PYG{o}{=} \PYG{n+nb}{false}\PYG{p}{)}\PYG{p}{;}

  \PYG{n}{Error} \PYG{n+nf}{GetReady}\PYG{p}{(}\PYG{k+kt}{bool}\PYG{o}{*} \PYG{n}{ready}\PYG{p}{)} \PYG{k}{override}\PYG{p}{;}
  \PYG{n}{Error} \PYG{n+nf}{GetLive}\PYG{p}{(}\PYG{k+kt}{bool}\PYG{o}{*} \PYG{n}{live}\PYG{p}{)} \PYG{k}{override}\PYG{p}{;}

 \PYG{k}{private}\PYG{o}{:}
  \PYG{n}{ServerHealthHttpContext}\PYG{p}{(}\PYG{k}{const} \PYG{n}{std}\PYG{o}{:}\PYG{o}{:}\PYG{n}{string}\PYG{o}{\PYGZam{}}\PYG{p}{,} \PYG{k+kt}{bool}\PYG{p}{)}\PYG{p}{;}
  \PYG{n}{Error} \PYG{n+nf}{GetHealth}\PYG{p}{(}\PYG{k}{const} \PYG{n}{std}\PYG{o}{:}\PYG{o}{:}\PYG{n}{string}\PYG{o}{\PYGZam{}} \PYG{n}{url}\PYG{p}{,} \PYG{k+kt}{bool}\PYG{o}{*} \PYG{n}{health}\PYG{p}{)}\PYG{p}{;}

  \PYG{c+c1}{// URL for health endpoint on inference server.}
  \PYG{k}{const} \PYG{n}{std}\PYG{o}{:}\PYG{o}{:}\PYG{n}{string} \PYG{n}{url\PYGZus{}}\PYG{p}{;}
\PYG{p}{\PYGZcb{}}\PYG{p}{;}

\PYG{c+c1}{//==============================================================================}
\PYG{k}{class} \PYG{n+nc}{ServerStatusHttpContext} \PYG{o}{:} \PYG{k}{public} \PYG{n}{ServerStatusContext} \PYG{p}{\PYGZob{}}
 \PYG{k}{public}\PYG{o}{:}
  \PYG{k}{static} \PYG{n}{Error} \PYG{n}{Create}\PYG{p}{(}
      \PYG{n}{std}\PYG{o}{:}\PYG{o}{:}\PYG{n}{unique\PYGZus{}ptr}\PYG{o}{\PYGZlt{}}\PYG{n}{ServerStatusContext}\PYG{o}{\PYGZgt{}}\PYG{o}{*} \PYG{n}{ctx}\PYG{p}{,} \PYG{k}{const} \PYG{n}{std}\PYG{o}{:}\PYG{o}{:}\PYG{n}{string}\PYG{o}{\PYGZam{}} \PYG{n}{server\PYGZus{}url}\PYG{p}{,}
      \PYG{k+kt}{bool} \PYG{n}{verbose} \PYG{o}{=} \PYG{n+nb}{false}\PYG{p}{)}\PYG{p}{;}

  \PYG{k}{static} \PYG{n}{Error} \PYG{n+nf}{Create}\PYG{p}{(}
      \PYG{n}{std}\PYG{o}{:}\PYG{o}{:}\PYG{n}{unique\PYGZus{}ptr}\PYG{o}{\PYGZlt{}}\PYG{n}{ServerStatusContext}\PYG{o}{\PYGZgt{}}\PYG{o}{*} \PYG{n}{ctx}\PYG{p}{,} \PYG{k}{const} \PYG{n}{std}\PYG{o}{:}\PYG{o}{:}\PYG{n}{string}\PYG{o}{\PYGZam{}} \PYG{n}{server\PYGZus{}url}\PYG{p}{,}
      \PYG{k}{const} \PYG{n}{std}\PYG{o}{:}\PYG{o}{:}\PYG{n}{string}\PYG{o}{\PYGZam{}} \PYG{n}{model\PYGZus{}name}\PYG{p}{,} \PYG{k+kt}{bool} \PYG{n}{verbose} \PYG{o}{=} \PYG{n+nb}{false}\PYG{p}{)}\PYG{p}{;}

  \PYG{n}{Error} \PYG{n+nf}{GetServerStatus}\PYG{p}{(}\PYG{n}{ServerStatus}\PYG{o}{*} \PYG{n}{status}\PYG{p}{)} \PYG{k}{override}\PYG{p}{;}

 \PYG{k}{private}\PYG{o}{:}
  \PYG{k}{static} \PYG{k+kt}{size\PYGZus{}t} \PYG{n}{ResponseHeaderHandler}\PYG{p}{(}\PYG{k+kt}{void}\PYG{o}{*}\PYG{p}{,} \PYG{k+kt}{size\PYGZus{}t}\PYG{p}{,} \PYG{k+kt}{size\PYGZus{}t}\PYG{p}{,} \PYG{k+kt}{void}\PYG{o}{*}\PYG{p}{)}\PYG{p}{;}
  \PYG{k}{static} \PYG{k+kt}{size\PYGZus{}t} \PYG{n+nf}{ResponseHandler}\PYG{p}{(}\PYG{k+kt}{void}\PYG{o}{*}\PYG{p}{,} \PYG{k+kt}{size\PYGZus{}t}\PYG{p}{,} \PYG{k+kt}{size\PYGZus{}t}\PYG{p}{,} \PYG{k+kt}{void}\PYG{o}{*}\PYG{p}{)}\PYG{p}{;}

  \PYG{n}{ServerStatusHttpContext}\PYG{p}{(}\PYG{k}{const} \PYG{n}{std}\PYG{o}{:}\PYG{o}{:}\PYG{n}{string}\PYG{o}{\PYGZam{}}\PYG{p}{,} \PYG{k+kt}{bool}\PYG{p}{)}\PYG{p}{;}
  \PYG{n}{ServerStatusHttpContext}\PYG{p}{(}\PYG{k}{const} \PYG{n}{std}\PYG{o}{:}\PYG{o}{:}\PYG{n}{string}\PYG{o}{\PYGZam{}}\PYG{p}{,} \PYG{k}{const} \PYG{n}{std}\PYG{o}{:}\PYG{o}{:}\PYG{n}{string}\PYG{o}{\PYGZam{}}\PYG{p}{,} \PYG{k+kt}{bool}\PYG{p}{)}\PYG{p}{;}

  \PYG{c+c1}{// URL for status endpoint on inference server.}
  \PYG{k}{const} \PYG{n}{std}\PYG{o}{:}\PYG{o}{:}\PYG{n}{string} \PYG{n}{url\PYGZus{}}\PYG{p}{;}

  \PYG{c+c1}{// RequestStatus received in server response}
  \PYG{n}{RequestStatus} \PYG{n}{request\PYGZus{}status\PYGZus{}}\PYG{p}{;}

  \PYG{c+c1}{// Serialized ServerStatus response from server.}
  \PYG{n}{std}\PYG{o}{:}\PYG{o}{:}\PYG{n}{string} \PYG{n}{response\PYGZus{}}\PYG{p}{;}
\PYG{p}{\PYGZcb{}}\PYG{p}{;}

\PYG{c+c1}{//==============================================================================}
\PYG{k}{class} \PYG{n+nc}{InferHttpContext} \PYG{o}{:} \PYG{k}{public} \PYG{n}{InferContext} \PYG{p}{\PYGZob{}}
 \PYG{k}{public}\PYG{o}{:}
  \PYG{o}{\PYGZti{}}\PYG{n}{InferHttpContext}\PYG{p}{(}\PYG{p}{)} \PYG{k}{override}\PYG{p}{;}

  \PYG{k}{static} \PYG{n}{Error} \PYG{n+nf}{Create}\PYG{p}{(}
      \PYG{n}{std}\PYG{o}{:}\PYG{o}{:}\PYG{n}{unique\PYGZus{}ptr}\PYG{o}{\PYGZlt{}}\PYG{n}{InferContext}\PYG{o}{\PYGZgt{}}\PYG{o}{*} \PYG{n}{ctx}\PYG{p}{,} \PYG{k}{const} \PYG{n}{std}\PYG{o}{:}\PYG{o}{:}\PYG{n}{string}\PYG{o}{\PYGZam{}} \PYG{n}{server\PYGZus{}url}\PYG{p}{,}
      \PYG{k}{const} \PYG{n}{std}\PYG{o}{:}\PYG{o}{:}\PYG{n}{string}\PYG{o}{\PYGZam{}} \PYG{n}{model\PYGZus{}name}\PYG{p}{,} \PYG{k+kt}{int64\PYGZus{}t} \PYG{n}{model\PYGZus{}version} \PYG{o}{=} \PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{,}
      \PYG{k+kt}{bool} \PYG{n}{verbose} \PYG{o}{=} \PYG{n+nb}{false}\PYG{p}{)}\PYG{p}{;}

  \PYG{k}{static} \PYG{n}{Error} \PYG{n+nf}{Create}\PYG{p}{(}
      \PYG{n}{std}\PYG{o}{:}\PYG{o}{:}\PYG{n}{unique\PYGZus{}ptr}\PYG{o}{\PYGZlt{}}\PYG{n}{InferContext}\PYG{o}{\PYGZgt{}}\PYG{o}{*} \PYG{n}{ctx}\PYG{p}{,} \PYG{n}{CorrelationID} \PYG{n}{correlation\PYGZus{}id}\PYG{p}{,}
      \PYG{k}{const} \PYG{n}{std}\PYG{o}{:}\PYG{o}{:}\PYG{n}{string}\PYG{o}{\PYGZam{}} \PYG{n}{server\PYGZus{}url}\PYG{p}{,} \PYG{k}{const} \PYG{n}{std}\PYG{o}{:}\PYG{o}{:}\PYG{n}{string}\PYG{o}{\PYGZam{}} \PYG{n}{model\PYGZus{}name}\PYG{p}{,}
      \PYG{k+kt}{int64\PYGZus{}t} \PYG{n}{model\PYGZus{}version} \PYG{o}{=} \PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{k+kt}{bool} \PYG{n}{verbose} \PYG{o}{=} \PYG{n+nb}{false}\PYG{p}{)}\PYG{p}{;}

  \PYG{n}{Error} \PYG{n+nf}{Run}\PYG{p}{(}\PYG{n}{ResultMap}\PYG{o}{*} \PYG{n}{results}\PYG{p}{)} \PYG{k}{override}\PYG{p}{;}
  \PYG{n}{Error} \PYG{n+nf}{AsyncRun}\PYG{p}{(}\PYG{n}{std}\PYG{o}{:}\PYG{o}{:}\PYG{n}{shared\PYGZus{}ptr}\PYG{o}{\PYGZlt{}}\PYG{n}{Request}\PYG{o}{\PYGZgt{}}\PYG{o}{*} \PYG{n}{async\PYGZus{}request}\PYG{p}{)} \PYG{k}{override}\PYG{p}{;}
  \PYG{n}{Error} \PYG{n+nf}{GetAsyncRunResults}\PYG{p}{(}
      \PYG{n}{ResultMap}\PYG{o}{*} \PYG{n}{results}\PYG{p}{,} \PYG{k}{const} \PYG{n}{std}\PYG{o}{:}\PYG{o}{:}\PYG{n}{shared\PYGZus{}ptr}\PYG{o}{\PYGZlt{}}\PYG{n}{Request}\PYG{o}{\PYGZgt{}}\PYG{o}{\PYGZam{}} \PYG{n}{async\PYGZus{}request}\PYG{p}{,}
      \PYG{k+kt}{bool} \PYG{n}{wait}\PYG{p}{)} \PYG{k}{override}\PYG{p}{;}

 \PYG{k}{private}\PYG{o}{:}
  \PYG{k}{static} \PYG{k+kt}{size\PYGZus{}t} \PYG{n}{RequestProvider}\PYG{p}{(}\PYG{k+kt}{void}\PYG{o}{*}\PYG{p}{,} \PYG{k+kt}{size\PYGZus{}t}\PYG{p}{,} \PYG{k+kt}{size\PYGZus{}t}\PYG{p}{,} \PYG{k+kt}{void}\PYG{o}{*}\PYG{p}{)}\PYG{p}{;}
  \PYG{k}{static} \PYG{k+kt}{size\PYGZus{}t} \PYG{n+nf}{ResponseHeaderHandler}\PYG{p}{(}\PYG{k+kt}{void}\PYG{o}{*}\PYG{p}{,} \PYG{k+kt}{size\PYGZus{}t}\PYG{p}{,} \PYG{k+kt}{size\PYGZus{}t}\PYG{p}{,} \PYG{k+kt}{void}\PYG{o}{*}\PYG{p}{)}\PYG{p}{;}
  \PYG{k}{static} \PYG{k+kt}{size\PYGZus{}t} \PYG{n+nf}{ResponseHandler}\PYG{p}{(}\PYG{k+kt}{void}\PYG{o}{*}\PYG{p}{,} \PYG{k+kt}{size\PYGZus{}t}\PYG{p}{,} \PYG{k+kt}{size\PYGZus{}t}\PYG{p}{,} \PYG{k+kt}{void}\PYG{o}{*}\PYG{p}{)}\PYG{p}{;}

  \PYG{n}{InferHttpContext}\PYG{p}{(}
      \PYG{k}{const} \PYG{n}{std}\PYG{o}{:}\PYG{o}{:}\PYG{n}{string}\PYG{o}{\PYGZam{}}\PYG{p}{,} \PYG{k}{const} \PYG{n}{std}\PYG{o}{:}\PYG{o}{:}\PYG{n}{string}\PYG{o}{\PYGZam{}}\PYG{p}{,} \PYG{k+kt}{int64\PYGZus{}t}\PYG{p}{,} \PYG{n}{CorrelationID}\PYG{p}{,} \PYG{k+kt}{bool}\PYG{p}{)}\PYG{p}{;}

  \PYG{c+c1}{// @see InferContext.AsyncTransfer()}
  \PYG{k+kt}{void} \PYG{n+nf}{AsyncTransfer}\PYG{p}{(}\PYG{p}{)} \PYG{k}{override}\PYG{p}{;}

  \PYG{c+c1}{// @see InferContext.PreRunProcessing()}
  \PYG{n}{Error} \PYG{n+nf}{PreRunProcessing}\PYG{p}{(}\PYG{n}{std}\PYG{o}{:}\PYG{o}{:}\PYG{n}{shared\PYGZus{}ptr}\PYG{o}{\PYGZlt{}}\PYG{n}{Request}\PYG{o}{\PYGZgt{}}\PYG{o}{\PYGZam{}} \PYG{n}{request}\PYG{p}{)} \PYG{k}{override}\PYG{p}{;}

  \PYG{c+c1}{// curl multi handle for processing asynchronous requests}
  \PYG{n}{CURLM}\PYG{o}{*} \PYG{n}{multi\PYGZus{}handle\PYGZus{}}\PYG{p}{;}

  \PYG{c+c1}{// URL to POST to}
  \PYG{n}{std}\PYG{o}{:}\PYG{o}{:}\PYG{n}{string} \PYG{n}{url\PYGZus{}}\PYG{p}{;}

  \PYG{c+c1}{// Serialized InferRequestHeader}
  \PYG{n}{std}\PYG{o}{:}\PYG{o}{:}\PYG{n}{string} \PYG{n}{infer\PYGZus{}request\PYGZus{}str\PYGZus{}}\PYG{p}{;}

  \PYG{c+c1}{// Keep an easy handle alive to reuse the connection}
  \PYG{n}{CURL}\PYG{o}{*} \PYG{n}{curl\PYGZus{}}\PYG{p}{;}
\PYG{p}{\PYGZcb{}}\PYG{p}{;}

\PYG{c+c1}{//==============================================================================}
\PYG{k}{class} \PYG{n+nc}{ProfileHttpContext} \PYG{o}{:} \PYG{k}{public} \PYG{n}{ProfileContext} \PYG{p}{\PYGZob{}}
 \PYG{k}{public}\PYG{o}{:}
  \PYG{k}{static} \PYG{n}{Error} \PYG{n}{Create}\PYG{p}{(}
      \PYG{n}{std}\PYG{o}{:}\PYG{o}{:}\PYG{n}{unique\PYGZus{}ptr}\PYG{o}{\PYGZlt{}}\PYG{n}{ProfileContext}\PYG{o}{\PYGZgt{}}\PYG{o}{*} \PYG{n}{ctx}\PYG{p}{,} \PYG{k}{const} \PYG{n}{std}\PYG{o}{:}\PYG{o}{:}\PYG{n}{string}\PYG{o}{\PYGZam{}} \PYG{n}{server\PYGZus{}url}\PYG{p}{,}
      \PYG{k+kt}{bool} \PYG{n}{verbose} \PYG{o}{=} \PYG{n+nb}{false}\PYG{p}{)}\PYG{p}{;}

 \PYG{k}{private}\PYG{o}{:}
  \PYG{k}{static} \PYG{k+kt}{size\PYGZus{}t} \PYG{n}{ResponseHeaderHandler}\PYG{p}{(}\PYG{k+kt}{void}\PYG{o}{*}\PYG{p}{,} \PYG{k+kt}{size\PYGZus{}t}\PYG{p}{,} \PYG{k+kt}{size\PYGZus{}t}\PYG{p}{,} \PYG{k+kt}{void}\PYG{o}{*}\PYG{p}{)}\PYG{p}{;}

  \PYG{n}{ProfileHttpContext}\PYG{p}{(}\PYG{k}{const} \PYG{n}{std}\PYG{o}{:}\PYG{o}{:}\PYG{n}{string}\PYG{o}{\PYGZam{}}\PYG{p}{,} \PYG{k+kt}{bool}\PYG{p}{)}\PYG{p}{;}
  \PYG{n}{Error} \PYG{n+nf}{SendCommand}\PYG{p}{(}\PYG{k}{const} \PYG{n}{std}\PYG{o}{:}\PYG{o}{:}\PYG{n}{string}\PYG{o}{\PYGZam{}} \PYG{n}{cmd\PYGZus{}str}\PYG{p}{)} \PYG{k}{override}\PYG{p}{;}

  \PYG{c+c1}{// URL for status endpoint on inference server.}
  \PYG{k}{const} \PYG{n}{std}\PYG{o}{:}\PYG{o}{:}\PYG{n}{string} \PYG{n}{url\PYGZus{}}\PYG{p}{;}

  \PYG{c+c1}{// RequestStatus received in server response}
  \PYG{n}{RequestStatus} \PYG{n}{request\PYGZus{}status\PYGZus{}}\PYG{p}{;}
\PYG{p}{\PYGZcb{}}\PYG{p}{;}

\PYG{c+c1}{//==============================================================================}
\PYG{k}{class} \PYG{n+nc}{ServerHealthGrpcContext} \PYG{o}{:} \PYG{k}{public} \PYG{n}{ServerHealthContext} \PYG{p}{\PYGZob{}}
 \PYG{k}{public}\PYG{o}{:}
  \PYG{k}{static} \PYG{n}{Error} \PYG{n}{Create}\PYG{p}{(}
      \PYG{n}{std}\PYG{o}{:}\PYG{o}{:}\PYG{n}{unique\PYGZus{}ptr}\PYG{o}{\PYGZlt{}}\PYG{n}{ServerHealthContext}\PYG{o}{\PYGZgt{}}\PYG{o}{*} \PYG{n}{ctx}\PYG{p}{,} \PYG{k}{const} \PYG{n}{std}\PYG{o}{:}\PYG{o}{:}\PYG{n}{string}\PYG{o}{\PYGZam{}} \PYG{n}{server\PYGZus{}url}\PYG{p}{,}
      \PYG{k+kt}{bool} \PYG{n}{verbose} \PYG{o}{=} \PYG{n+nb}{false}\PYG{p}{)}\PYG{p}{;}

  \PYG{n}{Error} \PYG{n+nf}{GetReady}\PYG{p}{(}\PYG{k+kt}{bool}\PYG{o}{*} \PYG{n}{ready}\PYG{p}{)} \PYG{k}{override}\PYG{p}{;}
  \PYG{n}{Error} \PYG{n+nf}{GetLive}\PYG{p}{(}\PYG{k+kt}{bool}\PYG{o}{*} \PYG{n}{live}\PYG{p}{)} \PYG{k}{override}\PYG{p}{;}

 \PYG{k}{private}\PYG{o}{:}
  \PYG{n}{ServerHealthGrpcContext}\PYG{p}{(}\PYG{k}{const} \PYG{n}{std}\PYG{o}{:}\PYG{o}{:}\PYG{n}{string}\PYG{o}{\PYGZam{}}\PYG{p}{,} \PYG{k+kt}{bool}\PYG{p}{)}\PYG{p}{;}
  \PYG{n}{Error} \PYG{n+nf}{GetHealth}\PYG{p}{(}\PYG{k}{const} \PYG{n}{std}\PYG{o}{:}\PYG{o}{:}\PYG{n}{string}\PYG{o}{\PYGZam{}} \PYG{n}{mode}\PYG{p}{,} \PYG{k+kt}{bool}\PYG{o}{*} \PYG{n}{health}\PYG{p}{)}\PYG{p}{;}

  \PYG{c+c1}{// GRPC end point.}
  \PYG{n}{std}\PYG{o}{:}\PYG{o}{:}\PYG{n}{unique\PYGZus{}ptr}\PYG{o}{\PYGZlt{}}\PYG{n}{GRPCService}\PYG{o}{:}\PYG{o}{:}\PYG{n}{Stub}\PYG{o}{\PYGZgt{}} \PYG{n}{stub\PYGZus{}}\PYG{p}{;}
\PYG{p}{\PYGZcb{}}\PYG{p}{;}

\PYG{c+c1}{//==============================================================================}
\PYG{k}{class} \PYG{n+nc}{ServerStatusGrpcContext} \PYG{o}{:} \PYG{k}{public} \PYG{n}{ServerStatusContext} \PYG{p}{\PYGZob{}}
 \PYG{k}{public}\PYG{o}{:}
  \PYG{k}{static} \PYG{n}{Error} \PYG{n}{Create}\PYG{p}{(}
      \PYG{n}{std}\PYG{o}{:}\PYG{o}{:}\PYG{n}{unique\PYGZus{}ptr}\PYG{o}{\PYGZlt{}}\PYG{n}{ServerStatusContext}\PYG{o}{\PYGZgt{}}\PYG{o}{*} \PYG{n}{ctx}\PYG{p}{,} \PYG{k}{const} \PYG{n}{std}\PYG{o}{:}\PYG{o}{:}\PYG{n}{string}\PYG{o}{\PYGZam{}} \PYG{n}{server\PYGZus{}url}\PYG{p}{,}
      \PYG{k+kt}{bool} \PYG{n}{verbose} \PYG{o}{=} \PYG{n+nb}{false}\PYG{p}{)}\PYG{p}{;}

  \PYG{k}{static} \PYG{n}{Error} \PYG{n+nf}{Create}\PYG{p}{(}
      \PYG{n}{std}\PYG{o}{:}\PYG{o}{:}\PYG{n}{unique\PYGZus{}ptr}\PYG{o}{\PYGZlt{}}\PYG{n}{ServerStatusContext}\PYG{o}{\PYGZgt{}}\PYG{o}{*} \PYG{n}{ctx}\PYG{p}{,} \PYG{k}{const} \PYG{n}{std}\PYG{o}{:}\PYG{o}{:}\PYG{n}{string}\PYG{o}{\PYGZam{}} \PYG{n}{server\PYGZus{}url}\PYG{p}{,}
      \PYG{k}{const} \PYG{n}{std}\PYG{o}{:}\PYG{o}{:}\PYG{n}{string}\PYG{o}{\PYGZam{}} \PYG{n}{model\PYGZus{}name}\PYG{p}{,} \PYG{k+kt}{bool} \PYG{n}{verbose} \PYG{o}{=} \PYG{n+nb}{false}\PYG{p}{)}\PYG{p}{;}

  \PYG{n}{Error} \PYG{n+nf}{GetServerStatus}\PYG{p}{(}\PYG{n}{ServerStatus}\PYG{o}{*} \PYG{n}{status}\PYG{p}{)} \PYG{k}{override}\PYG{p}{;}

 \PYG{k}{private}\PYG{o}{:}
  \PYG{n}{ServerStatusGrpcContext}\PYG{p}{(}\PYG{k}{const} \PYG{n}{std}\PYG{o}{:}\PYG{o}{:}\PYG{n}{string}\PYG{o}{\PYGZam{}}\PYG{p}{,} \PYG{k+kt}{bool}\PYG{p}{)}\PYG{p}{;}
  \PYG{n}{ServerStatusGrpcContext}\PYG{p}{(}\PYG{k}{const} \PYG{n}{std}\PYG{o}{:}\PYG{o}{:}\PYG{n}{string}\PYG{o}{\PYGZam{}}\PYG{p}{,} \PYG{k}{const} \PYG{n}{std}\PYG{o}{:}\PYG{o}{:}\PYG{n}{string}\PYG{o}{\PYGZam{}}\PYG{p}{,} \PYG{k+kt}{bool}\PYG{p}{)}\PYG{p}{;}

  \PYG{c+c1}{// Model name}
  \PYG{k}{const} \PYG{n}{std}\PYG{o}{:}\PYG{o}{:}\PYG{n}{string} \PYG{n}{model\PYGZus{}name\PYGZus{}}\PYG{p}{;}

  \PYG{c+c1}{// GRPC end point.}
  \PYG{n}{std}\PYG{o}{:}\PYG{o}{:}\PYG{n}{unique\PYGZus{}ptr}\PYG{o}{\PYGZlt{}}\PYG{n}{GRPCService}\PYG{o}{:}\PYG{o}{:}\PYG{n}{Stub}\PYG{o}{\PYGZgt{}} \PYG{n}{stub\PYGZus{}}\PYG{p}{;}
\PYG{p}{\PYGZcb{}}\PYG{p}{;}

\PYG{c+c1}{//==============================================================================}
\PYG{k}{class} \PYG{n+nc}{InferGrpcContext} \PYG{o}{:} \PYG{k}{public} \PYG{n}{InferContext} \PYG{p}{\PYGZob{}}
 \PYG{k}{public}\PYG{o}{:}
  \PYG{k}{virtual} \PYG{o}{\PYGZti{}}\PYG{n}{InferGrpcContext}\PYG{p}{(}\PYG{p}{)} \PYG{k}{override}\PYG{p}{;}

  \PYG{k}{static} \PYG{n}{Error} \PYG{n+nf}{Create}\PYG{p}{(}
      \PYG{n}{std}\PYG{o}{:}\PYG{o}{:}\PYG{n}{unique\PYGZus{}ptr}\PYG{o}{\PYGZlt{}}\PYG{n}{InferContext}\PYG{o}{\PYGZgt{}}\PYG{o}{*} \PYG{n}{ctx}\PYG{p}{,} \PYG{k}{const} \PYG{n}{std}\PYG{o}{:}\PYG{o}{:}\PYG{n}{string}\PYG{o}{\PYGZam{}} \PYG{n}{server\PYGZus{}url}\PYG{p}{,}
      \PYG{k}{const} \PYG{n}{std}\PYG{o}{:}\PYG{o}{:}\PYG{n}{string}\PYG{o}{\PYGZam{}} \PYG{n}{model\PYGZus{}name}\PYG{p}{,} \PYG{k+kt}{int64\PYGZus{}t} \PYG{n}{model\PYGZus{}version} \PYG{o}{=} \PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{,}
      \PYG{k+kt}{bool} \PYG{n}{verbose} \PYG{o}{=} \PYG{n+nb}{false}\PYG{p}{)}\PYG{p}{;}

  \PYG{k}{static} \PYG{n}{Error} \PYG{n+nf}{Create}\PYG{p}{(}
      \PYG{n}{std}\PYG{o}{:}\PYG{o}{:}\PYG{n}{unique\PYGZus{}ptr}\PYG{o}{\PYGZlt{}}\PYG{n}{InferContext}\PYG{o}{\PYGZgt{}}\PYG{o}{*} \PYG{n}{ctx}\PYG{p}{,} \PYG{n}{CorrelationID} \PYG{n}{correlation\PYGZus{}id}\PYG{p}{,}
      \PYG{k}{const} \PYG{n}{std}\PYG{o}{:}\PYG{o}{:}\PYG{n}{string}\PYG{o}{\PYGZam{}} \PYG{n}{server\PYGZus{}url}\PYG{p}{,} \PYG{k}{const} \PYG{n}{std}\PYG{o}{:}\PYG{o}{:}\PYG{n}{string}\PYG{o}{\PYGZam{}} \PYG{n}{model\PYGZus{}name}\PYG{p}{,}
      \PYG{k+kt}{int64\PYGZus{}t} \PYG{n}{model\PYGZus{}version} \PYG{o}{=} \PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{k+kt}{bool} \PYG{n}{verbose} \PYG{o}{=} \PYG{n+nb}{false}\PYG{p}{)}\PYG{p}{;}

  \PYG{k}{virtual} \PYG{n}{Error} \PYG{n+nf}{Run}\PYG{p}{(}\PYG{n}{ResultMap}\PYG{o}{*} \PYG{n}{results}\PYG{p}{)} \PYG{k}{override}\PYG{p}{;}
  \PYG{k}{virtual} \PYG{n}{Error} \PYG{n+nf}{AsyncRun}\PYG{p}{(}\PYG{n}{std}\PYG{o}{:}\PYG{o}{:}\PYG{n}{shared\PYGZus{}ptr}\PYG{o}{\PYGZlt{}}\PYG{n}{Request}\PYG{o}{\PYGZgt{}}\PYG{o}{*} \PYG{n}{async\PYGZus{}request}\PYG{p}{)} \PYG{k}{override}\PYG{p}{;}
  \PYG{n}{Error} \PYG{n+nf}{GetAsyncRunResults}\PYG{p}{(}
      \PYG{n}{ResultMap}\PYG{o}{*} \PYG{n}{results}\PYG{p}{,} \PYG{k}{const} \PYG{n}{std}\PYG{o}{:}\PYG{o}{:}\PYG{n}{shared\PYGZus{}ptr}\PYG{o}{\PYGZlt{}}\PYG{n}{Request}\PYG{o}{\PYGZgt{}}\PYG{o}{\PYGZam{}} \PYG{n}{async\PYGZus{}request}\PYG{p}{,}
      \PYG{k+kt}{bool} \PYG{n}{wait}\PYG{p}{)} \PYG{k}{override}\PYG{p}{;}

 \PYG{k}{protected}\PYG{o}{:}
  \PYG{n}{InferGrpcContext}\PYG{p}{(}
      \PYG{k}{const} \PYG{n}{std}\PYG{o}{:}\PYG{o}{:}\PYG{n}{string}\PYG{o}{\PYGZam{}}\PYG{p}{,} \PYG{k}{const} \PYG{n}{std}\PYG{o}{:}\PYG{o}{:}\PYG{n}{string}\PYG{o}{\PYGZam{}}\PYG{p}{,} \PYG{k+kt}{int64\PYGZus{}t}\PYG{p}{,} \PYG{n}{CorrelationID}\PYG{p}{,} \PYG{k+kt}{bool}\PYG{p}{)}\PYG{p}{;}

  \PYG{c+c1}{// Helper function to initialize the context}
  \PYG{n}{Error} \PYG{n+nf}{InitHelper}\PYG{p}{(}
      \PYG{k}{const} \PYG{n}{std}\PYG{o}{:}\PYG{o}{:}\PYG{n}{string}\PYG{o}{\PYGZam{}} \PYG{n}{server\PYGZus{}url}\PYG{p}{,} \PYG{k}{const} \PYG{n}{std}\PYG{o}{:}\PYG{o}{:}\PYG{n}{string}\PYG{o}{\PYGZam{}} \PYG{n}{model\PYGZus{}name}\PYG{p}{,}
      \PYG{k+kt}{bool} \PYG{n}{verbose}\PYG{p}{)}\PYG{p}{;}

  \PYG{c+c1}{// @see InferContext.AsyncTransfer()}
  \PYG{k}{virtual} \PYG{k+kt}{void} \PYG{n+nf}{AsyncTransfer}\PYG{p}{(}\PYG{p}{)} \PYG{k}{override}\PYG{p}{;}

  \PYG{c+c1}{// @see InferContext.PreRunProcessing()}
  \PYG{n}{Error} \PYG{n+nf}{PreRunProcessing}\PYG{p}{(}\PYG{n}{std}\PYG{o}{:}\PYG{o}{:}\PYG{n}{shared\PYGZus{}ptr}\PYG{o}{\PYGZlt{}}\PYG{n}{Request}\PYG{o}{\PYGZgt{}}\PYG{o}{\PYGZam{}} \PYG{n}{request}\PYG{p}{)} \PYG{k}{override}\PYG{p}{;}

  \PYG{c+c1}{// The producer\PYGZhy{}consumer queue used to communicate asynchronously with}
  \PYG{c+c1}{// the GRPC runtime.}
  \PYG{n}{grpc}\PYG{o}{:}\PYG{o}{:}\PYG{n}{CompletionQueue} \PYG{n}{async\PYGZus{}request\PYGZus{}completion\PYGZus{}queue\PYGZus{}}\PYG{p}{;}

  \PYG{c+c1}{// GRPC end point.}
  \PYG{n}{std}\PYG{o}{:}\PYG{o}{:}\PYG{n}{unique\PYGZus{}ptr}\PYG{o}{\PYGZlt{}}\PYG{n}{GRPCService}\PYG{o}{:}\PYG{o}{:}\PYG{n}{Stub}\PYG{o}{\PYGZgt{}} \PYG{n}{stub\PYGZus{}}\PYG{p}{;}

  \PYG{c+c1}{// request for GRPC call, one request object can be used for multiple calls}
  \PYG{c+c1}{// since it can be overwritten as soon as the GRPC send finishes.}
  \PYG{n}{InferRequest} \PYG{n}{request\PYGZus{}}\PYG{p}{;}
\PYG{p}{\PYGZcb{}}\PYG{p}{;}

\PYG{c+c1}{//==============================================================================}
\PYG{k}{class} \PYG{n+nc}{InferGrpcStreamContext} \PYG{o}{:} \PYG{k}{public} \PYG{n}{InferGrpcContext} \PYG{p}{\PYGZob{}}
 \PYG{k}{public}\PYG{o}{:}
  \PYG{o}{\PYGZti{}}\PYG{n}{InferGrpcStreamContext}\PYG{p}{(}\PYG{p}{)} \PYG{k}{override}\PYG{p}{;}

  \PYG{k}{static} \PYG{n}{Error} \PYG{n+nf}{Create}\PYG{p}{(}
      \PYG{n}{std}\PYG{o}{:}\PYG{o}{:}\PYG{n}{unique\PYGZus{}ptr}\PYG{o}{\PYGZlt{}}\PYG{n}{InferContext}\PYG{o}{\PYGZgt{}}\PYG{o}{*} \PYG{n}{ctx}\PYG{p}{,} \PYG{k}{const} \PYG{n}{std}\PYG{o}{:}\PYG{o}{:}\PYG{n}{string}\PYG{o}{\PYGZam{}} \PYG{n}{server\PYGZus{}url}\PYG{p}{,}
      \PYG{k}{const} \PYG{n}{std}\PYG{o}{:}\PYG{o}{:}\PYG{n}{string}\PYG{o}{\PYGZam{}} \PYG{n}{model\PYGZus{}name}\PYG{p}{,} \PYG{k+kt}{int64\PYGZus{}t} \PYG{n}{model\PYGZus{}version} \PYG{o}{=} \PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{,}
      \PYG{k+kt}{bool} \PYG{n}{verbose} \PYG{o}{=} \PYG{n+nb}{false}\PYG{p}{)}\PYG{p}{;}

  \PYG{k}{static} \PYG{n}{Error} \PYG{n+nf}{Create}\PYG{p}{(}
      \PYG{n}{std}\PYG{o}{:}\PYG{o}{:}\PYG{n}{unique\PYGZus{}ptr}\PYG{o}{\PYGZlt{}}\PYG{n}{InferContext}\PYG{o}{\PYGZgt{}}\PYG{o}{*} \PYG{n}{ctx}\PYG{p}{,} \PYG{n}{CorrelationID} \PYG{n}{correlation\PYGZus{}id}\PYG{p}{,}
      \PYG{k}{const} \PYG{n}{std}\PYG{o}{:}\PYG{o}{:}\PYG{n}{string}\PYG{o}{\PYGZam{}} \PYG{n}{server\PYGZus{}url}\PYG{p}{,} \PYG{k}{const} \PYG{n}{std}\PYG{o}{:}\PYG{o}{:}\PYG{n}{string}\PYG{o}{\PYGZam{}} \PYG{n}{model\PYGZus{}name}\PYG{p}{,}
      \PYG{k+kt}{int64\PYGZus{}t} \PYG{n}{model\PYGZus{}version} \PYG{o}{=} \PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{k+kt}{bool} \PYG{n}{verbose} \PYG{o}{=} \PYG{n+nb}{false}\PYG{p}{)}\PYG{p}{;}

  \PYG{n}{Error} \PYG{n+nf}{Run}\PYG{p}{(}\PYG{n}{ResultMap}\PYG{o}{*} \PYG{n}{results}\PYG{p}{)} \PYG{k}{override}\PYG{p}{;}
  \PYG{n}{Error} \PYG{n+nf}{AsyncRun}\PYG{p}{(}\PYG{n}{std}\PYG{o}{:}\PYG{o}{:}\PYG{n}{shared\PYGZus{}ptr}\PYG{o}{\PYGZlt{}}\PYG{n}{Request}\PYG{o}{\PYGZgt{}}\PYG{o}{*} \PYG{n}{async\PYGZus{}request}\PYG{p}{)} \PYG{k}{override}\PYG{p}{;}

 \PYG{k}{private}\PYG{o}{:}
  \PYG{n}{InferGrpcStreamContext}\PYG{p}{(}
      \PYG{k}{const} \PYG{n}{std}\PYG{o}{:}\PYG{o}{:}\PYG{n}{string}\PYG{o}{\PYGZam{}}\PYG{p}{,} \PYG{k}{const} \PYG{n}{std}\PYG{o}{:}\PYG{o}{:}\PYG{n}{string}\PYG{o}{\PYGZam{}}\PYG{p}{,} \PYG{k+kt}{int64\PYGZus{}t}\PYG{p}{,} \PYG{n}{CorrelationID}\PYG{p}{,} \PYG{k+kt}{bool}\PYG{p}{)}\PYG{p}{;}

  \PYG{c+c1}{// @see InferContext.AsyncTransfer()}
  \PYG{k+kt}{void} \PYG{n+nf}{AsyncTransfer}\PYG{p}{(}\PYG{p}{)} \PYG{k}{override}\PYG{p}{;}

  \PYG{c+c1}{// gRPC objects for using the streaming API}
  \PYG{n}{grpc}\PYG{o}{:}\PYG{o}{:}\PYG{n}{ClientContext} \PYG{n}{context\PYGZus{}}\PYG{p}{;}
  \PYG{n}{std}\PYG{o}{:}\PYG{o}{:}\PYG{n}{shared\PYGZus{}ptr}\PYG{o}{\PYGZlt{}}\PYG{n}{grpc}\PYG{o}{:}\PYG{o}{:}\PYG{n}{ClientReaderWriter}\PYG{o}{\PYGZlt{}}\PYG{n}{InferRequest}\PYG{p}{,} \PYG{n}{InferResponse}\PYG{o}{\PYGZgt{}}\PYG{o}{\PYGZgt{}}
      \PYG{n}{stream\PYGZus{}}\PYG{p}{;}
\PYG{p}{\PYGZcb{}}\PYG{p}{;}

\PYG{c+c1}{//==============================================================================}
\PYG{k}{class} \PYG{n+nc}{ProfileGrpcContext} \PYG{o}{:} \PYG{k}{public} \PYG{n}{ProfileContext} \PYG{p}{\PYGZob{}}
 \PYG{k}{public}\PYG{o}{:}
  \PYG{k}{static} \PYG{n}{Error} \PYG{n}{Create}\PYG{p}{(}
      \PYG{n}{std}\PYG{o}{:}\PYG{o}{:}\PYG{n}{unique\PYGZus{}ptr}\PYG{o}{\PYGZlt{}}\PYG{n}{ProfileContext}\PYG{o}{\PYGZgt{}}\PYG{o}{*} \PYG{n}{ctx}\PYG{p}{,} \PYG{k}{const} \PYG{n}{std}\PYG{o}{:}\PYG{o}{:}\PYG{n}{string}\PYG{o}{\PYGZam{}} \PYG{n}{server\PYGZus{}url}\PYG{p}{,}
      \PYG{k+kt}{bool} \PYG{n}{verbose} \PYG{o}{=} \PYG{n+nb}{false}\PYG{p}{)}\PYG{p}{;}

 \PYG{k}{private}\PYG{o}{:}
  \PYG{n}{ProfileGrpcContext}\PYG{p}{(}\PYG{k}{const} \PYG{n}{std}\PYG{o}{:}\PYG{o}{:}\PYG{n}{string}\PYG{o}{\PYGZam{}}\PYG{p}{,} \PYG{k+kt}{bool}\PYG{p}{)}\PYG{p}{;}
  \PYG{n}{Error} \PYG{n+nf}{SendCommand}\PYG{p}{(}\PYG{k}{const} \PYG{n}{std}\PYG{o}{:}\PYG{o}{:}\PYG{n}{string}\PYG{o}{\PYGZam{}} \PYG{n}{cmd\PYGZus{}str}\PYG{p}{)} \PYG{k}{override}\PYG{p}{;}

  \PYG{c+c1}{// GRPC end point.}
  \PYG{n}{std}\PYG{o}{:}\PYG{o}{:}\PYG{n}{unique\PYGZus{}ptr}\PYG{o}{\PYGZlt{}}\PYG{n}{GRPCService}\PYG{o}{:}\PYG{o}{:}\PYG{n}{Stub}\PYG{o}{\PYGZgt{}} \PYG{n}{stub\PYGZus{}}\PYG{p}{;}
\PYG{p}{\PYGZcb{}}\PYG{p}{;}

\PYG{c+c1}{//==============================================================================}

\PYG{n}{std}\PYG{o}{:}\PYG{o}{:}\PYG{n}{ostream}\PYG{o}{\PYGZam{}} \PYG{k}{operator}\PYG{o}{\PYGZlt{}}\PYG{o}{\PYGZlt{}}\PYG{p}{(}\PYG{n}{std}\PYG{o}{:}\PYG{o}{:}\PYG{n}{ostream}\PYG{o}{\PYGZam{}}\PYG{p}{,} \PYG{k}{const} \PYG{n}{Error}\PYG{o}{\PYGZam{}}\PYG{p}{)}\PYG{p}{;}

\PYG{k}{template} \PYG{o}{\PYGZlt{}}\PYG{o}{\PYGZgt{}}
\PYG{n}{Error} \PYG{n}{InferContext}\PYG{o}{:}\PYG{o}{:}\PYG{n}{Result}\PYG{o}{:}\PYG{o}{:}\PYG{n}{GetRawAtCursor}\PYG{p}{(}\PYG{k+kt}{size\PYGZus{}t} \PYG{n}{batch\PYGZus{}idx}\PYG{p}{,} \PYG{n}{std}\PYG{o}{:}\PYG{o}{:}\PYG{n}{string}\PYG{o}{*} \PYG{n}{out}\PYG{p}{)}\PYG{p}{;}

\PYG{k}{template} \PYG{o}{\PYGZlt{}}\PYG{k}{typename} \PYG{n}{T}\PYG{o}{\PYGZgt{}}
\PYG{n}{Error}
\PYG{n}{InferContext}\PYG{o}{:}\PYG{o}{:}\PYG{n}{Result}\PYG{o}{:}\PYG{o}{:}\PYG{n}{GetRawAtCursor}\PYG{p}{(}\PYG{k+kt}{size\PYGZus{}t} \PYG{n}{batch\PYGZus{}idx}\PYG{p}{,} \PYG{n}{T}\PYG{o}{*} \PYG{n}{out}\PYG{p}{)}
\PYG{p}{\PYGZob{}}
  \PYG{k}{const} \PYG{k+kt}{uint8\PYGZus{}t}\PYG{o}{*} \PYG{n}{buf}\PYG{p}{;}
  \PYG{n}{Error} \PYG{n}{err} \PYG{o}{=} \PYG{n}{GetRawAtCursor}\PYG{p}{(}\PYG{n}{batch\PYGZus{}idx}\PYG{p}{,} \PYG{o}{\PYGZam{}}\PYG{n}{buf}\PYG{p}{,} \PYG{k}{sizeof}\PYG{p}{(}\PYG{n}{T}\PYG{p}{)}\PYG{p}{)}\PYG{p}{;}
  \PYG{k}{if} \PYG{p}{(}\PYG{o}{!}\PYG{n}{err}\PYG{p}{.}\PYG{n}{IsOk}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)} \PYG{p}{\PYGZob{}}
    \PYG{k}{return} \PYG{n}{err}\PYG{p}{;}
  \PYG{p}{\PYGZcb{}}

  \PYG{n}{std}\PYG{o}{:}\PYG{o}{:}\PYG{n}{copy}\PYG{p}{(}\PYG{n}{buf}\PYG{p}{,} \PYG{n}{buf} \PYG{o}{+} \PYG{k}{sizeof}\PYG{p}{(}\PYG{n}{T}\PYG{p}{)}\PYG{p}{,} \PYG{k}{reinterpret\PYGZus{}cast}\PYG{o}{\PYGZlt{}}\PYG{k+kt}{uint8\PYGZus{}t}\PYG{o}{*}\PYG{o}{\PYGZgt{}}\PYG{p}{(}\PYG{n}{out}\PYG{p}{)}\PYG{p}{)}\PYG{p}{;}
  \PYG{k}{return} \PYG{n}{Error}\PYG{o}{:}\PYG{o}{:}\PYG{n}{Success}\PYG{p}{;}
\PYG{p}{\PYGZcb{}}

\PYG{p}{\PYGZcb{}}\PYG{p}{\PYGZcb{}}\PYG{p}{\PYGZcb{}}  \PYG{c+c1}{// namespace nvidia::inferenceserver::client}
\end{sphinxVerbatim}


\paragraph{Includes}
\label{\detokenize{cpp_api/file_src_clients_c++_request.h:includes}}\begin{itemize}
\item {} 
\sphinxcode{\sphinxupquote{condition\_variable}}

\item {} 
\sphinxcode{\sphinxupquote{curl/curl.h}}

\item {} 
\sphinxcode{\sphinxupquote{grpcpp/grpcpp.h}}

\item {} 
\sphinxcode{\sphinxupquote{memory}}

\item {} 
\sphinxcode{\sphinxupquote{mutex}}

\item {} 
\sphinxcode{\sphinxupquote{src/core/api.pb.h}}

\item {} 
\sphinxcode{\sphinxupquote{src/core/grpc\_service.grpc.pb.h}}

\item {} 
\sphinxcode{\sphinxupquote{src/core/grpc\_service.pb.h}}

\item {} 
\sphinxcode{\sphinxupquote{src/core/model\_config.h}}

\item {} 
\sphinxcode{\sphinxupquote{src/core/model\_config.pb.h}}

\item {} 
\sphinxcode{\sphinxupquote{src/core/request\_status.pb.h}}

\item {} 
\sphinxcode{\sphinxupquote{src/core/server\_status.pb.h}}

\item {} 
\sphinxcode{\sphinxupquote{string}}

\item {} 
\sphinxcode{\sphinxupquote{thread}}

\item {} 
\sphinxcode{\sphinxupquote{vector}}

\end{itemize}


\paragraph{Namespaces}
\label{\detokenize{cpp_api/file_src_clients_c++_request.h:namespaces}}\begin{itemize}
\item {} 
{\hyperref[\detokenize{cpp_api/namespace_nvidia:namespace-nvidia}]{\sphinxcrossref{\DUrole{std,std-ref}{Namespace nvidia}}}}

\item {} 
{\hyperref[\detokenize{cpp_api/namespace_nvidia__inferenceserver:namespace-nvidia-inferenceserver}]{\sphinxcrossref{\DUrole{std,std-ref}{Namespace nvidia::inferenceserver}}}}

\item {} 
{\hyperref[\detokenize{cpp_api/namespace_nvidia__inferenceserver__client:namespace-nvidia-inferenceserver-client}]{\sphinxcrossref{\DUrole{std,std-ref}{Namespace nvidia::inferenceserver::client}}}}

\end{itemize}


\paragraph{Classes}
\label{\detokenize{cpp_api/file_src_clients_c++_request.h:classes}}\begin{itemize}
\item {} 
{\hyperref[\detokenize{cpp_api/structnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result_1_1ClassResult:exhale-struct-structnvidia-1-1inferenceserver-1-1client-1-1infercontext-1-1result-1-1classresult}]{\sphinxcrossref{\DUrole{std,std-ref}{Struct Result::ClassResult}}}}

\item {} 
{\hyperref[\detokenize{cpp_api/structnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Stat:exhale-struct-structnvidia-1-1inferenceserver-1-1client-1-1infercontext-1-1stat}]{\sphinxcrossref{\DUrole{std,std-ref}{Struct InferContext::Stat}}}}

\item {} 
{\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1Error:exhale-class-classnvidia-1-1inferenceserver-1-1client-1-1error}]{\sphinxcrossref{\DUrole{std,std-ref}{Class Error}}}}

\item {} 
{\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext:exhale-class-classnvidia-1-1inferenceserver-1-1client-1-1infercontext}]{\sphinxcrossref{\DUrole{std,std-ref}{Class InferContext}}}}

\item {} 
{\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Input:exhale-class-classnvidia-1-1inferenceserver-1-1client-1-1infercontext-1-1input}]{\sphinxcrossref{\DUrole{std,std-ref}{Class InferContext::Input}}}}

\item {} 
{\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Options:exhale-class-classnvidia-1-1inferenceserver-1-1client-1-1infercontext-1-1options}]{\sphinxcrossref{\DUrole{std,std-ref}{Class InferContext::Options}}}}

\item {} 
{\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Output:exhale-class-classnvidia-1-1inferenceserver-1-1client-1-1infercontext-1-1output}]{\sphinxcrossref{\DUrole{std,std-ref}{Class InferContext::Output}}}}

\item {} 
{\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Request:exhale-class-classnvidia-1-1inferenceserver-1-1client-1-1infercontext-1-1request}]{\sphinxcrossref{\DUrole{std,std-ref}{Class InferContext::Request}}}}

\item {} 
{\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1RequestTimers:exhale-class-classnvidia-1-1inferenceserver-1-1client-1-1infercontext-1-1requesttimers}]{\sphinxcrossref{\DUrole{std,std-ref}{Class InferContext::RequestTimers}}}}

\item {} 
{\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferContext_1_1Result:exhale-class-classnvidia-1-1inferenceserver-1-1client-1-1infercontext-1-1result}]{\sphinxcrossref{\DUrole{std,std-ref}{Class InferContext::Result}}}}

\item {} 
{\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferGrpcContext:exhale-class-classnvidia-1-1inferenceserver-1-1client-1-1infergrpccontext}]{\sphinxcrossref{\DUrole{std,std-ref}{Class InferGrpcContext}}}}

\item {} 
{\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferGrpcStreamContext:exhale-class-classnvidia-1-1inferenceserver-1-1client-1-1infergrpcstreamcontext}]{\sphinxcrossref{\DUrole{std,std-ref}{Class InferGrpcStreamContext}}}}

\item {} 
{\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1InferHttpContext:exhale-class-classnvidia-1-1inferenceserver-1-1client-1-1inferhttpcontext}]{\sphinxcrossref{\DUrole{std,std-ref}{Class InferHttpContext}}}}

\item {} 
{\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ProfileContext:exhale-class-classnvidia-1-1inferenceserver-1-1client-1-1profilecontext}]{\sphinxcrossref{\DUrole{std,std-ref}{Class ProfileContext}}}}

\item {} 
{\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ProfileGrpcContext:exhale-class-classnvidia-1-1inferenceserver-1-1client-1-1profilegrpccontext}]{\sphinxcrossref{\DUrole{std,std-ref}{Class ProfileGrpcContext}}}}

\item {} 
{\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ProfileHttpContext:exhale-class-classnvidia-1-1inferenceserver-1-1client-1-1profilehttpcontext}]{\sphinxcrossref{\DUrole{std,std-ref}{Class ProfileHttpContext}}}}

\item {} 
{\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ServerHealthContext:exhale-class-classnvidia-1-1inferenceserver-1-1client-1-1serverhealthcontext}]{\sphinxcrossref{\DUrole{std,std-ref}{Class ServerHealthContext}}}}

\item {} 
{\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ServerHealthGrpcContext:exhale-class-classnvidia-1-1inferenceserver-1-1client-1-1serverhealthgrpccontext}]{\sphinxcrossref{\DUrole{std,std-ref}{Class ServerHealthGrpcContext}}}}

\item {} 
{\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ServerHealthHttpContext:exhale-class-classnvidia-1-1inferenceserver-1-1client-1-1serverhealthhttpcontext}]{\sphinxcrossref{\DUrole{std,std-ref}{Class ServerHealthHttpContext}}}}

\item {} 
{\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ServerStatusContext:exhale-class-classnvidia-1-1inferenceserver-1-1client-1-1serverstatuscontext}]{\sphinxcrossref{\DUrole{std,std-ref}{Class ServerStatusContext}}}}

\item {} 
{\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ServerStatusGrpcContext:exhale-class-classnvidia-1-1inferenceserver-1-1client-1-1serverstatusgrpccontext}]{\sphinxcrossref{\DUrole{std,std-ref}{Class ServerStatusGrpcContext}}}}

\item {} 
{\hyperref[\detokenize{cpp_api/classnvidia_1_1inferenceserver_1_1client_1_1ServerStatusHttpContext:exhale-class-classnvidia-1-1inferenceserver-1-1client-1-1serverstatushttpcontext}]{\sphinxcrossref{\DUrole{std,std-ref}{Class ServerStatusHttpContext}}}}

\end{itemize}


\paragraph{Functions}
\label{\detokenize{cpp_api/file_src_clients_c++_request.h:functions}}\begin{itemize}
\item {} 
{\hyperref[\detokenize{cpp_api/function_request_8h_1a19d352bb13848c14b5f03274b25ecee3:exhale-function-request-8h-1a19d352bb13848c14b5f03274b25ecee3}]{\sphinxcrossref{\DUrole{std,std-ref}{Function nvidia::inferenceserver::client::operator\textless{}\textless{}}}}}

\end{itemize}


\chapter{Python API}
\label{\detokenize{python_api:python-api}}\label{\detokenize{python_api::doc}}

\section{Client}
\label{\detokenize{python_api:client}}

\chapter{Indices and tables}
\label{\detokenize{index:indices-and-tables}}\begin{itemize}
\item {} 
\DUrole{xref,std,std-ref}{genindex}

\end{itemize}



\renewcommand{\indexname}{Index}
\printindex
\end{document}